{
  "hash": "7bd4ba5d37fb59bef4d02d64d549fde5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 3: Modeling correlation and regression\"\nsubtitle: \"<span style='font-size:2em;'> Practice session covering topics discussed in Lecture 3 </span>\"\nauthor: \"<a href='https://r4biostats.com/me.html' style='color:#72aed8;font-weight:600;'>M. Chiara Mimmi, Ph.D.</a>&ensp;|&ensp;Universit√† degli Studi di Pavia\"\ndate: 2024-07-26\ndate-format: long\ncode-link: true\nformat:\n  revealjs:\n    smaller: true\n    scrollable: true\n    theme: ../../theme/slidesMine.scss # QUARTO LOOKS IN SAME FOLDER \n#    logo: imgs_slides/mitgest_logo.png\n    footer: '[R 4 Biostatistics](https://r4biostats.com/) | MITGEST::training(2024)'\n#    footer: <https://lulliter.github.io/R4biostats/lectures.html>\n## ------------- x salvare come PDF \n    standalone: false\n    ## -------Produce a standalone HTML file with no external dependencies,\n    embed-resources: true\n    transition: fade\n    background-transition: fade\n    highlight-style: ayu-mirage\n    slide-number: true\n    fig-cap-location: top\n    # fig-format: svg\n    pdf-separate-fragments: false\n    # fig-align: center\nexecute:\n  # Quarto pre code blocks do not echo their source code by default\n  echo: true\n  include: true\n  freeze: auto\nbibliography: ../../bib/R4biostats.bib\ncsl: ../../bib/apa-6th-edition.csl \nsuppress-bibliography: true\n---\n\n\n# [GOAL OF TODAY'S PRACTICE SESSION]{.r-fit-text}\n\n::: {style=\"font-size: 90%;\"}\n::: {style=\"color:#77501a\"}\n+ Review the basic questions we can ask about ASSOCIATION between any two variables:\n  + does it exist?\n  + how strong is it?\n  + what is its direction?\n+ Introduce a widely used analytical tool: REGRESSION\n\n:::\n:::\n\n<br><br>\n\n::: {style=\"font-size: 70%;\"}\nThe examples and code from this lab session follow very closely the open access book:  \n\n+ Vu, J., & Harrington, D. (2021). **Introductory Statistics for the Life and Biomedical Sciences**. [https://www.openintro.org/book/biostat/](https://www.openintro.org/book/biostat)\n:::\n\n\n## Topics discussed in Lecture # 3\n\n\n::: {style=\"font-size: 75%;\"}\n**Lecture 3: topics** \n\n+ Testing and summarizing relationship between 2 variables (**correlation**)\n  + Pearson‚Äôs ùíì analysis (param)  \n  + Spearman test (no param)  \n+ Measures of **association** \n  + Chi-Square test of independence  \n  + Fisher‚Äôs Exact Test\n    + alternative to the Chi-Square Test of Independence\n+ From correlation/association to **prediction/causation** \n  + The purpose of observational and experimental studies\n+ Widely used analytical tools\n  + Simple linear regression models\n  + Multiple Linear Regression models\n+ Shifting the emphasis on **empirical prediction** \n  + Introduction to Machine Learning (ML)\n  + Distinction between Supervised & Unsupervised algorithms\n\n:::  \n\n# R ENVIRONMENT SET UP & DATA\n\n## Needed R Packages\n::: {style=\"font-size: 80%;\"}\n+ We will use functions from packages `base`, `utils`, and `stats` (pre-installed and pre-loaded) \n+ We will also use the packages below (specifying `package::function` for clarity).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load pckgs for this R session\n\n# -- General \nlibrary(fs)      # file/directory interactions\nlibrary(here)    # tools find your project's files, based on working directory\nlibrary(paint) # paint data.frames summaries in colour\nlibrary(janitor) # tools for examining and cleaning data\nlibrary(dplyr)   # {tidyverse} tools for manipulating and summarizing tidy data \nlibrary(forcats) # {tidyverse} tool for handling factors\nlibrary(openxlsx) # Read, Write and Edit xlsx Files\nlibrary(flextable) # Functions for Tabular Reporting\n# -- Statistics\nlibrary(rstatix) # Pipe-Friendly Framework for Basic Statistical Tests\nlibrary(lmtest) # Testing Linear Regression Models \nlibrary(broom) # Convert Statistical Objects into Tidy Tibbles\n#library(tidymodels) # not installed on this machine\nlibrary(performance) # Assessment of Regression Models Performance \n# -- Plotting\nlibrary(ggplot2) # Create Elegant Data Visualisations Using the Grammar of Graphics\n```\n:::\n\n\n\n<!-- # Data  -->\n<!-- # devtools::install_github(\"OI-Biostat/oi_biostat_data\") -->\n<!-- #library(oibiostat) # Data Package for OpenIntro Biostatistics  -->\n\n\n# DATASETS FOR TODAY\n\n::: {style=\"font-size: 70%;\"}\nWe will use examples (with adapted datasets) from real clinical studies, provided among the learning materials of the open access books:  \n\n+ Vu, J., & Harrington, D. (2021). **Introductory Statistics for the Life and Biomedical Sciences**. [https://www.openintro.org/book/biostat/](https://www.openintro.org/book/biostat)\n+ √áetinkaya-Rundel, M., & Hardim, J. (2023). **Introduction to Modern Statistics (1st Ed)**. [https://openintro-ims.netlify.app/](https://openintro-ims.netlify.app/)\n:::\n\n<!-- FATTO IO MA LORO NON VEDONO -->\n\n::: {.cell}\n\n:::\n\n \n## [Importing Dataset 1 (NHANES)]{.r-fit-text}\n\n::: {style=\"font-size: 85%;\"}\n**Name**: NHANES (National Health and Nutrition Examination Survey) combines interviews and physical examinations to assess the health and nutritional status of adults and children in the United States. Sterted in the 1960s, it became a continuous program in 1999.  \n**Documentation**: [dataset1](https://wwwn.cdc.gov/nchs/nhanes/Default.aspx)  \n**Sampling details**: Here we use a sample of 500 adults from NHANES 2009-2010 & 2011-2012 (`nhanes.samp.adult.500` in the R `oibiostat` package, which has been adjusted so that it can be viewed as a random sample of the US population)\n::: \n\n::: {.cell}\n\n```{.r .cell-code}\n# Check my working directory location\n# here::here()\n\n# Use `here` in specifying all the subfolders AFTER the working directory \nnhanes_samp <- read.csv(file = here::here(\"practice\", \"data_input\", \"03_datasets\",\n                                      \"nhanes.samp.csv\"), \n                          header = TRUE, # 1st line is the name of the variables\n                          sep = \",\", # which is the field separator character.\n                          na.strings = c(\"?\",\"NA\" ), # specific MISSING values  \n                          row.names = NULL) \n```\n:::\n\n\n \n+ Adapting the function `here` to match your own folder structure\n \n## [*NHANES* Variables and their description]{.r-fit-text}\n::: {style=\"font-size: 60%;\"}\n[[EXCERPT: see complete file in Input Data Folder]]{style=\"color:#77501a\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Variable </th>\n   <th style=\"text-align:left;\"> Type </th>\n   <th style=\"text-align:left;\"> Description </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> X </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> xxxx </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ID </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> xxxxx </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> SurveyYr </td>\n   <td style=\"text-align:left;\"> chr </td>\n   <td style=\"text-align:left;\"> yyyy_mm. Ex. 2011_12 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Gender </td>\n   <td style=\"text-align:left;\"> chr </td>\n   <td style=\"text-align:left;\"> Gender (sex) of study participant coded as male or female </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Age </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> ## </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> AgeDecade </td>\n   <td style=\"text-align:left;\"> chr </td>\n   <td style=\"text-align:left;\"> yy-yy es 20-29 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Education </td>\n   <td style=\"text-align:left;\"> chr </td>\n   <td style=\"text-align:left;\"> [&gt;= 20 yro]. Ex. 8thGrade, 9-11thGrade, HighSchool, SomeCollege, or CollegeGrad. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Weight </td>\n   <td style=\"text-align:left;\"> dbl </td>\n   <td style=\"text-align:left;\"> Weight in kg </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Height </td>\n   <td style=\"text-align:left;\"> dbl </td>\n   <td style=\"text-align:left;\"> Standing height in cm. Reported for participants aged 2 years or older. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> BMI </td>\n   <td style=\"text-align:left;\"> dbl </td>\n   <td style=\"text-align:left;\"> Body mass index (weight/height2 in kg/m2). Reported for participants aged 2 years or older </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Pulse </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> 60 second pulse rate </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> DirectChol </td>\n   <td style=\"text-align:left;\"> dbl </td>\n   <td style=\"text-align:left;\"> Direct HDL cholesterol in mmol/L. Reported for participants aged 6 years or older </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TotChol </td>\n   <td style=\"text-align:left;\"> dbl </td>\n   <td style=\"text-align:left;\"> Total HDL cholesterol in mmol/L. Reported for participants aged 6 years or older </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Diabetes </td>\n   <td style=\"text-align:left;\"> chr </td>\n   <td style=\"text-align:left;\"> Study participant told by a doctor or health professional that they have diabetes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> DiabetesAge </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Age of study participant when first told they had diabetes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> HealthGen </td>\n   <td style=\"text-align:left;\"> chr </td>\n   <td style=\"text-align:left;\"> Self-reported rating of health: Excellent, Vgood, Good, Fair, or Poor Fair </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Alcohol12PlusYr </td>\n   <td style=\"text-align:left;\"> chr </td>\n   <td style=\"text-align:left;\"> Participant has consumed at least 12 drinks of any type of alcoholic beverage in any one year </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ... </td>\n   <td style=\"text-align:left;\"> ... </td>\n   <td style=\"text-align:left;\"> ... </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n:::\n\n\n## [Importing Dataset 2 (PREVEND)]{.r-fit-text}\n\n::: {style=\"font-size: 85%;\"}\n**Name**: PREVEND (**Prevention of REnal and Vascular END-stage Disease**) is a study which took place in the Netherlands starting in the 1990s, with subsequent follow-ups throughout the 2000s. This dataset is from the third survey, which participants completed in 2003-2006; data is provided for 4,095 individuals who completed cognitive testing.  \n**Documentation**: [dataset2](https://research.rug.nl/en/datasets/prevention-of-renal-and-vascular-end-stage-disease-prevend) and sample dataset variables' [codebook](https://www.openintro.org/data/index.php?data=prevend)  \n**Sampling details**: Here we use a sample of 500 adults taken from 4,095 individuals who completed cognitive testing (i.e. the `prevend.samp` dataset in the R `oibiostat` package)\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check my working directory location\n# here::here()\n\n# Use `here` in specifying all the subfolders AFTER the working directory \nprevend_samp <- read.csv(file = here::here(\"practice\", \"data_input\", \"03_datasets\",\n                                      \"prevend.samp.csv\"), \n                          header = TRUE, # 1st line is the name of the variables\n                          sep = \",\", # which is the field separator character.\n                          na.strings = c(\"?\",\"NA\" ), # specific MISSING values  \n                          row.names = NULL) \n```\n:::\n\n\n## [*PREVEND* Variables and their description]{.r-fit-text}\n::: {style=\"font-size: 60%;\"}\n[[EXCERPT: see complete file in Input Data Folder]]{style=\"color:#77501a\"}\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Variable </th>\n   <th style=\"text-align:left;\"> Type </th>\n   <th style=\"text-align:left;\"> Description </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> X </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Patient ID </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Age </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Age in years </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Gender </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Expressed as: 0 = males; 1 = females </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RFFT </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Performance on the Ruff Figural Fluency Test. Scores range from 0 (worst) to 175 (best) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> VAT </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Visual Association Test score. Scores may range from 0 (worst) to 12 (best) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Chol </td>\n   <td style=\"text-align:left;\"> dbl </td>\n   <td style=\"text-align:left;\"> Total cholesterol, in mmol/L. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> HDL </td>\n   <td style=\"text-align:left;\"> dbl </td>\n   <td style=\"text-align:left;\"> HDL cholesterol, in mmol/L. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Statin </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Statin use at enrollment. Numeric vector: 0 = No; 1 = Yes. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> CVD </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> History of cardiovascular event. Numeric vector: 0 = No; 1 = Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> DM </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Diabetes mellitus status at enrollment. Numeric vector: 0 = No; 1 = Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Education </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Highest level of education. Numeric: 0 primary school; 1 = lower secondary education; 3 = university </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Smoking </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Smoking at enrollment. numeric vector: 0 = No; 1 = Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hypertension </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Status of hypertension at enrollment. Numeric vector: 0 = No; 1 = Yes </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Ethnicity </td>\n   <td style=\"text-align:left;\"> int </td>\n   <td style=\"text-align:left;\"> Expressed as: 0 = Western European; 1 = African; 2 = Asian; 3 = Other </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ... </td>\n   <td style=\"text-align:left;\"> ... </td>\n   <td style=\"text-align:left;\"> ... </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n:::\n\n## [Importing Dataset 3 (FAMuSS)]{.r-fit-text}\n\n::: {style=\"font-size: 85%;\"}\n**Name**: FAMuSS (Functional SNPs Associated with Muscle Size and Strength) examine the association of demographic, physiological and genetic characteristics with muscle strength -- including data on race and genotype at a specific locus on the ACTN3 gene (the \"sports gene\").  \n**Documentation**: [dataset3](https://www.openintro.org/data/index.php?data=famuss)  \n**Sampling details**: the DATASET includes 595 observations on 9 variables (`famuss` in the R `oibiostat` package)\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check my working directory location\n# here::here()\n\n# Use `here` in specifying all the subfolders AFTER the working directory \nfamuss <- read.csv(file = here::here(\"practice\", \"data_input\", \"03_datasets\",\n                                      \"famuss.csv\"), \n                          header = TRUE, # 1st line is the name of the variables\n                          sep = \",\", # which is the field separator character.\n                          na.strings = c(\"?\",\"NA\" ), # specific MISSING values  \n                          row.names = NULL) \n```\n:::\n\n\n## [*FAMuSS* Variables and their description]{.r-fit-text}\n::: {style=\"font-size: 60%;\"}\n[[See complete file in Input Data Folder]]{style=\"color:#77501a\"}\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Variable </th>\n   <th style=\"text-align:left;\"> Description </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> X </td>\n   <td style=\"text-align:left;\"> id </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ndrm.ch </td>\n   <td style=\"text-align:left;\"> Percent change in strength in the non-dominant arm </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> drm.ch </td>\n   <td style=\"text-align:left;\"> Percent change in strength in the  dominant arm </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sex </td>\n   <td style=\"text-align:left;\"> Sex of the participant </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> age </td>\n   <td style=\"text-align:left;\"> Age in years </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> race </td>\n   <td style=\"text-align:left;\"> Recorded as African Am (African American), Caucasian, Asian, Hispanic, Other </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> height </td>\n   <td style=\"text-align:left;\"> Height in inches </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> weight </td>\n   <td style=\"text-align:left;\"> Weight in pounds </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> actn3.r577x </td>\n   <td style=\"text-align:left;\"> Genotype at the location r577x in the ACTN3 gene. </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> bmi </td>\n   <td style=\"text-align:left;\"> Body Mass Index </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\n# CORRELATION\n\n> [Using NHANES and FAMuSS datasets]\n\n## [Explore relationships between two variables]{.r-fit-text}\nApproaches for summarizing relationships between two variables vary depending on variable types...\n\n- Two **numerical** variables\n- Two **categorical** variables\n- One **numerical** variable and one **categorical** variable\n\nTwo variables $x$ and $y$ are \n\n- *positively associated* if $y$ increases as $x$ increases. \n- *negatively associated* if $y$ decreases as $x$ increases.\n\n# TWO NUMERICAL VARIABLES (NHANES)\n\n## [Two numerical variables (plot)]{.r-fit-text}\n\n**Height** and **weight** (taken from the `nhanes_samp` dataset) are positively associated.\n\n+ notice we can also use the generic base R function `plot` for a quick scatter plot\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# rename for convenience\nnhanes <- nhanes_samp %>% \n  janitor::clean_names()\n\n# basis plot \nplot(nhanes$height, nhanes$weight,\n     xlab = \"Height (cm)\", ylab = \"Weight (kg)\", cex = 0.8)  \n```\n\n::: {.cell-output-display}\n![](slides_lab03_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## [Two numerical variables: correlation (with `stats::cor`)]{.r-fit-text}\n::: {style=\"font-size: 85%;\"}\n**Correlation** is a numerical summary that measures the strength of a linear relationship between two variables.\n\n <!-- - Introduced in *OI Biostat* Section 1.6.1; details in Ch. 6. -->\n\n - The correlation coefficient $r$ takes on values between $-1$ and $1$.\n  - The closer $r$ is to $\\pm 1$, the stronger the linear association.\n\n+ Here we compute the **Pearson rho (parametric)**, with base R function `stats::cor`  \n  + the `use` argument let us choose how to deal with missing values (in this case only using **all complete pairs**)  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nis.numeric(nhanes$height) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nis.numeric(nhanes$weight)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\n# using `stats` package\nstats::cor(x = nhanes$height, y =  nhanes$weight, \n    # argument for dealing with missing values\n    use = \"pairwise.complete.obs\",\n    method = \"pearson\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4102269\n```\n\n\n:::\n:::\n\n\n::: \n\n## [Two numerical variables: correlation (with `stats::cor.test`)]{.r-fit-text}\n\n+ Here we compute the **Pearson rho (parametric)**, with the function `cor.test` (the same we used for testing paired samples)\n  + implicitely takes care on `NAs`  \n\n::: {.cell}\n\n```{.r .cell-code}\n# using `stats` package \ncor_test_result <- cor.test(x = nhanes$height, y =  nhanes$weight, \n                            method = \"pearson\")\n\n# looking at the cor estimate\ncor_test_result[[\"estimate\"]][[\"cor\"]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4102269\n```\n\n\n:::\n:::\n\n\n+ The function `ggpubr::ggscatter` gives us all in one (scatter plot + $r$ (\"R\"))! ü§Ø \n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nlibrary(\"ggpubr\") # 'ggplot2' Based Publication Ready Plots\nggpubr::ggscatter(nhanes, x = \"height\", y = \"weight\", \n                  cor.coef = TRUE, cor.method = \"pearson\", #cor.coef.coord = 2,\n                  xlab = \"Height (in)\", ylab = \"Weight (lb)\")\n```\n\n::: {.cell-output-display}\n![](slides_lab03_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n:::\n\n\n\n## [Spearman rank-order correlation]{.r-fit-text} \nThe **Spearman's rank-order correlation is the nonparametric version** of the `Pearson` correlation. \n\nSpearman's correlation coefficient, ($œÅ$, also signified by $rs$) measures the strength and direction of association between two ranked variables.\n \n+ used when 2 variables have a **non-linear** relationship\n+ excellent for **ordinal** data (when Pearson's is not appropriate), i.e. Likert scale items\n\n> To compute it, we simply calculate Pearson‚Äôs correlation of the `rankings` of the raw data (instead of the data).\n\n## [Spearman rank-order correlation (example)]{.r-fit-text}\n::: {style=\"font-size: 70%;\"}\nLet's say we want to get Spearman's correlation with ordinal factors `Education` and `HealthGen` in the `NHANES` sample. \n\n+ We have to convert them to their underlying numeric code, to compare rankings. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntabyl(nhanes$education)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n nhanes$education   n percent valid_percent\n        8th Grade  32   0.064    0.06412826\n   9 - 11th Grade  68   0.136    0.13627255\n     College Grad 157   0.314    0.31462926\n      High School  94   0.188    0.18837675\n     Some College 148   0.296    0.29659319\n             <NA>   1   0.002            NA\n```\n\n\n:::\n\n```{.r .cell-code}\ntabyl(nhanes$health_gen)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n nhanes$health_gen   n percent valid_percent\n         Excellent  47   0.094    0.10444444\n              Fair  53   0.106    0.11777778\n              Good 177   0.354    0.39333333\n              Poor  11   0.022    0.02444444\n             Vgood 162   0.324    0.36000000\n              <NA>  50   0.100            NA\n```\n\n\n:::\n\n```{.r .cell-code}\nnhanes <- nhanes %>% \n  # reorder education\n  mutate (edu_ord = factor (education, \n                            levels = c(\"8th Grade\", \"9 - 11th Grade\",\n                                       \"High School\", \"Some College\",\n                                       \"College Grad\" , NA))) %>%  \n  # create edu_rank \n  mutate (edu_rank = as.numeric(edu_ord)) %>% \n  # reorder health education\n  mutate (health_ord = factor (health_gen, \n                            levels = c( NA, \"Poor\", \"Fair\",\n                                       \"Good\", \"Vgood\",\n                                       \"Excellent\"))) %>%\n  # create health_rank \n  mutate (health_rank = as.numeric(health_ord))\n```\n:::\n\n:::\n\n## [Spearman rank-order correlation (example), cont.]{.r-fit-text}\n\n::: {style=\"font-size: 70%;\"}\n+ Let's check out the `..._rank` version of the 2 categorical variables of interest: \n  + **education** from `edu_ord` to `edu_rank`\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nhanes$edu_ord, useNA = \"ifany\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     8th Grade 9 - 11th Grade    High School   Some College   College Grad \n            32             68             94            148            157 \n          <NA> \n             1 \n```\n\n\n:::\n\n```{.r .cell-code}\ntable(nhanes$edu_rank, useNA = \"ifany\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   1    2    3    4    5 <NA> \n  32   68   94  148  157    1 \n```\n\n\n:::\n:::\n\n\n  + **general health** from `health_ord` to `health_rank`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(nhanes$health_ord, useNA = \"ifany\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Poor      Fair      Good     Vgood Excellent      <NA> \n       11        53       177       162        47        50 \n```\n\n\n:::\n\n```{.r .cell-code}\ntable(nhanes$health_rank,  useNA = \"ifany\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   1    2    3    4    5 <NA> \n  11   53  177  162   47   50 \n```\n\n\n:::\n:::\n\n::: \n\n\n## [Spearman rank-order correlation (example cont.)]{.r-fit-text}\nAfter setting up the variables in the correct (numerical rank) format, now we can actually compute it: \n+ same function call `stats::cor.test`\n+ but specifying argument `method = \"spearman\"`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# -- using `stats` package \ncor_test_result_sp <- cor.test(x = nhanes$edu_rank,\n                               y = nhanes$health_rank, \n                               method = \"spearman\", \n                               exact = FALSE) # removes the Ties message warning \n# looking at the cor estimate\ncor_test_result_sp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tSpearman's rank correlation rho\n\ndata:  nhanes$edu_rank and nhanes$health_rank\nS = 10641203, p-value = 1.915e-10\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.2946493 \n```\n\n\n:::\n\n```{.r .cell-code}\n# -- only print Spearman rho \n#cor_test_result_sp[[\"estimate\"]][[\"rho\"]]\n```\n:::\n\n\n# TWO CATEGORICAL VARIABLES (FAMuSS)\n\n## [Two categorical variables (plot)]{.r-fit-text}\n::: {style=\"font-size: 80%;\"}\nIn the `famuss` dataset, the variables `race`, and `actn3.r577x` are categorical variables.\n\n+ we can use the generic base R function `graphics::barplot`  \n\n::: {.cell}\n\n```{.r .cell-code}\nmycolors_contrast <- c(\"#9b2339\", \"#E7B800\",\"#239b85\", \"#85239b\", \"#9b8523\",\"#23399b\", \"#d8e600\", \"#0084e6\",\"#399B23\",  \"#e60066\" , \"#00d8e6\",  \"#005ca1\", \"#e68000\")\n\n## genotypes as columns\ngenotype.race = matrix(table(famuss$actn3.r577x, famuss$race), ncol=3, byrow=T)\ncolnames(genotype.race) = c(\"CC\", \"CT\", \"TT\")\nrownames(genotype.race) = c(\"African Am\", \"Asian\", \"Caucasian\", \"Hispanic\", \"Other\")\n\n# using generic base::barplot\ngraphics::barplot(genotype.race, col = mycolors_contrast[1:5], ylim=c(0,300), width=2)\nlegend(\"topright\", inset=c(.05, 0), fill=mycolors_contrast[1:5], \n       legend=rownames(genotype.race))\n```\n\n::: {.cell-output-display}\n![](slides_lab03_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n::: \n\n## [Two categorical variables (contingency table)]{.r-fit-text}\n::: {style=\"font-size: 80%;\"}\nSpecifically, the variable `actn3.r577x` takes on three possible levels (`CC`, `CT`, or `TT`) which indicate the distribution of genotype at location `r577x` on the `ACTN3` gene for the FAMuSS study participants.\n\nA **contingency table** summarizes data for two categorical variables. \n\n+ the function `stats::addmargins` puts arbitrary *Margins* on multidimensional tables\n  + The extra column & row `\"Sum\"` provide the *marginal totals* across each row and each column, respectively \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# levels of actn3.r577x\ntable(famuss$actn3.r577x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n CC  CT  TT \n173 261 161 \n```\n\n\n:::\n\n```{.r .cell-code}\n# contingency table to summarize race and actn3.r577x\naddmargins(table(famuss$race, famuss$actn3.r577x))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            \n              CC  CT  TT Sum\n  African Am  16   6   5  27\n  Asian       21  18  16  55\n  Caucasian  125 216 126 467\n  Hispanic     4  10   9  23\n  Other        7  11   5  23\n  Sum        173 261 161 595\n```\n\n\n:::\n:::\n\n:::\n## [Two categorical variables (contingency table prop)]{.r-fit-text}\n::: {style=\"font-size: 80%;\"}\nContingency tables can also be converted to show *proportions*. Since there are 2 variables, it is necessary to specify whether the proportions are calculated according to the row variable or the column variable.\n\n  + using the `margin = ` argument in the `base::prop.table` function (1 indicates rows, 2 indicates columns)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# adding row proportions\naddmargins(prop.table(table(famuss$race, famuss$actn3.r577x), margin =  1))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            \n                    CC        CT        TT       Sum\n  African Am 0.5925926 0.2222222 0.1851852 1.0000000\n  Asian      0.3818182 0.3272727 0.2909091 1.0000000\n  Caucasian  0.2676660 0.4625268 0.2698073 1.0000000\n  Hispanic   0.1739130 0.4347826 0.3913043 1.0000000\n  Other      0.3043478 0.4782609 0.2173913 1.0000000\n  Sum        1.7203376 1.9250652 1.3545972 5.0000000\n```\n\n\n:::\n\n```{.r .cell-code}\n# adding column proportions\naddmargins(prop.table(table(famuss$race, famuss$actn3.r577x),margin =  2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            \n                     CC         CT         TT        Sum\n  African Am 0.09248555 0.02298851 0.03105590 0.14652996\n  Asian      0.12138728 0.06896552 0.09937888 0.28973168\n  Caucasian  0.72254335 0.82758621 0.78260870 2.33273826\n  Hispanic   0.02312139 0.03831418 0.05590062 0.11733618\n  Other      0.04046243 0.04214559 0.03105590 0.11366392\n  Sum        1.00000000 1.00000000 1.00000000 3.00000000\n```\n\n\n:::\n:::\n\n:::\n\n## [Chi Squared test of `independence`]{.r-fit-text}\n\nThe **Chi-squared test** is a hypothesis test used to determine whether there is a relationship between **two categorical variables**. \n\n  + categorical vars. can have *nominal* or *ordinal* measurement scale\n  + the *observed* frequencies are compared with the *expected* frequencies and their deviations are examined.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Chi-squared test\n# (Test of association to see if \n# H0: the 2 cat var (race  & actn3.r577x ) are independent\n# H1: the 2 cat var are correlated in __some way__\n\ntab <- table(famuss$race, famuss$actn3.r577x)\ntest_chi <- chisq.test(tab)\n```\n:::\n\n\n\nthe obtained result (`test_chi`) is a list of objects...\n\n::: {.callout-warning icon=false}\n### {{< bi terminal-fill color=rgba(155,103,35,1.00) >}} You try...\n...run `View(test_chi)` to check \n:::\n\n\n## [Chi Squared test of `independence` (cont)]{.r-fit-text}\n\nWithin `test_chi` results there are:\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n+ `Observed frequencies` =   \nhow often a combination occurs in our sample\n\n::: {.cell}\n\n```{.r .cell-code}\n# Observed frequencies\ntest_chi$observed\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            \n              CC  CT  TT\n  African Am  16   6   5\n  Asian       21  18  16\n  Caucasian  125 216 126\n  Hispanic     4  10   9\n  Other        7  11   5\n```\n\n\n:::\n:::\n\n\n:::\n  \n::: {.column width=\"50%\"}\n+ `Expected frequencies` = what would it be if the 2 vars were PERFECTLY INDEPENDENT  \n\n::: {.cell}\n\n```{.r .cell-code}\n# Expected frequencies\nround(test_chi$expected  , digits = 1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            \n                CC    CT    TT\n  African Am   7.9  11.8   7.3\n  Asian       16.0  24.1  14.9\n  Caucasian  135.8 204.9 126.4\n  Hispanic     6.7  10.1   6.2\n  Other        6.7  10.1   6.2\n```\n\n\n:::\n:::\n\n:::\n  \n::::\n\n## [Chi Squared test of `independence` (results)]{.r-fit-text}\n\n+ Recall that:\n  + $H_{0}$: the 2 cat. var. are **independent**\n  + $H_{1}$: the 2 cat. var. are **correlated** in some way \n\n+ The result of Chi-Square test represents a comparison of the above two tables (*observed* v. *expected*): \n  + p-value = 0.01286 smaller than Œ± = 0.05 so **we REJECT the null hypothesis** (i.e. there‚Äôs likely an association between race and ACTN3 gene)\n  \n\n::: {.cell}\n\n```{.r .cell-code}\ntest_chi\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  tab\nX-squared = 19.4, df = 8, p-value = 0.01286\n```\n\n\n:::\n:::\n\n\n<!-- + Additional tables (from test results) -->\n<!-- ```{r} -->\n<!-- # Pearson's residual -->\n<!-- test_chi$residuals   -->\n<!-- # Standardized residual -->\n<!-- test_chi$stdres      -->\n<!-- ``` -->\n\n## [Computing Cramer's V after test of independence]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\nRecall that **Crammer's V** allows to measure the *effect size* of the test of independence (i.e. the **strength of association** between two nominal variables)\n\n+ $V$ ranges from [0 1] (the smaller $V$, the lower the correlation)\n\n$$V=\\sqrt{\\frac{\\chi^2}{n(k-1)}} $$\n\nwhere:\n\n+ $V$ denotes Cram√©r‚Äôs V \n+ $\\chi^2$ is the Pearson chi-square statistic from the prior test\n+ $n$ is the sample size involved in the test \n+ $k$ is the lesser number of categories of either variable\n::: \n## [Computing Cramer's V after test of independence (2 ways)]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\n+ ‚úçüèª \"By hand\" first to see the steps \n\n::: {.cell}\n\n```{.r .cell-code}\n# Compute Creamer's V by hand\n \n# inputs \nchi_calc <- test_chi$statistic\nn <- nrow(famuss) # N of obd \nn_r <- nrow(test_chi$observed) # number of rows in the contingency table\nn_c <- ncol(test_chi$observed) # number of columns in the contingency table\n\n# Cramer‚Äôs V\nsqrt(chi_calc / (n*min(n_r -1, n_c -1)) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nX-squared \n0.1276816 \n```\n\n\n:::\n:::\n\n\n+ üë©üèª‚Äçüíª Using an R function `rstatix::cramer_v`\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cramer‚Äôs V with rstatix\nrstatix::cramer_v(test_chi$observed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1276816\n```\n\n\n:::\n:::\n\n\n**Cramer‚Äôs V = 0.12**, which indicates a relatively weak association between the two categorical variables. It suggests that while there may be some relationship between the variables, it is not particularly strong.\n:::\n\n## [Chi Squared test of `goodness of fit`]{.r-fit-text}\n\nIn some cases the `Chi-square test` examines **whether or not an observed frequency distribution matches an expected theoretical distribution**.\n\nHere, we are conducting a type of `Chi-square Goodness of Fit Test` which:\n\n+ serves to test whether the observed distribution of a categorical variable differs from your expectations \n+ interprets the statistic based on the discrepancies between observed and expected counts\n\n\n## [Chi Squared test of `goodness of fit` (example)]{.r-fit-text}\n::: {style=\"font-size: 80%;\"}\nSince the participants of the **FAMuSS study** where *volunteers* at a university, they did not come from a \"representative\" sample of the US population, we can use the $\\chi^{2}$ goodness of fit test to test against:\n\n  + $H_{0}$: the study participants (1st row below) are racially representative of the general population (2nd row below)\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"tabwid\"><style>.cl-a8f14bda{}.cl-a8ec6d5e{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a8ee9b38{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a8ee9b39{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a8eea844{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a8eea845{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 1.5pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a8eea84e{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a8eea84f{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a8eea850{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a8eea851{width:0.75in;background-color:transparent;vertical-align: middle;border-bottom: 1.5pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style><table data-quarto-disable-processing='true' class='cl-a8f14bda'><thead><tr style=\"overflow-wrap:break-word;\"><th class=\"cl-a8eea844\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">Race</span></p></th><th class=\"cl-a8eea844\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">African.American</span></p></th><th class=\"cl-a8eea844\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">Asian</span></p></th><th class=\"cl-a8eea844\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">Caucasian</span></p></th><th class=\"cl-a8eea844\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">Other</span></p></th><th class=\"cl-a8eea845\"><p class=\"cl-a8ee9b39\"><span class=\"cl-a8ec6d5e\">Total</span></p></th></tr></thead><tbody><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-a8eea84e\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">FAMuSS (Observed) </span></p></td><td class=\"cl-a8eea84e\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">27</span></p></td><td class=\"cl-a8eea84e\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">55</span></p></td><td class=\"cl-a8eea84e\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">467</span></p></td><td class=\"cl-a8eea84e\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">46</span></p></td><td class=\"cl-a8eea84f\"><p class=\"cl-a8ee9b39\"><span class=\"cl-a8ec6d5e\">595</span></p></td></tr><tr style=\"overflow-wrap:break-word;\"><td class=\"cl-a8eea850\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">US Census (Expected) </span></p></td><td class=\"cl-a8eea850\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">76.16</span></p></td><td class=\"cl-a8eea850\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">5.95</span></p></td><td class=\"cl-a8eea850\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">478.38</span></p></td><td class=\"cl-a8eea850\"><p class=\"cl-a8ee9b38\"><span class=\"cl-a8ec6d5e\">34.51</span></p></td><td class=\"cl-a8eea851\"><p class=\"cl-a8ee9b39\"><span class=\"cl-a8ec6d5e\">595</span></p></td></tr></tbody></table></div>\n```\n\n:::\n:::\n\n\nWe use the formula \n$$\\chi^{2} = \\sum_{k}\\frac{(Observed - Expected)^{2}}{Expected}$$\n\nUnder $H_{0}$, the sample proportions should equal the population proportions.\n:::\n\n## [Chi Squared test of `goodness of fit` (example)]{.r-fit-text}\n::: {style=\"font-size: 80%;\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Subset the vectors of frequencies from the 2 rows  \nobserved <- c(27,  55,  467, 46)\nexpected <- c(76.2,  5.95, 478.38,  34.51)\n\n# Calculate Chi-Square statistic manually \nchi_sq_statistic <- sum((observed - expected)^2 / expected) \ndf <- length(observed) - 1 \np_value <- 1 - pchisq(chi_sq_statistic, df) \n\n# Print results \nchi_sq_statistic\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 440.2166\n```\n\n\n:::\n\n```{.r .cell-code}\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n\n```{.r .cell-code}\np_value \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nThe calculated $\\chi^{2}$ statistic is very large, and the `p_value` is close to 0. Hence, there is more than sufficient evidence to **reject the null hypothesis** that the sample is representative of the general population.  \nComparing the observed and expected values (or the residuals), we find the **largest discrepancy with the over-representation of Asian study participants**.\n\n:::\n<!-- # 1 CATEGORICAL & 1 NUMERICAL VARIABLES (???) -->\n\n# SIMPLE LINEAR REGRESSION \n\n> [Using NHANES dataset]\n\n## Visualize the data: BMI and age\n\nWe are mainly looking for a \"vaguely\" linear shape here \n\n+ `ggplot2` gives us a visual confirmation with `geom_point()`\n+ Essentially, `geom_smooth()` adds a trend line over an existing plot\n  + inside the function, we have different options with the `method` argument (default is LOESS (locally estimated scatterplot smoothing))\n  + with `method = lm` we get the linear best fit (the **least squares regression line**) & its 95% CI \n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(nhanes, aes (x = age, \n                          y = bmi)) + \n  geom_point() + \n  geom_smooth(method = lm,  \n              #se = FALSE\n              )\n```\n\n::: {.cell-output-display}\n![](slides_lab03_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n\n## [Linear regression model]{.r-fit-text}\nThe `lm()` function is used to fit linear models has the following generic structure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ x, data)\n```\n:::\n\n\nwhere:\n\n+ the 1st argument `y ~ x` specifies the variables used in the model (here the model regresses a **response variable**  $y$ against an **explanatory variable** $x$. \n+ The 2nd argument `data` is used only when the dataframe name is not already specified in the first argument. \n\n## [Linear regression models syntax]{.r-fit-text}\n\nThe following example shows fitting a linear model that predicts **BMI** from **age (in years)** using data from `nhanes` adult sample (individuals 21 years of age or older from the NHANES data). \n\n::: {.cell}\n\n```{.r .cell-code}\n# fitting linear model\nlm(nhanes$bmi ~ nhanes$age)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# or equivalently...\nlm(bmi ~ age, data = nhanes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = bmi ~ age, data = nhanes)\n\nCoefficients:\n(Intercept)          age  \n   28.40113      0.01982  \n```\n\n\n:::\n:::\n\n+ Running the function creates an *object* (of class `lm`) that contains several components (model coefficients, etc), either directly displayed or accessible with `summary()` notation or specific functions.\n\n\n## [Linear regression models syntax]{.r-fit-text}\n::: {style=\"font-size: 80%;\"}\n<!-- https://www.youtube.com/watch?v=4wS3n54Kon0 -->\nWe can save the model and then extract individual output elements from it using the `$` syntax \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# name the model object\nlr_model <- lm(bmi ~ age, data = nhanes)\n\n# extract model output elements\nlr_model$coefficients\nlr_model$residuals\nlr_model$fitted.values\n```\n:::\n\n\nThe command `summary` returns these elements \n\n+ `Call`: reminds the equation used for this regression model\n+ `Residuals`: a 5 number summary of the distribution of residuals from the regression model\n+ `Coefficients`:displays the estimated coefficients of the regression model and relative hypothesis testing, given for:  \n  + intercept \n  + explanatory variable(s) slope\n  \n::: \n\n## [Linear regression models interpretation: coefficients]{.r-fit-text}\n\n+ The model tests the null hypothesis $H_{0}$ that a coefficient is 0\n+ `coefficients` outputs are: `estimate`, `std. error`, `t-statistic`, and `p-value` correspondent to the t-statistic for:\n  + *intercept* \n  + *explanatory variable(s)* slope\n+ In regression, the population **parameter of interest** is typically the *slope* parameter \n  + in this model, `age` doesn't appear significantly ‚â† 0 \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lr_model)$coefficients \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               Estimate Std. Error   t value      Pr(>|t|)\n(Intercept) 28.40112932 0.96172389 29.531480 2.851707e-111\nage          0.01981675 0.01824641  1.086063  2.779797e-01\n```\n\n\n:::\n:::\n\n\n## [Linear regression models interpretation: Coefficients 2 ]{.r-fit-text}\n\nFor the the estimated coefficients of the regression model, we get:\n\n+ `Estimate` = the average increase in the response variable associated with a one unit increase in the predictor variable, (assuming all other predictor variables are held constant).\n+ `Std. Error` = a measure of the uncertainty in our estimate of the coefficient.\n<!-- `Std. Error` = of coefficients = `Residual Standard Error` (see below) divided by the square root of the sum of the square of that particular x variable. -->\n+ `t value`  = the t-statistic for the predictor variable, calculated as (Estimate) / (Std. Error).\n+ `Pr(>|t|)` = the p-value that corresponds to the t-statistic. If less than some alpha level (e.g. 0.05). the predictor variable is said to be *statistically significant*. \n\n\n## [Linear regression models outputs: fitted values]{.r-fit-text}\n\nHere we see $\\hat{y}_i$, i.e. the **fitted $y$ value for the $i$-th individual**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_val <- lr_model$fitted.values\n\n# print the first 6 elements\nhead(fit_val)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6 \n29.39197 29.33252 29.31270 28.95600 29.39197 29.17398 \n```\n\n\n:::\n:::\n\n\n## [Linear regression models outputs: residuals]{.r-fit-text}\n\nHere we see $e_i = y_i - \\hat{y}_i$, i.e. the **residual value for the $i$-th individual**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresid_val <- lr_model$residuals \n\n# print the first 6 elements\nhead(resid_val)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          1           2           3           4           5           6 \n-1.49196704  0.06748322 -3.96270002 -3.15599844 -2.49196704  3.75601726 \n```\n\n\n:::\n:::\n\n\n\n## [Linear regression model's fit: Residual standard error]{.r-fit-text}\n::: {style=\"font-size: 80%;\"}\n+ The `Residual standard error` (an estimate of the parameter $\\sigma$) tells the average distance that the observed values fall from the regression line (we are assuming constant variance). \n  + *The smaller it is, the better the model fits the dataset!*\n\nWe can compute it manually as: \n\n${\\rm SE}_{resid}=\\ \\sqrt{\\frac{\\sum_{i=1}^{n}{(y_i-{\\hat{y}}_i)}^2}{{\\rm df}_{resid}}}$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residual Standard error (Like Standard Deviation)\n\n# ---  inputs \n# sample size\nn =length(lr_model$residuals)\n# n of parameters in the model\nk = length(lr_model$coefficients)-1 #Subtract one to ignore intercept\n# degrees of freedom of the the residuals \ndf_resid = n-k-1\n# Squared Sum of Errors\nSSE =sum(lr_model$residuals^2) # 22991.19\n\n# --- Residual Standard Error\nResStdErr <- sqrt(SSE/df_resid)  # 6.815192\nResStdErr\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.815192\n```\n\n\n:::\n:::\n\n\n:::\n\n## [Linear regression model's fit: : $R^2$ and $Adj. R^2$]{.r-fit-text} \n::: {style=\"font-size: 85%;\"}\nThe **$R^2$** tells us the **proportion of the variance in the response variable** that can be explained by the predictor variable(s).\n\n+ if $R^2$ close to 0 -> data more spread\n+ if $R^2$ close to 1 -> data more tight around the regression line\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- R^2\nsummary(lr_model)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.00237723\n```\n\n\n:::\n:::\n\n\nThe **$Adj. R^2$** is a **modified version of $R^2$** that has been adjusted for the number of predictors in the model. \n\n+ It is always lower than the R-squared\n+ It can be useful for comparing the fit of different regression models that use different numbers of predictor variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- Adj. R^2\nsummary(lr_model)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0003618303\n```\n\n\n:::\n:::\n\n:::\n\n## [Linear regression model's fit: : F statistic]{.r-fit-text}\nThe **F-statistic** indicates whether the regression model provides a better fit to the data than a model that contains no independent variables. In essence, it tests if the regression model as a whole is useful.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract only F statistic \nsummary(lr_model)$fstatistic \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     value      numdf      dendf \n  1.179533   1.000000 495.000000 \n```\n\n\n:::\n\n```{.r .cell-code}\n# define function to extract overall p-value of model\noverall_p <- function(my_model) {\n    f <- summary(my_model)$fstatistic\n    p <- pf(f[1],f[2],f[3],lower.tail=F)\n    attributes(p) <- NULL\n    return(p)\n}\n\n# extract overall p-value of model\noverall_p(lr_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2779797\n```\n\n\n:::\n:::\n\n\nGiven the **p-value is > 0.05**, this indicate that *the predictor variable is not useful for predicting the value of the response variable*.\n\n# DIAGNOSTIC PLOTS \n\nThe following plots help us checking if (most of) the assumptions of linear regression are met!\n\n> (the **independence** assumption is more linked to the study design than to the data used in modeling)\n\n<!-- 2. Independence: The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data. -->\n\n## [Linear regression diagnostic plots: residuals 1/4]{.r-fit-text}\n\n> **ASSUMPTION 1**: there exists a linear relationship between the independent variable, x, and the dependent variable, y\n\nFor an observation $(x_i, y_i)$, where $\\hat{y}_i$ is the `predicted value` according to the line $\\hat{y} = b_0 + b_1x$, the `residual` is the value\n$e_i = y_i - \\hat{y}_i$\n\n+ A linear (e.g. `lr_model`) is a particularly good fit for the data when the residual plot shows random scatter above and below the horizontal line.\n  + (In this R plot, we look for a red line that is fairly straight)\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# residual plot\nplot(lr_model, which = 1 )\n```\n\n::: {.cell-output-display}\n![](slides_lab03_files/figure-revealjs/unnamed-chunk-41-1.png){width=960}\n:::\n:::\n\n\n+ We use the argument `which` in the function `plot` so we see the plots one at a time.\n\n## [Linear regression diagnostic plots: normality of residuals 2/4]{.r-fit-text}\n\n> **ASSUMPTION 2**: The residuals of the model are normally distributed\n\nWith the quantile-quantile plot (Q-Q) we can checking normality of the residuals.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# quantile-quantile plot\nplot(lr_model, which = 2 )\n```\n\n::: {.cell-output-display}\n![The data appear roughly normal, but there are deviations from normality in the tails, particularly the upper tail. ](slides_lab03_files/figure-revealjs/unnamed-chunk-42-1.png){width=960}\n:::\n:::\n\n\n\n## [Linear regression diagnostic plots: Homoscedasticity 3/4]{.r-fit-text}\n\n>**ASSUMPTION 3**: The residuals have constant variance at every level of x (\"*homoscedasticity*\")\n\nThis one is called a **Spread-location plot**: shows if residuals are spread equally along the ranges of predictors\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# Spread-location plot\nplot(lr_model, which = 3 )\n```\n\n::: {.cell-output-display}\n![](slides_lab03_files/figure-revealjs/unnamed-chunk-43-1.png){width=960}\n:::\n:::\n\n\n\n## [Test for Homoscedasticity]{.r-fit-text} \n::: {style=\"font-size: 90%;\"}\nBesides visual check, we can perform the `Breusch-Pagan test` to verify the assumption of homoscedasticity. In this case:\n\n  + $H_{0}$: residuals are distributed with **equal variance** \n  + $H_{1}$: residuals are distributed with **UNequal variance**  \n\n+ we use `bptest` function from the `lmtest` package\n \n\n<!--  https://www.codingprof.com/3-easy-ways-to-test-for-heteroscedasticity-in-r-examples/ -->\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Breusch-Pagan test against heteroskedasticity \nlmtest::bptest(lr_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  lr_model\nBP = 2.7548, df = 1, p-value = 0.09696\n```\n\n\n:::\n:::\n\nBecause the test statistic (BP) is small and the p-value is not significant  (p-value > 0.05): **WE DO NOT REJECT THE NULL HYPOTHESIS** (i.e. we can assume equal variance)\n:::\n\n## [Linear regression diagnostic plots: leverage 4/4]{.r-fit-text}\n\nThis last diagnostic plot has to do with **outliers**: \n\n+ A **residuals vs. leverage plot** allows us to identify *influential observations* in a regression model\n  + The x-axis shows the  \"`leverage`\"  of each point and the y-axis shows the \"`standardized residual of each point`\", i.e. *\"How much would the coefficients in the regression model would change if a particular observation was removed from the dataset?\"* \n  + `Cook's distance lines` (red dashed lines)  -- not visible here -- should appear on the corners of the plot when there are influential cases \n\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nplot(lr_model, which = 5 )\n```\n\n::: {.cell-output-display}\n![In this particular case, there is no influential case, or cases](slides_lab03_files/figure-revealjs/unnamed-chunk-45-1.png){width=960}\n:::\n:::\n\n\n## [(Digression on the `broom` package)]{.r-fit-text}\n\n::: {style=\"font-size: 90%;\"}\n+ The `broom` package introduces the ***tidy approach*** to regression modeling code and outputs, allowing to convert/save them in the form of `tibbles`\n+ The function `tidy` will turn an object into a tidy tibble\n+ The function `glance` will construct a single row summary \"glance\" of a model, fit, or other object\n+ The function `augment` will show a lot of results for the model attached to each observation \n  + this is very useful for further use of such objects, like `ggplot2` etc. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# render model as a dataframe \nbroom::tidy(lr_model)\n\n# see overal performance \nbroom::glance(lr_model)\n\n# save an object with all the model output elements \nmodel_aug <- broom::augment(lr_model)\n```\n:::\n\n\n::: {.callout-warning icon=false}\n### {{< bi terminal-fill color=rgba(155,103,35,1.00) >}} You try...\nRun these functions and then run `View(model_aug)` to check out the output\n:::\n\n:::\n\n\n# MULTIPLE LINEAR REGRESSION \n\n> [Using PREVEND dataset: a sample of 500 obs]\n\n\n<!-- PREVEND STESSO ESEMPIO -->\n\n<!-- copia  practice/slides_lesson_03_INPUT.Rmd  -->\n<!-- guardando libro Vu  pg 334  -->\n\n<!-- poi adatta su lezione  -->\n\n\n## [Visualize the data: Statin use and cognitive function]{.r-fit-text}\n\n**Statins** are a class of drugs widely used to lower **cholesterol** (recent guidelines would lead to statin use in almost half of Americans between 40 - 75 years of age and nearly all men over 60). But a few small studies have suggested that statins may be associated with lower **cognitive ability**.\n\n+ From this sample of the PREVEND study, we can observe the relationship between **statin use** (`statin_use`) and **cognitive ability** (`rfft`). \n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# rename for convenience\nprevend <- prevend_samp %>% janitor::clean_names() %>% \n  #create statin.use logical + factor\n  mutate(statin_use = as.logical(statin)) %>% \n  mutate(statin_use_f = factor(statin, levels = c(0,1), labels = c(\"NonUser\", \"User\")))   \n \n# box plot \nggplot(prevend, \n       aes (x = statin_use_f, y = rfft, fill = statin_use_f)) + \n  geom_boxplot(alpha=0.5) +\n  scale_fill_manual(values=c(\"#005ca1\",\"#9b2339\" )) +\n  # drop legend and Y-axis title\n  theme(legend.position = \"none\") \n```\n\n::: {.cell-output-display}\n![The boxplot suggests that statin user (red) present lower cognitive ability score, on average](slides_lab03_files/figure-revealjs/unnamed-chunk-47-1.png){width=960}\n:::\n:::\n\n\n\n## [Confirm visual intuition with independent sample t-test]{.r-fit-text}\nWe could use an independent t-test to confirm what the boxplot shows \n\n\n::: {.cell}\n\n```{.r .cell-code}\nt_test_w <- t.test(prevend$rfft[prevend$statin == 1], \n                   prevend$rfft[prevend$statin == 0],\n                   # here we specify the situation\n                   var.equal = TRUE,\n                   paired = FALSE, alternative = \"two.sided\") \n\nt_test_w\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  prevend$rfft[prevend$statin == 1] and prevend$rfft[prevend$statin == 0]\nt = -3.4917, df = 498, p-value = 0.0005226\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -15.710276  -4.396556\nsample estimates:\nmean of x mean of y \n 60.66087  70.71429 \n```\n\n\n:::\n:::\n\n\n...statistically significant difference in means (Statin use: yes and no) do exist \n\n## [Consider Simple Linear regression: Statin use and cognitive function]{.r-fit-text}\n::: {style=\"font-size: 80%;\"}\n\n... and build a simple linear regression model like so:\n\n $$E(RFFT) = b_0 + b_{statin} {(Statin\\ use)}$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#fit the linear model\nmodel_1 <- lm(rfft ~ statin, data=prevend)\nsummary(model_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = rfft ~ statin, data = prevend)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-56.714 -22.714   0.286  18.299  73.339 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   70.714      1.381  51.212  < 2e-16 ***\nstatin       -10.053      2.879  -3.492 0.000523 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 27.09 on 498 degrees of freedom\nMultiple R-squared:  0.0239,\tAdjusted R-squared:  0.02194 \nF-statistic: 12.19 on 1 and 498 DF,  p-value: 0.0005226\n```\n\n\n:::\n:::\n\n\n+ This preliminary model shows that, on average, **statin** users score approximately 10 points lower on the RFFT cognitive test (and the statin coefficient is **highly significant**!)\n  \n:::\n\n## [Visualize the data: Statin use and cognitive function + age]{.r-fit-text}\n\nHowever, following the literature, this prelimary model might be misleading (`biased`) because it does not account for the underlying relationship between age and statin  \n\n+ hence **age** could be a `confounder` within the **statin** -> **RFFT** relationship\n\n<!-- ```{r} -->\n<!-- #create statin.use logical -->\n<!-- statin.use = (prevend.samp$Statin == 1) -->\n\n<!-- #plot blue points -->\n<!-- plot(prevend.samp$Age[statin.use == FALSE], -->\n<!--      prevend.samp$RFFT[statin.use == FALSE], -->\n<!--      pch = 21, bg = COL[1, 3], col = COL[1], -->\n<!--      cex = 1.3, -->\n<!--      xlab = \"Age (yrs)\", -->\n<!--      ylab = \"RFFT Score\", -->\n<!--      main = \"RFFT Score versus Age in PREVEND (n = 500)\") -->\n\n<!-- #plot red points -->\n<!-- points(prevend.samp$Age[statin.use == TRUE], -->\n<!--      prevend.samp$RFFT[statin.use == TRUE], -->\n<!--      pch = 21, bg = COL[4, 3], col = COL[4], -->\n<!--      cex = 1.3) -->\n\n<!-- #draw vertical lines -->\n<!-- abline(v = 40, lty = 2) -->\n<!-- abline(v = 50, lty = 2) -->\n<!-- abline(v = 60, lty = 2) -->\n<!-- abline(v = 70, lty = 2) -->\n<!-- abline(v = 80, lty = 2) -->\n<!-- ``` -->\n\n \n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(prevend, \n       aes (x = age, y = rfft, group = statin_use)) + \n  geom_point (aes(color = statin_use , size=.01, alpha = 0.75),\n              show.legend = c(size = F, alpha = F) )+\n  scale_color_manual(values=c(\"#005ca1\",\"#9b2339\" )) + \n  # decades line separators \n  geom_vline(xintercept = 40, color = \"#A6A6A6\")+\n  geom_vline(xintercept = 50, color = \"#A6A6A6\")+\n  geom_vline(xintercept = 60, color = \"#A6A6A6\")+\n  geom_vline(xintercept = 70, color = \"#A6A6A6\")+\n  geom_vline(xintercept = 80, color = \"#A6A6A6\")  \n```\n\n::: {.cell-output-display}\n![Statin users are represented with red points; participants not using statins are shown as blue points](slides_lab03_files/figure-revealjs/unnamed-chunk-50-1.png){width=960}\n:::\n:::\n\n\n\n## [Multiple linear regression model]{.r-fit-text}\n\nMultiple regression allows for a (richer) model that incorporates both statin use and age: \n\n $$E(RFFT) = b_0 + b_{statin} {(Statin\\ use)}+ b_{age} {(Age)}$$\n\n + or (*in statistical terms*) the association between **RFFT** and **Statin use** is being estimated `after adjusting` for **Age**\n\nThe R syntax is very easy: simply use `+` to add covariates\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit the (multiple) linear model\nmodel_2 <- lm(rfft ~ statin + age , data=prevend)\n```\n:::\n\n\n\n## [RFFT vs. statin use & age...]{.r-fit-text}\n\nAlthough the use of statins appeared to be associated with lower RFFT scores when no adjustment was made for possible confounders, **`statin use` is not significantly associated with `RFFT score` in a regression model that adjusts for `age`**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = rfft ~ statin + age, data = prevend)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-63.855 -16.860  -1.178  15.730  58.751 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 137.8822     5.1221  26.919   <2e-16 ***\nstatin        0.8509     2.5957   0.328    0.743    \nage          -1.2710     0.0943 -13.478   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.21 on 497 degrees of freedom\nMultiple R-squared:  0.2852,\tAdjusted R-squared:  0.2823 \nF-statistic: 99.13 on 2 and 497 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n# Evaluating a multiple regression model\n\n## [Assumptions for multiple regression]{.r-fit-text}\nSimilar to those of simple linear regression...\n\n1. **Linearity**: For each predictor variable $x_j$, change in the predictor is linearly related to change in the response variable when the value of all other predictors is held constant.\n2. **Constant variability**: The residuals have approximately constant variance.\n3. **Normality of residuals**: The residuals are approximately normally distributed. \n4. **Independent observations**: Each set of observations $(y, x_1, x_2, \\dots, x_p)$ is independent.\n5. **No multicollinearity**: i.e. no situations when there is a strong linear correlation between the independent variables, conditional on the other variables in the model\n\n## [Using residual plots to assess LINEARITY: age]{.r-fit-text}\n\n> **ASSUMPTION 1**: there exists a linear relationship between the independent variables, $(x_1, x_2, \\dots, x_p)$, and the dependent variable, y\n\nIt is not possible to make a scatterplot of a response against several simultaneous predictors. Instead, use a `modified residual plot` to assess linearity:\n\n  - For **each** (numerical) predictor, plot the residuals on the $y$-axis and the predictor values on the $x$-axis. \n  - Patterns/curvature are indicative of non-linearity.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# recall \nmodel_2 <- lm(rfft ~ statin + age , data=prevend)\n\n# assess linearity\nplot(residuals(model_2) ~ prevend$age,\n     main = \"Residuals vs Age in PREVEND (n = 500)\",\n     xlab = \"Age (years)\", ylab = \"Residual\",\n     pch = 21, col = \"cornflowerblue\", bg = \"slategray2\",\n     cex = 0.60)\nabline(h = 0, col = \"red\", lty = 2)\n```\n\n::: {.cell-output-display}\n![There are no apparent trends; the data scatter evenly above and below the horizontal line. There does not seem to be remaining nonlinearity with respect to age after the model is fit.](slides_lab03_files/figure-revealjs/unnamed-chunk-53-1.png){width=960}\n:::\n:::\n\n  \n## [Using residual plots to assess LINEARITY: statin use]{.r-fit-text}\n  \nShould we be testing linearity of residuals also against a **categorical variable** (`statin use`)? (not really, because not meaningful)\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# recall \nmodel_2 <- lm(rfft ~ statin + age , data=prevend)\n\n#assess linearity\nplot(residuals(model_2) ~ prevend$statin,\n     main = \"Residuals vs Age in PREVEND (n = 500)\",\n     xlab = \"Age (years)\", ylab = \"Residual\",\n     pch = 21, col = \"cornflowerblue\", bg = \"slategray2\",\n     cex = 0.60)\nabline(h = 0, col = \"red\", lty = 2)\n```\n\n::: {.cell-output-display}\n![It is not necessary to assess linearity with respect to statin use since statin use is measured as a categorical variable. A line drawn through two points (that is, the mean of the two groups defined by a binary variable) is necessarily linear](slides_lab03_files/figure-revealjs/unnamed-chunk-54-1.png){width=960}\n:::\n:::\n\n  \n## [Using residual plots to assess CONSTANT VARIABILITY]{.r-fit-text}\n\n>**ASSUMPTION 2**: The residuals have constant variance at every level of x (\"*homoscedasticity*\")\n\n  - Constant variability: plot the residual values on the $y$-axis and the predicted values on the $x$-axis\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n#assess constant variance of residuals\nplot(residuals(model_2) ~ fitted(model_2),\n     main = \"Resid. vs Predicted RFFT in PREVEND (n = 500)\",\n     xlab = \"Predicted RFFT Score\", ylab = \"Residual\",\n     pch = 21, col = \"cornflowerblue\", bg = \"slategray2\",\n     cex = 0.60)\nabline(h = 0, col = \"red\", lty = 2)\n```\n\n::: {.cell-output-display}\n![The variance of the residuals is somewhat smaller for lower predicted values of RFFT score, but this may simply be an artifact from observing few individuals with relatively low predicted scores. It seems reasonable to assume approximately constant variance.](slides_lab03_files/figure-revealjs/unnamed-chunk-55-1.png){width=960}\n:::\n:::\n\n\n## [Using residual plots to assess NORMALITY of residuals]{.r-fit-text}\n\n> **ASSUMPTION 3**: The residuals of the model are normally distributed\n  - Normality of residuals: use Q-Q plots\n  \n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n#assess normality of residuals\nqqnorm(resid(model_2),\n       pch = 21, col = \"cornflowerblue\", bg = \"slategray2\", cex = 0.75,\n       main = \"Q-Q Plot of Residuals\")\nqqline(resid(model_2), col = \"red\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![The residuals are reasonably normally distributed, with only slight departures from normality in the tails.](slides_lab03_files/figure-revealjs/unnamed-chunk-56-1.png){width=960}\n:::\n:::\n\n\nIn our example, we see that most data points are OK, except some observations at the tails. However, if all other plots indicate no violation of assumptions, some deviation of normality, particularly at the tails, can be less critical.\n\n## [Assumption of INDEPENDENCE of observations]{.r-fit-text}\n\n>  **ASSUMPTION 4**: Each set of observations $(y, x_1, x_2, \\dots, x_p)$ is independent.\n\n\nIs it reasonable to assume that each set of observations is independent of the others?\n    \n> Using the PREVEND data, it is reasonable to assume that the observations in this dataset are independent. The participants were recruited from a large city in the Netherlands for a study focusing on factors associated with renal and cardiovascular disease.\n\n## [Assumption of NO MULTICOLLINEARITY]{.r-fit-text}\n::: {style=\"font-size: 95%;\"}\n\n>  **ASSUMPTION 5**: Each set of observations $(y, x_1, x_2, \\dots, x_p)$ is independent.\n\n\nThe R package `performance` actually provides a very helpful function `check_model()` which tests these assumptions all at the same time \n\n+ **Multicollinearity** is not an issue (based on a general threshold of 10 for VIF, all of them are below 10)\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# return and store a list of single plots\ndiagnostic_plots <- plot(performance::check_model(model_2, panel = FALSE))\n\n# see multicollinearity plot \ndiagnostic_plots[[5]]\n```\n\n::: {.cell-output-display}\n![](slides_lab03_files/figure-revealjs/unnamed-chunk-57-1.png){width=960}\n:::\n:::\n\n\n:::\n## [Checking out the `performance` R package ]{.r-fit-text}\n\n+ Find more info on the helpful `performance` R package [here](https://easystats.github.io/performance/index.html) for verifying assumptions and model's quality and goodness of fit. \n\n::: {.callout-warning icon=false}\n### {{< bi terminal-fill color=rgba(155,103,35,1.00) >}} You try...\nRun also the following commands \n\n+ Diagnostic plot of linearity\n`diagnostic_plots[[2]]`\n+ Diagnostic plot of influential observations - outliers\n`diagnostic_plots[[4]]`\n+ Diagnostic plot of normally distributed residuals\n`diagnostic_plots[[6]]`\n:::\n\n\n\n## [$R^2$ with multiple regression]{.r-fit-text}\n\n::: {style=\"font-size: 90%;\"}\nAs in simple regression, $R^2$ represents the proportion of variability in the response variable explained by the model.\n\n+ As variables are added, $R^2$ always increases.\n\nIn the `summary(lm( ))`  output,  `Multiple R-squared` is $R^2$.\n \n\n::: {.cell}\n\n```{.r .cell-code}\n#extract R^2 of a model\nsummary(model_2)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2851629\n```\n\n\n:::\n:::\n\n\n> The $R^2$ is 0.285; **the model explains 28.5\\% of the observed variation in RFFT score**. \n> The moderately low $R^2$ suggests that the model is missing other predictors of RFFT score.\n\n:::\n## [Adjusted $R^2$ as a tool for model assessment]{.r-fit-text}\n\nThe **adjusted $R^2$** is computed as:  \n$$R_{adj}^{2} = 1- \\left( \\frac{\\text{Var}(e_i)}{\\text{Var}(y_i)} \\times \\frac{n-1}{n-p-1} \\right)$$\n\n+ where $n$ is the number of cases and $p$ is the number of predictor variables.\n\nAdjusted $R^2$ incorporates a penalty for including predictors that do not contribute much towards explaining observed variation in the response variable.\n\n - It is often used to balance predictive ability with model complexity.\n \n - Unlike $R^2$, $R^2_{adj}$ does not have an inherent interpretation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#extract adjusted R^2 of a model\nsummary(model_2)$adj.r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2822863\n```\n\n\n:::\n:::\n\n\n# INTRODUCING SPECIAL KINDS OF PREDICTORS\n\n## [Categorical predictor in regression - (example)]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\nIs RFFT score associated with **education**?\nThe variable `Education` in the `PREVEND` dataset indicates the highest level of education an individual completed in the Dutch educational system: \n\n - 0: primary school\n - 1: lower secondary school\n - 2: higher secondary education\n - 3: university education\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# convert Education to a factor\nprevend <- prevend %>% \n  mutate(educ_f = factor(education,\n                          levels = c(0, 1, 2, 3),\n                          labels = c(\"Primary\", \"LowerSecond\",\n                                     \"HigherSecond\", \"Univ\")))\n```\n:::\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# load 4 color palette\npacificharbour_shades <- c( \"#d4e6f3\",  \"#9cc6e3\", \"#5b8bac\", \"#39576c\",  \"#16222b\")\n\n# create plot\nplot(rfft ~ educ_f, data = prevend,\n     xlab = \"Educational Level\", ylab = \"RFFT Score\",\n     main = \"RFFT by Education in PREVEND (n = 500)\",\n     names = c(\"Primary\", \"LowSec\", \"HighSec\", \"Univ\"),\n     col = pacificharbour_shades[1:4])\n```\n\n::: {.cell-output-display}\n![A very clear association seems to exist between education level and average RFFT score in the sample ](slides_lab03_files/figure-revealjs/unnamed-chunk-61-1.png){width=960}\n:::\n:::\n\n\n:::\n\n## [Categorical predictor in regression - model]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\nCalculate the average RFFT score in the sample across education levels\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate group means\nprevend %>% \n  group_by(educ_f) %>% \n  summarise(avg_RFFT_score = mean(rfft))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 √ó 2\n  educ_f       avg_RFFT_score\n  <fct>                 <dbl>\n1 Primary                40.9\n2 LowerSecond            55.7\n3 HigherSecond           73.1\n4 Univ                   85.9\n```\n\n\n:::\n:::\n\n\nFitting a model with `education` as a predictor\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit a model\nmodel_cat <- lm(rfft ~ educ_f, data = prevend) \nmodel_cat$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       (Intercept)  educ_fLowerSecond educ_fHigherSecond         educ_fUniv \n          40.94118           14.77857           32.13345           44.96389 \n```\n\n\n:::\n:::\n\n\n+ Notice how `Primary` level of `educ_f` does NOT appear as a coefficient \n:::\n\n## [Categorical predictor in regression - model interpretation]{.r-fit-text}\n\n::: {style=\"font-size: 80%;\"}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = rfft ~ educ_f, data = prevend)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.905 -15.975  -0.905  16.068  63.280 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          40.941      3.203  12.783  < 2e-16 ***\neduc_fLowerSecond    14.779      3.686   4.009 7.04e-05 ***\neduc_fHigherSecond   32.133      3.763   8.539  < 2e-16 ***\neduc_fUniv           44.964      3.684  12.207  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.87 on 496 degrees of freedom\nMultiple R-squared:  0.3072,\tAdjusted R-squared:  0.303 \nF-statistic:  73.3 on 3 and 496 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe baseline category represents individuals who at most completed primary school `Education = 0`. The coefficients represent the change in estimated average RFFT relative to the baseline category.\n\n  -  `(Intercept)` is the sample mean RFFT score for these individuals, 40.94 points\n  - An increase of 14.78 points is predicted for `LowerSecond` level, $40.94 + 14.78 = 55.72 \\ points$\n  - An increase of 32.13 points is predicted for `HigherSecond` level, $40.94 + 32.13 = 73.07  \\ points$\n  - An increase of 44.96 points is predicted for `Univ` level,  $40.94 + 44.96 = 85.90 \\ points$\n\n:::\n\n## [Interaction in regression - (example) - NHANES]{.r-fit-text}\n\nLet's go back to the `NHANES` dataset and consider a linear model that predicts **total cholesterol level (mmol/L)** from **age (yrs.)** and **diabetes status**.\n\n\nThe multiple regression model:\n\n$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_px_p + \\epsilon$$ \n\nassumes that when one of the predictors $x_j$ is changed by 1 unit and the values of the other variables remain constant, the predicted response changes by $\\beta_j$, *regardless of the values of the other variables*.\n\n+ With statistical **interaction**, this assumption is not true, such that *the effect of one explanatory variable $x_j$ on the $y$ depends on the particular value(s) of one or more other explanatory variables*.\n\n\n## [Interaction in regression - visual]{.r-fit-text}\n\nFitting a model with `age` and `diabetes` as independent predictors (i.e. WITHOUT interaction terms)\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit a model\nmodel_NOinterac <- lm(tot_chol ~ age + diabetes, data = nhanes) \nmodel_NOinterac$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept)          age  diabetesYes \n 4.800011340  0.007491805 -0.317665963 \n```\n\n\n:::\n:::\n\n\n+ Using `geom_smooth` for a visual intuition of a linear relationship\n  + ‚ö†Ô∏è here I consider sample DATA **as a whole** for plotting a smooth line\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(nhanes, \n       aes (x = age, y = tot_chol)) + \n  # For POINTS I split by category (category)\n  geom_point (aes(color = diabetes, \n                  alpha = 0.75),\n              show.legend = c(size = F, alpha = F) )+\n  scale_color_manual(values=c(\"#005ca1\",\"#9b2339\" )) + \n  # For SMOOTHED LINES I take ALL data\n  geom_smooth(colour=\"#BD8723\",  method = lm) \n```\n\n::: {.cell-output-display}\n![Users in two categories are represented points; linear relationship is representated by ONE golden line for ALL SAMPLE](slides_lab03_files/figure-revealjs/unnamed-chunk-66-1.png){width=960}\n:::\n:::\n\n\n\n## [Interaction in regression - visual (RETHINKING)]{.r-fit-text}\nSuppose two separate models were fit for the relationship between total cholesterol and age; one in diabetic individuals and one in non-diabetic individuals.\n\n+ Using `geom_smooth` for a visual intuition of a linear relationship\n  + ‚ö†Ô∏è here I consider sample DATA **as 2 separate groups** for plotting a smooth line\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(nhanes, \n       # For *both POINTS & LINES* I split by category (category)\n       aes (x = age, y = tot_chol, color = diabetes)) + \n  geom_point (aes(alpha = 0.75),\n              show.legend = c(size = F, alpha = F) )+\n  geom_smooth(method = lm)+ \n  scale_color_manual(values=c(\"#005ca1\",\"#9b2339\" ))  \n```\n\n::: {.cell-output-display}\n![Users in two categories are represented points; linear relationship is representated by 2 respective line according to diabetes status... the association has DIFFERENT DIRECTION!](slides_lab03_files/figure-revealjs/unnamed-chunk-67-1.png){width=960}\n:::\n:::\n\n\n\n\n## [Interaction in regression - adding term in model]{.r-fit-text}\n\nLet's rethink the model and consider this new *specification*: \n\n$$E(TotChol) = \\beta_0 + \\beta_1(Age) + \\beta_2(Diabetes) + \\beta_3(Diabetes \\times Age).$$\n\nWhere: \n+ the term $(Diabetes \\times Age)$ is the `interaction term` between `diabetes` status and `age`, and $\\beta_3$ is the coefficient of such interaction term.\n\n+ notice the use of `...*...` in the model syntax\n\n::: {.cell}\n\n```{.r .cell-code}\n#fit a model\nmodel_interac2 <- lm(tot_chol ~ age*diabetes, data = nhanes) \nmodel_interac2$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    (Intercept)             age     diabetesYes age:diabetesYes \n    4.695702513     0.009638183     1.718704342    -0.033451562 \n```\n\n\n:::\n:::\n\n\n## [Interaction in regression - prediction model]{.r-fit-text}\n::: {style=\"font-size: 80%;\"}\nWe obtained this predictive model: \n$$\\widehat{TotChol} = 4.70 + 0.0096(Age) + 0.1.72(Diabetes) - 0.033(Age \\times Diabetes)$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model_interac2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = tot_chol ~ age * diabetes, data = nhanes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3587 -0.7448 -0.0845  0.6307  4.2480 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      4.695703   0.159691  29.405  < 2e-16 ***\nage              0.009638   0.003108   3.101  0.00205 ** \ndiabetesYes      1.718704   0.763905   2.250  0.02492 *  \nage:diabetesYes -0.033452   0.012272  -2.726  0.00665 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.061 on 469 degrees of freedom\n  (27 observations deleted due to missingness)\nMultiple R-squared:  0.03229,\tAdjusted R-squared:  0.0261 \nF-statistic: 5.216 on 3 and 469 DF,  p-value: 0.001498\n```\n\n\n:::\n:::\n\n:::\n\n\n## [Interaction in regression - interactive term interpretation]{.r-fit-text}\n\n::: {style=\"font-size: 90%;\"}\nGiven:  \n\n$\\widehat{TotChol} = 4.70 + 0.0096(Age) + 0.1.72(Diabetes) - 0.033(Age \\times Diabetes)$  \n<br>\nFor diabetics ( `DiabetesYes = 1` ), the model equation is:\n            \n$TotChol_{diab} =  4.70 + 0.0096(Age) + 1.72(1) - 0.034(Age)(1)$ i.e.  \n$TotChol_{diab} =  6.42 - 0.024(Age)$\n\n<br>               \nFor non-diabetics ( `DiabetesYes = 0` ), the model equation is:\n\n$TotChol_{NOdiab} =  4.70 + 0.0096(Age) + 1.72(0) - 0.034(Age)(0)$ i.e.  \n$TotChol_{NOdiab} =  4.70 + 0.0096(Age)$  \n\n:::\n\n## Final thoughts/recommendations\n\n::: {style=\"font-size: 85%;\"}\n\n+ The analyses proposed in this Lab are very similar to the process we go through in real life. The following steps are always included:\n  + Thorough **understanding of the input data** and the data collection process \n  + Bivariate **analysis of correlation / association** to form an intuition of which explanatory variable(s) may or may not affect the response variable \n  + **Diagnostic plots** to verify if the necessary assumptions are met for a linear model to be suitable \n  + Upon verifying the assumptions, we **fit data** to hypothesized (linear) model \n  + **Assessment of the model performance** ($R^2$, $Adj. R^2$, $F-Statistic$, etc.)\n\n+ As we saw with hypothesis testing, the **assumptions** we make (and require) for regression are of utter importance\n\n+ Clearly, we only scratched the surface in terms of all the possible predictive models, but we got a hang of the **fundamental steps** and some **useful tools** that might serve us also in more advanced analysis \n  + e.g. `broom` (within `tidymodels`), `performace` `rstatix`, `lmtest`\n:::\n  \n",
    "supporting": [
      "slides_lab03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/tabwid-1.1.3/tabwid.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/tabwid-1.1.3/tabwid.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}