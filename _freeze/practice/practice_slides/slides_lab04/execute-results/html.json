{
  "hash": "284c341a92efef520b882ede241760b2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 5: Intro to Machine Learning\"\nsubtitle: \"<span style='font-size:2em;'> Practice session covering topics discussed in Lecture 4 </span>\"\nauthor: \"<a href='https://r4biostats.com/me.html' style='color:#72aed8;font-weight:600;'>M. Chiara Mimmi, Ph.D.</a>&ensp;|&ensp;Universit√† degli Studi di Pavia\"\ndate: 2024-07-27\ndate-format: long\ncode-link: true\nformat:\n  revealjs:\n    smaller: true\n    scrollable: true\n    theme: ../../theme/slidesMine.scss # QUARTO LOOKS IN SAME FOLDER \n#    logo: imgs_slides/mitgest_logo.png\n    footer: '[R 4 Biostatistics](https://r4biostats.com/) | MITGEST::training(2024)'\n#    footer: <https://lulliter.github.io/R4biostats/lectures.html>\n## ------------- x salvare come PDF \n    standalone: false\n    ## -------Produce a standalone HTML file with no external dependencies,\n    embed-resources: true\n    transition: fade\n    background-transition: fade\n    highlight-style: ayu-mirage\n    slide-number: true\n    fig-cap-location: top\n    # fig-format: svg\n    pdf-separate-fragments: false\n    # fig-align: center\nexecute:\n  # Quarto pre code blocks do not echo their source code by default\n  echo: true\n  include: true\n  freeze: auto\nbibliography: ../../bib/R4biostats.bib\ncsl: ../../bib/apa-6th-edition.csl \nsuppress-bibliography: true\n---\n\n\n# [GOAL OF TODAY'S PRACTICE SESSION]{.r-fit-text}\n\n::: {style=\"font-size: 100%;\"}\n::: {style=\"color:#77501a\"}\n+ Revisit PCA algorithm explored via MetaboAnalyst, to learn how we can compute it with R \n+ Understand some key elements of statistical Power Analysis\n+ Introduce how ML approaches deal with available data  \n:::\n:::\n\n<br><br>\n\n::: {style=\"font-size: 70%;\"}\nThe examples and datasets in this Lab session follow very closely two sources:\n\n1. The tutorial on \"Principal Component Analysis (PCA) in R\" by: [Statistics Globe](https://statisticsglobe.com/principal-component-analysis-r)\n2. The materials in support of the \"Core Statistics using R\" course by: [Martin van Rongen](https://github.com/mvanrongen/corestats-in-r_tidyverse)\n<!-- https://mvanrongen.github.io/corestats-in-r_tidyverse/power-analysis.html -->\n:::\n\n\n## Topics discussed in Lecture # 5\n\n\n::: {style=\"font-size: 95%;\"}\n**Lecture 5: topics** \n<!-- + Shifting the emphasis on **empirical prediction**  -->\n<!--   + Distinction between supervised & **unsupervised** algorithms -->\n<!--     + Unsupervised ML Example -->\n<!--       + PCA  -->\n\n+ Introduction to `MetaboAnalyst` software\n  + A useful R-based resources for metabolomics \n+ Elements of statistical Power Analysis\n\n<!-- + Workshop Conclusions -->\n\n:::  \n\n# R ENVIRONMENT SET UP & DATA\n\n## Needed R Packages\n::: {style=\"font-size: 85%;\"}\n\n+ We will use functions from packages `base`, `utils`, and `stats` (pre-installed and pre-loaded) \n+ We may also use the packages below (specifying `package::function` for clarity).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load pckgs for this R session\n\n# --- General \nlibrary(here)     # tools find your project's files, based on working directory\nlibrary(dplyr)    # A Grammar of Data Manipulation\nlibrary(skimr)    # Compact and Flexible Summaries of Data\nlibrary(magrittr) # A Forward-Pipe Operator for R \nlibrary(readr)    # A Forward-Pipe Operator for R \n\n# Plotting & data visualization\nlibrary(ggplot2)      # Create Elegant Data Visualisations Using the Grammar of Graphics\nlibrary(ggfortify)     # Data Visualization Tools for Statistical Analysis Results\nlibrary(scatterplot3d) # 3D Scatter Plot\n\n# --- Statistics\nlibrary(MASS)       # Support Functions and Datasets for Venables and Ripley's MASS\nlibrary(factoextra) # Extract and Visualize the Results of Multivariate Data Analyses\nlibrary(FactoMineR) # Multivariate Exploratory Data Analysis and Data Mining\nlibrary(rstatix)    # Pipe-Friendly Framework for Basic Statistical Tests\n\n# --- Tidymodels (meta package)\nlibrary(rsample)    # General Resampling Infrastructure  \nlibrary(broom)      # Convert Statistical Objects into Tidy Tibbles\n```\n:::\n\n\n<!-- # Data  -->\n<!-- # devtools::install_github(\"OI-Biostat/oi_biostat_data\") -->\n<!-- #library(oibiostat) # Data Package for OpenIntro Biostatistics  -->\n:::\n\n# DATASETS for today\n\n<br>\n\n::: {style=\"font-size: 80%;\"}\nIn this tutorial, we will use: \n\n+ the biopsy data attached to the [`MASS` package](https://cran.r-project.org/web/packages/MASS/MASS.pdf). \n+ a few clean datasets used in the \"Core Statistics using R\" course by: [Martin van Rongen](https://github.com/mvanrongen/corestats-in-r_tidyverse)\n\n:::\n\n\n## [Dataset on Breast Cancer Biopsy]{.r-fit-text}\n\n::: {style=\"font-size: 95%;\"}\n**Name**: Biopsy Data on Breast Cancer Patients  \n**Documentation**: See reference on the data downloaded and conditioned for R here [https://cran.r-project.org/web/packages/MASS/MASS.pdf](https://cran.r-project.org/web/packages/MASS/MASS.pdf)  \n**Sampling details**: This breast cancer database was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. He assessed biopsies of breast tumours for 699 patients up to 15 July\n1992; each of nine attributes has been scored on a scale of 1 to 10, and the outcome is also known. The dataset contains the original Wisconsin breast cancer data with 699 observations on 11 variables.  \n::: \n\n## [Importing Dataset `biopsy`]{.r-fit-text}\n\n<!-- + **[Option 1]** the data can be directly obtained form the `MASS` R package -->\n<!--   + Adapting the function `here` to match your own folder structure -->\n\n<!-- <!-- FATTO IO MA LORO NON VEDONO -->  \n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- #| output: false -->\n<!-- #| echo: false -->\n\n<!-- # (after loading pckg) -->\n<!-- library(MASS) # Support Functions and Datasets for Venables and Ripley's MASS -->\n<!-- # I can call  -->\n<!-- utils::data(biopsy) -->\n\n<!-- # li salvo nel mio folder per poi darglieli  -->\n<!-- readr::write_csv(biopsy, file = here::here(\"practice\", \"data_input\", \"04_datasets\", -->\n<!--                                       \"biopsy.csv\")) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- #| output: false -->\n<!-- #| echo: true -->\n\n<!-- # (Check my working directory location) -->\n\n<!-- # Use `here` in specifying all the subfolders AFTER the working directory  -->\n<!-- biopsy2 <- readr::read_csv(file = here::here(\"practice\", \"data_input\", \"04_datasets\", -->\n<!--                                       \"biopsy.csv\") ,  -->\n<!--                           show_col_types = FALSE, -->\n<!--                           col_types = c(\"c\", # ID,  -->\n<!--                                          \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\",   #V1:V9  -->\n<!--                                          \"f\"  #class  -->\n<!-- )) -->\n<!-- ``` -->\n\n+ The data can be interactively obtained form the `MASS` R package\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# (after loading pckg)\n# library(MASS)  \n\n# I can call \nutils::data(biopsy)\n```\n:::\n\n\n \n## [`biopsy` variables with description]{.r-fit-text}\n\n::: {style=\"font-size: 80%;\"}\n<!-- [[EXCERPT: see complete file in Input Data Folder]]{style=\"color:#77501a\"} -->\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Variable </th>\n   <th style=\"text-align:left;\"> Type </th>\n   <th style=\"text-align:left;\"> Description </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> ID </td>\n   <td style=\"text-align:left;\"> character </td>\n   <td style=\"text-align:left;\"> Sample ID </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V1 </td>\n   <td style=\"text-align:left;\"> integer 1 - 10 </td>\n   <td style=\"text-align:left;\"> clump thickness </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V2 </td>\n   <td style=\"text-align:left;\"> integer 1 - 10 </td>\n   <td style=\"text-align:left;\"> uniformity of cell size </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V3 </td>\n   <td style=\"text-align:left;\"> integer 1 - 10 </td>\n   <td style=\"text-align:left;\"> uniformity of cell shape </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V4 </td>\n   <td style=\"text-align:left;\"> integer 1 - 10 </td>\n   <td style=\"text-align:left;\"> marginal adhesion </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V5 </td>\n   <td style=\"text-align:left;\"> integer 1 - 10 </td>\n   <td style=\"text-align:left;\"> single epithelial cell size </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V6 </td>\n   <td style=\"text-align:left;\"> integer 1 - 10 </td>\n   <td style=\"text-align:left;\"> bare nuclei (16 values are missing) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V7 </td>\n   <td style=\"text-align:left;\"> integer 1 - 10 </td>\n   <td style=\"text-align:left;\"> bland chromatin </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V8 </td>\n   <td style=\"text-align:left;\"> integer 1 - 10 </td>\n   <td style=\"text-align:left;\"> normal nucleoli </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> V9 </td>\n   <td style=\"text-align:left;\"> integer 1 - 10 </td>\n   <td style=\"text-align:left;\"> mitoses </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> class </td>\n   <td style=\"text-align:left;\"> factor </td>\n   <td style=\"text-align:left;\"> benign or malignant </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n:::\n\n## [`biopsy` variables exploration 1/2]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\nThe `biopsy` data contains **699 observations of 11 variables**. \n\nThe dataset also contains a character variable: `ID`, and a factor variable: `class`, with two levels (\"benign\" and \"malignant\").\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# check variable types\nstr(biopsy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t699 obs. of  11 variables:\n $ ID   : chr  \"1000025\" \"1002945\" \"1015425\" \"1016277\" ...\n $ V1   : int  5 5 3 6 4 8 1 2 2 4 ...\n $ V2   : int  1 4 1 8 1 10 1 1 1 2 ...\n $ V3   : int  1 4 1 8 1 10 1 2 1 1 ...\n $ V4   : int  1 5 1 1 3 8 1 1 1 1 ...\n $ V5   : int  2 7 2 3 2 7 2 2 2 2 ...\n $ V6   : int  1 10 2 4 1 10 10 1 1 1 ...\n $ V7   : int  3 3 3 3 3 9 3 3 1 2 ...\n $ V8   : int  1 2 1 7 1 7 1 1 1 1 ...\n $ V9   : int  1 1 1 1 1 1 1 1 5 1 ...\n $ class: Factor w/ 2 levels \"benign\",\"malignant\": 1 1 1 1 1 2 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n:::\n\n## [`biopsy` variables exploration 2/2]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\nThere is also one incomplete variable `V6` \n\n  + remember the package `skimr` for exploring a dataframe?\n\n::: {.cell}\n\n```{.r .cell-code}\n# check if vars have missing values\nbiopsy %>% \n  # select only variables starting with \"V\"\n  skimr::skim(starts_with(\"V\")) %>%\n  dplyr::select(skim_variable, \n                n_missing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 √ó 2\n  skim_variable n_missing\n  <chr>             <int>\n1 V1                    0\n2 V2                    0\n3 V3                    0\n4 V4                    0\n5 V5                    0\n6 V6                   16\n7 V7                    0\n8 V8                    0\n9 V9                    0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n:::\n\n## [`biopsy` dataset manipulation]{.r-fit-text}\n\nWe will: \n\n+ exclude the non-numerical variables (`ID` and `class`) before conducting the PCA.   \n+ exclude the individuals with missing values using the `na.omit()` or `filter(complete.cases()` functions.\n\n+ We can do both in 2 equivalent ways:\n\n<br> \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n#### with `base` R (more compact)\n\n::: {.cell}\n\n```{.r .cell-code}\n# new (manipulated) dataset \ndata_biopsy <- na.omit(biopsy[,-c(1,11)])\n```\n:::\n\n:::\n  \n::: {.column width=\"50%\"}\n#### with `dplyr` (more explicit)\n\n::: {.cell}\n\n```{.r .cell-code}\n# new (manipulated) dataset \ndata_biopsy <- biopsy %>% \n  # drop incomplete & non-integer columns\n  dplyr::select(-ID, -class) %>% \n  # drop incomplete observations (rows)\n  dplyr::filter(complete.cases(.))\n```\n:::\n\n:::\n  \n::::\n\n\n## [`biopsy` dataset manipulation]{.r-fit-text}\n\nWe obtained a new dataset with 9 variables and 683 observations (instead of the original 699).  \n\n::: {.cell}\n\n```{.r .cell-code}\n# check reduced dataset \nstr(data_biopsy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t683 obs. of  9 variables:\n $ V1: int  5 5 3 6 4 8 1 2 2 4 ...\n $ V2: int  1 4 1 8 1 10 1 1 1 2 ...\n $ V3: int  1 4 1 8 1 10 1 2 1 1 ...\n $ V4: int  1 5 1 1 3 8 1 1 1 1 ...\n $ V5: int  2 7 2 3 2 7 2 2 2 2 ...\n $ V6: int  1 10 2 4 1 10 10 1 1 1 ...\n $ V7: int  3 3 3 3 3 9 3 3 1 2 ...\n $ V8: int  1 2 1 7 1 7 1 1 1 1 ...\n $ V9: int  1 1 1 1 1 1 1 1 5 1 ...\n```\n\n\n:::\n:::\n\n\n# PCA: EXAMPLE of UNSUPERVISED ML ALGORITHM\n\nReducing high-dimensional data to a lower number of variables\n<!-- 1) PCA fatta a mano. -->\n<!-- PCA step by step come in Statology ma con il data set della Lecture nmr_bins‚Ä¶csv  -->\n\n<!-- https://www.statology.org/principal-components-analysis-in-r/ -->\n\n<!-- Probabilmente non viene proprio uguale perch√® in MA fa normalizzazione e scaling mentre Statology fa solo scaling, ma fa niente, diciamo che ci serve per vedere la differenza -->\n\n\n## Calculate Principal Components\n\nThe first step of PCA is to calculate the principal components. To accomplish this, we use the `prcomp()` function from the `stats` package.  \n\n+ With argument `‚Äúscale = TRUE‚Äù` each variable in the biopsy data is scaled to have a mean of `0` and a standard deviation of `1` before calculating the principal components (just like option `Autoscaling` in MetaboAnalyst)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate principal component\nbiopsy_pca <- prcomp(data_biopsy, \n                     # standardize variables\n                     scale = TRUE)\n```\n:::\n\n\n\n## Analyze Principal Components\n\nLet‚Äôs check out the elements of our obtained `biopsy_pca` object \n\n  + (All accessible via the  `$` operator)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(biopsy_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n```\n\n\n:::\n:::\n\n\n**\"sdev\"** = the standard deviation of the principal components\n\n**\"sdev\"\\^2** = the variance of the principal components (**eigenvalues** of the covariance/correlation matrix)\n\n**\"rotation\"** = the matrix of variable **loadings** (i.e., a matrix whose columns contain the **eigenvectors**).\n\n**\"center\"** and **\"scale\"** = the means and standard deviations of the original variables before the transformation;\n\n**\"x\"** = the principal component scores (after PCA the observations are expressed in principal component scores)\n\n## Analyze Principal Components (cont.)\n\n::: {style=\"font-size: 90%;\"}\nWe can see the summary of the analysis using the `summary()` function\n\n1. The first row gives the **Standard deviation** of each component, which can also be retrieved via `biopsy_pca$sdev`. \n2. The second row shows the **Proportion of Variance**, i.e. the percentage of explained variance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(biopsy_pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1     PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     2.4289 0.88088 0.73434 0.67796 0.61667 0.54943 0.54259\nProportion of Variance 0.6555 0.08622 0.05992 0.05107 0.04225 0.03354 0.03271\nCumulative Proportion  0.6555 0.74172 0.80163 0.85270 0.89496 0.92850 0.96121\n                           PC8     PC9\nStandard deviation     0.51062 0.29729\nProportion of Variance 0.02897 0.00982\nCumulative Proportion  0.99018 1.00000\n```\n\n\n:::\n:::\n\n:::\n\n\n## [Proportion of Variance for components]{.r-fit-text}\n\n2. The row with **Proportion of Variance** can be either accessed from summary or calculated as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# a) Extracting Proportion of Variance from summary\nsummary(biopsy_pca)$importance[2,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    PC1     PC2     PC3     PC4     PC5     PC6     PC7     PC8     PC9 \n0.65550 0.08622 0.05992 0.05107 0.04225 0.03354 0.03271 0.02897 0.00982 \n```\n\n\n:::\n\n```{.r .cell-code}\n# b) (same thing)\nround(biopsy_pca$sdev^2 / sum(biopsy_pca$sdev^2), digits = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.65550 0.08622 0.05992 0.05107 0.04225 0.03354 0.03271 0.02897 0.00982\n```\n\n\n:::\n:::\n\n\n<br>\n\n> The output suggests the **1st principal component** explains around 65% of the total variance, the **2nd principal component** explains about 9% of the variance, and this goes on with diminishing proportion for each component. \n\n\n## [Cumulative Proportion of variance for components]{.r-fit-text}\n\n3. The last row from the `summary(biopsy_pca)`, shows the **Cumulative Proportion** of variance, which calculates the cumulative sum of the Proportion of Variance. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extracting Cumulative Proportion from summary\nsummary(biopsy_pca)$importance[3,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    PC1     PC2     PC3     PC4     PC5     PC6     PC7     PC8     PC9 \n0.65550 0.74172 0.80163 0.85270 0.89496 0.92850 0.96121 0.99018 1.00000 \n```\n\n\n:::\n:::\n\n\n<br>\n\n> Once you computed the PCA in R you must decide the number of components to retain based on the obtained results.\n\n\n# VISUALIZING PCA OUTPUTS\n\n## Scree plot\n\nThere are several ways to decide on the number of components to retain. \n\n+ One helpful option is visualizing the percentage of explained variance per principal component via a **scree plot**. \n  + Plotting with the `fviz_eig()` function from the `factoextra` package\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# Scree plot shows the variance of each principal component \nfactoextra::fviz_eig(biopsy_pca, \n                     addlabels = TRUE, \n                     ylim = c(0, 70))\n```\n\n::: {.cell-output-display}\n![The obtained **scree plot** simply visualizes the output of `summary(biopsy_pca)`.](slides_lab04_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n<br> \n\n> Visualization is essential in the interpretation of PCA results. Based on the number of retained principal components, which is usually the first few, the observations expressed in component scores can be plotted in several ways.\n\n## [Principal Component `Scores`]{.r-fit-text}\n\nAfter a PCA, the observations are expressed as **principal component scores**.   \n\n1. We can retrieve the principal component scores for each Variable by calling `biopsy_pca$x`, and  store them in a new dataframe `PC_scores`.\n2. Next we draw a `scatterplot` of the observations -- expressed in terms of principal components \n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# Create new object with PC_scores\nPC_scores <- as.data.frame(biopsy_pca$x)\nhead(PC_scores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        PC1         PC2         PC3         PC4         PC5         PC6\n1  1.469095 -0.10419679  0.56527102 -0.03193593  0.15088743 -0.05997679\n2 -1.440990 -0.56972390 -0.23642767 -0.47779958 -1.64188188  0.48268150\n3  1.591311 -0.07606412 -0.04882192 -0.09232038  0.05969539  0.27916615\n4 -1.478728 -0.52806481  0.60260642  1.40979365  0.56032669 -0.06298211\n5  1.343877 -0.09065261 -0.02997533 -0.33803588  0.10874960 -0.43105416\n6 -5.010654 -1.53379305 -0.46067165  0.29517264 -0.39155544 -0.11527442\n         PC7        PC8          PC9\n1 -0.3491471  0.4200360  0.005687222\n2  1.1150819  0.3792992 -0.023409926\n3 -0.2325697  0.2096465 -0.013361828\n4  0.2109599 -1.6059184 -0.182642900\n5 -0.2596714  0.4463277  0.038791241\n6 -0.3842529 -0.1489917  0.042953075\n```\n\n\n:::\n:::\n\n\nIt is also important to visualize the observations along the new axes (principal components) to interpret the relations in the dataset:\n\n## [Principal Component `Scores` plot (adding label variable)]{.r-fit-text}\n\n3. When data includes a factor variable, like in our case, it may be interesting to show the grouping on the plot as well.\n\n  + In such cases, the label variable `class` can be added to the PC set as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# retrieve class variable\nbiopsy_no_na <- na.omit(biopsy)\n# adding class grouping variable to PC_scores\nPC_scores$Label <- biopsy_no_na$class\n```\n:::\n\n\n<br>\nThe visualization of the observation points (point cloud) could be in 2D or 3D.\n\n## [Principal Component `Scores` plot (2D)]{.r-fit-text}\n\nThe Scores Plot can be visualized via the `ggplot2` package. \n\n+ grouping is indicated by argument the `color = Label`; \n+ `geom_point()` is used for the point cloud.\n\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(PC_scores, \n       aes(x = PC1, \n           y = PC2, \n           color = Label)) +\n  geom_point() +\n  scale_color_manual(values=c(\"#245048\", \"#CC0066\")) +\n  ggtitle(\"Figure 1: Scores Plot\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Figure 1 shows the observations projected into the new data space made up of principal components](slides_lab04_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n\n\n## [Principal Component `Scores` (2D Ellipse Plot)]{.r-fit-text}\n\nConfidence ellipses can also be added to a grouped scatter plot visualized after a PCA. We use the `ggplot2` package. \n\n+ grouping is indicated by argument the `color = Label`; \n+ `geom_point()` is used for the point cloud; \n+ the `stat_ellipse()` function is called to add the ellipses per biopsy group.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nggplot(PC_scores, \n       aes(x = PC1, \n           y = PC2, \n           color = Label)) +\n  geom_point() +\n  scale_color_manual(values=c(\"#245048\", \"#CC0066\")) +\n  stat_ellipse() + \n  ggtitle(\"Figure 2: Ellipse Plot\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![Figure 2 shows the observations projected into the new data space made up of principal components, with 95% confidence regions displayed.](slides_lab04_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n\n\n## [Principal Component `Scores` plot (3D)]{.r-fit-text}\n\n::: {style=\"font-size: 80%;\"}\nA 3D scatterplot of observations shows the first **3 principal components‚Äô scores**. \n\n+ For this one, we need the `scatterplot3d()` function of the `scatterplot3d` package;\n+ The color argument assigned to the Label variable;\n+ To add a legend, we use the `legend()` function and specify its coordinates via the `xyz.convert()` function.\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# 3D scatterplot ...\nplot_3d <- with(PC_scores, \n                scatterplot3d::scatterplot3d(PC_scores$PC1, \n                                             PC_scores$PC2, \n                                             PC_scores$PC3, \n                                             color = as.numeric(Label), \n                                             pch = 19, \n                                             main =\"Figure 3: 3D Scatter Plot\", \n                                             xlab=\"PC1\",\n                                             ylab=\"PC2\",\n                                             zlab=\"PC3\"))\n\n# ... + legend\nlegend(plot_3d$xyz.convert(0.5, 0.7, 0.5), \n       pch = 19, \n       yjust=-0.6,\n       xjust=-0.9,\n       legend = levels(PC_scores$Label), \n       col = seq_along(levels(PC_scores$Label)))\n```\n\n::: {.cell-output-display}\n![Figure 3 shows the observations projected into the new 3D data space made up of principal components.](slides_lab04_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n\n:::\n\n## [Biplot: principal components v. original variables]{.r-fit-text}\n\nNext, we create another special type of scatterplot (a **biplot**) to understand the relationship between the principal components and the original variables.  \nIn the `biplot` each of the observations is projected onto a scatterplot that uses the ***first and second principal components as the axes***.\n\n+ For this plot, we use the `fviz_pca_biplot()` function from the `factoextra` package \n  + We will specify the color for the variables, or rather, for the \"loading vectors\"\n  + The `habillage` argument allows to highlight with color the grouping by `class`\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\nfactoextra::fviz_pca_biplot(biopsy_pca, \n                repel = TRUE,\n                col.var = \"black\",\n                habillage = biopsy_no_na$class,\n                title = \"Figure 4: Biplot\", geom=\"point\")\n```\n\n::: {.cell-output-display}\n![The axes show the principal component scores, and the vectors are the loading vectors](slides_lab04_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n## Interpreting biplot output\n::: {style=\"font-size: 95%;\"}\nBiplots have two key elements: **scores** (the 2 axes) and **loadings** (the vectors). \nAs in the scores plot, each point represents an observation projected in the space of principal components where:\n\n+ Biopsies of the same class are located closer to each other, which indicates that they have similar **scores**  referred to the 2 main principal components; \n+ The **loading vectors** show strength and direction of association of original variables with new PC variables.\n\n> As expected from PCA, the single `PC1` accounts for variance in almost all original variables, while `V9` has the major projection along `PC2`.\n\n:::\n\n## Interpreting biplot output (cont.)\n \n\n::: {.cell}\n\n```{.r .cell-code}\nscores <- biopsy_pca$x\n\nloadings <- biopsy_pca$rotation\n# excerpt of first 2 components\nloadings[ ,1:2] \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          PC1         PC2\nV1 -0.3020626 -0.14080053\nV2 -0.3807930 -0.04664031\nV3 -0.3775825 -0.08242247\nV4 -0.3327236 -0.05209438\nV5 -0.3362340  0.16440439\nV6 -0.3350675 -0.26126062\nV7 -0.3457474 -0.22807676\nV8 -0.3355914  0.03396582\nV9 -0.2302064  0.90555729\n```\n\n\n:::\n:::\n\n\n<!-- # PLS-DA: step by step (example) -->\n<!-- 1) PCA + PLS_DA + CLuster  -->\n<!-- https://rpubs.com/Anita_0736/PD_ANALYSIS  -->\n\n<!-- 2) PLS fatta a mano -->\n<!-- PLS step by step come in Statology ma con il data set della Lecture nmr_bins‚Ä¶csv  -->\n\n<!-- https://www.statology.org/partial-least-squares-in-r/ -->\n\n<!-- In MetaboAnalyst usano la PLS-DA che non so cosa ha di diverso ma pu√≤ essere anche carino vedere la differenza -->\n\n\n\n<!-- # ML WITH UNSUPERVISED ALGORITHMS -->\n\n<!-- # Hierarchical Clustering (example) -->\n<!-- 3) Hierarchical Clustering fatto a mano come in Statology ma con il data set della Lecture nmr_bins‚Ä¶csv  -->\n\n<!-- https://www.statology.org/hierarchical-clustering-in-r/ -->\n\n<!-- Se non hai tempo o non si riesce l‚Äôalternativa √® che li faccio giocare anche loro con il MetaboAnalyst anche nelle esercitazioni, sperando che la rete regga e la piattaforma pure.. -->\n\n\n\n<!-- # _______   -->\n\n<!-- ## Fonti ...    -->\n\n<!--  + **Cocca**  https://www.statmethods.net/stats/power.html -->\n\n<!--  + **CORESTATS 6 !!!** https://mvanrongen.github.io/corestats-in-r_tidyverse/power-analysis.html  -->\n\n<!--  + **G*Power (free application)** https://www.linkedin.com/learning/the-data-science-of-experimental-design/installing-g-power?resume=false -->\n<!--  + **library(pwrss)** https://cran.r-project.org/web/packages/pwrss/vignettes/examples.html#7_Analysis_of_(Co)Variance_(F_Test) -->\n\n<!--  + **dakota** https://med.und.edu/research/daccota/_files/pdfs/berdc_resource_pdfs/sample_size_r_module.pdf -->\n<!--  + **!!! Salvatore Mangiafico**  https://rcompanion.org/rcompanion/d_02.html  -->\n<!--   + each test ends with power analsyis  -->\n\n\n# POWER ANALYSIS \n\n\n<br>\n\n::: {style=\"font-size: 80%;\"}\nIn this section, we will use: \n\n+ the *NHANES* clinical data, we already analysed in Lab # 3 \n+ a few, tidy *\"fish-related\"* datasets üç•ü¶ë üê† üç§ üéè  that we will load on the go\n  + Source: the materials of the \"Core Statistics using R\" by: [Martin van Rongen](https://github.com/mvanrongen/corestats-in-r_tidyverse)\n\n:::\n\n\n\n\n## Sample Size determination in Inferential statistics \n\n<br><br>\n\n::: {.r-fit-text}\n::: {style=\"font-size: 80%;\"}\n\n> *\"OK, so how big of a sample do I need?\"* <br><br>\n> ...the 1,000,000 $ question\"! üôÄ\n\n:::\n:::\n\n<!-- http://www.biostathandbook.com/power.html -->\n\n## [Purpose and challenges of Power Analysis]{.r-fit-text}\n\n::: {style=\"font-size: 85%;\"}\n\n+ `Power analysis` helps with the key question **How many observations/subjects do I need for my experiment?** (= $n$)\n  + **Too small** of a sample size can under detect the effect of interest in your experiment\n  + **Too large** of a sample size may lead to unnecessary wasting of resources\n  + We strive to have **just the sufficient** number of observations needed to have a good chance of detecting the effect researched. (Even more so in a very time-consuming or expensive experiment.)\n\n+ **When** should we do power analysis?\n\n  + (Ideally), *before* the experiment:  `a priori power analysis` allows *to determine the necessary sample size $n$ of a test*, given a desired $\\alpha$ level, a desired power level ($1- \\beta$), and the size of the effect to be detected (a measure of difference between $H_o$ and $H_1$)\n  + In reality, sometimes you can only do  `post-hoc power analysis` *after* the experiment, so the sample size $n$ is already given. \n    + In this case, given $n$, $\\alpha$, and a specified effect size, the analysis will return the power ($1- \\beta$) of the test, or $\\beta$ (i.e. the probability of Type II error = incorrectly retaining $H_o$).\n\n<!-- +  $\\alpha$ = type I error = probability of incorrectly rejecting $H_o$ (false positive) -->\n<!-- +  $\\beta$  = type II error = probability of incorrectly retaining $H_o$ (false negative) -->\n\n:::\n\n\n## [Required inputs to define the sample size `n`]{.r-fit-text}\n\n::: {style=\"font-size: 90%;\"}\n \n+ A specified **effect size** (i.e. the minimum deviation from $H_o$ that you hope to detect for a meaningful result)\n  + The larger the effect size, the easier it is to detect an effect and require fewer obs\n+ [**Standard \\ deviation**]: For *measurement variables*, you also need an estimate of the standard deviation. \n  + As $standard deviation$ gets bigger, it is harder to detect a significant difference, so you'll need a bigger sample size. \n\n<!-- Your estimate of the standard deviation can come from pilot experiments or from similar experiments in the published literature. Your standard deviation once you do the experiment is unlikely to be exactly the same, so your experiment will actually be somewhat more or less powerful than you had predicted. For *nominal variables*, the standard deviation is a simple function of the sample size, so you don't need to estimate it separately. -->\n+ $\\alpha$ is the **significance level** of the test (i.e. *the probability of incorrectly rejecting the null hypothesis (a false positive)*. \n  + Understanding if the test is one-tailed (difference has a direction) or two-tailed\n+ $\\beta$ is *the probability of accepting the null hypothesis, even though it is false (a false negative)*, when the real difference is equal to the minimum effect size. \n  + $1- \\beta$ is the **power of a test** is *the probability of correctly rejecting the null hypothesis (getting a significant result) when the real difference is equal to the minimum effect size*.  \n    + a power of 80% (equivalent to a beta of 20%) is probably the most common, while some people use 50% or 90%\n\n:::\n\n## [Specifying effect size]{.r-fit-text}\n\n::: {style=\"font-size: 75%;\"}\nSo (since $\\alpha$ and $1-\\beta$ are normally set) the key piece of information we need is the **effect size**, which is essentially a function of the difference between the means of the null and alternative hypotheses over the variation (standard deviation) in the data. \n\n> The tricky part is that effect size is related to biological/practical significance rather than statistical significance\n\nHow should you estimate a meaningful `Effect Size`?\n\n+ Use preliminary information in the form of pilot study\n+ Use background information in the form of similar studies in the literature\n+ (With no prior information), make an estimated guess on the effect size expected (see guidelines next)\n\n> Most R functions for sample size only allow you to enter effect size as input\n\n:::\n\n## [Specifying effect size: general guidelines]{.r-fit-text}\n\nAs a general indicative reference, below are the **\"Cohen's Standard Effect Sizes\"** (from statistician Jacob Cohen who came up with a rough set of numerical measures for `‚Äúsmall‚Äù`, `‚Äúmedium‚Äù` and `‚Äúlarge‚Äù` effect sizes that are still in use today)  \n\n![](../../images/EffSize.png)\n\n## The `pwr` package  \n::: {style=\"font-size: 80%;\"}\nThe `pwr` package (develped by St√©phane Champely), implements power analysis as outlined by Cohen (1988). \nThe key arguments of the function `pwr.t.test` are 4 quantities, plus 2 for the test description:\n\n1. `n` = sample size\n2. `d` = effect size (based on Cohen's)\n3. `sig.level` = the desired significance level \n  + The significance level ($\\alpha$)  defaults to 0.05. Therefore, to calculate the significance level, given an effect size, sample size, and power ($1- \\beta$), use the option `\"sig.level=NULL\"`.\n4. `power` = the desired power \n5. `type` =  the type of t-test you will eventually be carrying out (one of `two.sample`, `one.sample` or `paired`)\n6. `alternative` = the type of alternative hypothesis you want to test (one of `two.sided`, `less` or `greater`)\n\n+ The core idea behind its functions is that **you enter 3 of the 4 quantities** (effect size, sample size, significance level, power) **and the 4th is calculated**.\n\n:::\n\n## [One Sample Mean: EXE data]{.r-fit-text}\n::: {style=\"font-size: 95%;\"}\n<!-- https://mvanrongen.github.io/corestats-in-r_tidyverse/power-analysis.html#exercise-one-sample -->\nGOAL: Imagine this is a *pilot study*, in which we tested fish is (on average) different form 20 cm in length. \n\nThe `guanapo_data` dataset contains information on fish lengths from the Guanapo river pilot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data on river fish length \nfishlength_data <- readr::read_csv(here::here(\"practice\", \"data_input\", \"04_datasets\", \n                                              \"fishlength.csv\"),\n                              show_col_types = FALSE)\n\n# Select a portion of the data (i.e. out \"pilot\" experiment) \nguanapo_data <- fishlength_data %>% \n  dplyr::filter(river == \"Guanapo\")\n\n# Pilot experiment data \nnames(guanapo_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"id\"     \"river\"  \"length\"\n```\n\n\n:::\n\n```{.r .cell-code}\nmean_H1 <-  mean(guanapo_data$length) # 18.29655\nmean_H1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18.29655\n```\n\n\n:::\n\n```{.r .cell-code}\nsd_sample <- sd(guanapo_data$length)  # 2.584636\nsd_sample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.584636\n```\n\n\n:::\n:::\n\n:::\n\n## [One Sample Mean t-test: EXAMPLE cont.]{.r-fit-text}\n::: {style=\"font-size: 85%;\"}\nLet's compute the one sample t-test with `stats::t.test` against a hypothetical average fish length ($mean\\_H_o = 20$ )\n\n::: {.cell}\n\n```{.r .cell-code}\n# Hypothetical fish population length mean (H0)\nmean_H0 <- 20\n# one-sample mean t-test \nt_stat <- stats::t.test(x = guanapo_data$length,\n                        mu = mean_H0,\n                        alternative = \"two.sided\")\n# one-sample t-test results\nt_stat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  guanapo_data$length\nt = -3.5492, df = 28, p-value = 0.001387\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.31341 19.27969\nsample estimates:\nmean of x \n 18.29655 \n```\n\n\n:::\n:::\n\n\n+ There appear to be a statistically significant result here: the mean length of the fish appears to be different from 20 cm.\n\nQUESTION: In a new study of the same fish, what sample size `n` would you need to get a comparable result? \n\n:::\n\n## [One Sample Mean t-test: POWER ANALYSIS (`n`)]{.r-fit-text}\n\n::: {style=\"font-size: 90%;\"}\n+ We input Cohen's d (after calculating it manually) following: \n$effect\\ size\\ \\approx \\frac{{Mean}_{H_1}\\ -{\\ Mean}_{H_0}}{Std\\ Dev}$\n\n+ We use `pwr::pwr.t.test` to calculate the minimum sample size `n` required:\n<!-- + I  use `d= 0.50` following Cohen's guidance for a *medium* effect size -->\n \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cohen's d formula \neff_size <- (mean_H1 - mean_H0) / sd_sample # -0.6590669\n\n# power analysis to actually calculate the minimum sample size required:\npwr::pwr.t.test(d = eff_size, \n                sig.level = 0.05, \n                power = 0.8,\n                type = \"one.sample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 20.07483\n              d = 0.6590669\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n  \n> We would need `n = 21` (rounding up) observations for an experiment (e.g. in different river) to detect an effect size as the pilot study at a 5% significance level and 80% power.  \n\n:::\n\n## [One Sample Mean t-test: POWER ANALYSIS, stricter conditions]{.r-fit-text}\nWhat if we wanted the results to be even more stringent? \n\n  + e.g. require higher significance level (0.01) and power (0.90) with the same effect?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# power analysis to actually calculate the minimum sample size required:\npwr::pwr.t.test(d = eff_size, \n                sig.level = 0.01, \n                power = 0.9,\n                type = \"one.sample\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     One-sample t test power calculation \n\n              n = 37.62974\n              d = 0.6590669\n      sig.level = 0.01\n          power = 0.9\n    alternative = two.sided\n```\n\n\n:::\n:::\n\n  \n> This time, we would need `n = 38` observations for an experiment to detect the same effect size at the stricter level of significance and power.\n\n\n## [Two Independent Samples: EXE data]{.r-fit-text}\n<!-- https://mvanrongen.github.io/corestats-in-r_tidyverse/power-analysis.html -->\n::: {style=\"font-size: 90%;\"}\nLet‚Äôs look at the entire `fishlength_data` with the lengths of fish from 2 separate rivers.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Explore complete data \nfishlength_data %>% \n  dplyr::group_by (river) %>% \n  dplyr::summarise (N = n(), \n                    mean_len = mean(length),\n                    sd_len = sd(length)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 √ó 4\n  river       N mean_len sd_len\n  <chr>   <int>    <dbl>  <dbl>\n1 Aripo      39     20.3   1.78\n2 Guanapo    29     18.3   2.58\n```\n\n\n:::\n:::\n\n\nVisualize quickly the 2 samples (rivers) with a boxplot\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# visualize the data\nfishlength_data %>% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![The fish in the 2 samples appear to have different mean length](slides_lab04_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n:::\n\n\n## [Two Independent Samples: t-test]{.r-fit-text}\nLet's confirm it with a two sample t-test against $ùëØ_ùüé$: *The two population means are equal*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform two-samples unpaired test\nfishlength_data %>% \n  rstatix::t_test(length ~ river,\n                  paired = FALSE\n                    )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 √ó 8\n  .y.    group1 group2     n1    n2 statistic    df       p\n* <chr>  <chr>  <chr>   <int> <int>     <dbl> <dbl>   <dbl>\n1 length Aripo  Guanapo    39    29      3.64  46.9 0.00067\n```\n\n\n:::\n:::\n\n\n> The t-test analysis confirms that the difference is significant.\n\n<br>\nQUESTION: Can we use this information to design a more `efficient` experiment? I.e. run an experiment powerful enough to pick up the same observed difference in means but with **fewer observations**?\n\n## [Two Independent Samples: POWER ANALYSIS 1/2]{.r-fit-text}\n\n1. Let's work out exactly the **effect size** of this study by estimating Cohen‚Äôs d using this data.\n  + (We use a function from the package `rstatix::cohens_d` to estimate Cohen's d)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimate cohen's d \nfishlength_data %>%\n  rstatix::cohens_d(length ~ river, var.equal = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 √ó 7\n  .y.    group1 group2  effsize    n1    n2 magnitude\n* <chr>  <chr>  <chr>     <dbl> <int> <int> <ord>    \n1 length Aripo  Guanapo   0.942    39    29 large    \n```\n\n\n:::\n:::\n\n\n> The `effsize` column contains the information that we want, in this case **0.94**\n\n## [Two Independent Samples: POWER ANALYSIS 2/2  (`n`)]{.r-fit-text}\n2. Actually answer the question about **how many fish** we really need to catch in the future\n \n\n::: {.cell}\n\n```{.r .cell-code}\n# run power analysis \npwr::pwr.t.test(d = 0.94, power = 0.8, sig.level = 0.05,\n           type = \"two.sample\", alternative = \"two.sided\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 18.77618\n              d = 0.94\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n \n> The `n` output ( = **19 observations per group**) -as opposed to 39 + 29- would be sufficient if we wanted to confidently detect the difference observed in the previous study  \n\n## [Two Paired Samples: EXE data]{.r-fit-text}\n::: {style=\"font-size: 85%;\"}\nThe `cortisol_data` dataset contains information about cortisol levels measured on 20 participants in the morning and evening\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data \ncortisol_data <- read.csv(file = here::here(\"practice\", \"data_input\", \"04_datasets\", \n                                        \"cortisol.csv\"), \n                          header = TRUE, # 1st line is the name of the variables\n                          sep = \",\", # which is the field separator character.\n                          na.strings = c(\"?\",\"NA\" ), # specific MISSING values  \n                          row.names = NULL) \n\n# Explore data \nnames(cortisol_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"patient_id\" \"time\"       \"cortisol\"  \n```\n\n\n:::\n\n```{.r .cell-code}\ncortisol_data %>% \n  dplyr::group_by (time) %>% \n  dplyr::summarise (\n    N = n(), \n    mean_cort = mean(cortisol),\n    sd_cort = sd(cortisol)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 √ó 4\n  time        N mean_cort sd_cort\n  <chr>   <int>     <dbl>   <dbl>\n1 evening    20      197.    87.5\n2 morning    20      313.    73.8\n```\n\n\n:::\n:::\n\n\n> Notice the difference in the paired sample means is quite large\n\n:::\n\n## [Two Paired Samples t-test: visualization]{.r-fit-text}\n\nVisualize quickly the 2 paired samples (morning and evening) with a boxplot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# visualize the data\ncortisol_data %>% \n  ggplot(aes(x = time, y = cortisol)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![The cortisol levels in the 2 paired amples appear quite different](slides_lab04_files/figure-revealjs/unnamed-chunk-33-1.png){width=960}\n:::\n:::\n\n\n\n## [Two Paired Samples: POWER ANALYSIS (`d`)]{.r-fit-text}\n::: {style=\"font-size: 85%;\"}\nGOAL: Flipping the question, if we know the given `n` (20 patients observed twice): How big should the `effect size` be to be detected at `power` of 0.8 and `significance level` 0.05? \n\n+ We use `pwr::pwr.t.test`, with the argument specification `type = \"paired\"`, but this time to estimate the **effect size**\n<!-- + I  use `d= 0.50` following Cohen's guidance for a *medium* effect size -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# power analysis to actually calculate the effect size at the desired conditions:\npwr::pwr.t.test(n = 20, \n                #d =  eff_size, \n                sig.level = 0.05, \n                power = 0.8,\n                type = \"paired\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Paired t test power calculation \n\n              n = 20\n              d = 0.6604413\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n```\n\n\n:::\n:::\n\n  \n> The functions returns the effect size (Cohen‚Äôs metric): `d = 0.6604413`. So, with this experimental design we would be able to detect a **medium-large effect size**.\n\n:::\n\n## [Two Paired Samples t-test: POWER ANALYSIS on given `n`]{.r-fit-text}\n\nLooking instead at the **actual sample data**, what would be the observed effect size?\n\n+ To compute \"observed `d`\" we can use the function `rstatix::cohens_d` \n\n::: {.cell}\n\n```{.r .cell-code}\nd <- cortisol_data %>% \n  # estimate cohen's d\n  rstatix::cohens_d(cortisol ~ time, paired = TRUE)\n\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 √ó 7\n  .y.      group1  group2  effsize    n1    n2 magnitude\n* <chr>    <chr>   <chr>     <dbl> <int> <int> <ord>    \n1 cortisol evening morning   -1.16    20    20 large    \n```\n\n\n:::\n:::\n\n\nThe obtained `d` (-1.16) is extremely large, so ***we likely have more participants in this study than actually needed*** given such a large effect. \n\n## [Two Paired Samples t-test: POWER ANALYSIS gives sufficient `n`]{.r-fit-text}\n\nLet's re-compute the power analysis, but leave `n` as the unknown quantity, given the effect size (`d`) we have observed  \n\n::: {.cell}\n\n```{.r .cell-code}\n# power analysis to calculate minimunm n given the observed effect size in the sample \npwr::pwr.t.test(# n = 20, \n                d =  -1.16, \n                sig.level = 0.05, \n                power = 0.8,\n                type = \"paired\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Paired t test power calculation \n\n              n = 7.960846\n              d = 1.16\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n```\n\n\n:::\n:::\n\n\n> As a matter of fact, ` n = 8` pairs of observations would have sufficed in this study, given the size of effect we were trying to detect.\n\n\n## [One-way ANOVA test: EXE data]{.r-fit-text}\n::: {style=\"font-size: 85%;\"}\nThe `mussels_data` dataset contains information about the length of the *anterior adductor muscle scar* in the mussel `Mytilus trossulus` across five locations around the world!  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data \nmussels_data <- read.csv(file = here::here(\"practice\", \"data_input\", \"04_datasets\", \n                                        \"mussels.csv\"), \n                          header = TRUE, # 1st line is the name of the variables\n                          sep = \",\", # which is the field separator character.\n                          na.strings = c(\"?\",\"NA\" ), # specific MISSING values  \n                          row.names = NULL) \n\n# Explore data \nnames(mussels_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"length\"   \"location\"\n```\n\n\n:::\n\n```{.r .cell-code}\nstats <- mussels_data %>% \n  dplyr::group_by (location) %>% \n  dplyr::summarise (\n    N = n(), \n    mean_len = mean(length),\n    sd_len = sd(length)) \n\nstats\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 √ó 4\n  location       N mean_len  sd_len\n  <chr>      <int>    <dbl>   <dbl>\n1 Magadan        8   0.0780 0.0129 \n2 Newport        8   0.0748 0.00860\n3 Petersburg     7   0.103  0.0162 \n4 Tillamook     10   0.0802 0.0120 \n5 Tvarminne      6   0.0957 0.0130 \n```\n\n\n:::\n:::\n\n:::\n\n## [One-way ANOVA test: visualization]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\n> There appears to be a noticeable difference in lenght at average measurements *at least* between some of the locations\n\n:::\n\n<!-- ```{r} -->\n<!-- # Levene test for variance equality -->\n<!-- levene <- mussels_data %>%                       # name of the data -->\n<!--   car::leveneTest(length ~ as.factor(location),   # continuous DV ~  group IV -->\n<!--                   data = .,            # pipe the data from above -->\n<!--                   center = mean)       # default is median  -->\n<!-- levene -->\n<!-- ``` -->\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# Visualize the data with a boxplot\nmussels_data %>% \n  ggplot(aes(x = location, y = length)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](slides_lab04_files/figure-revealjs/unnamed-chunk-38-1.png){width=960}\n:::\n:::\n\n\n\n\n## [One-way ANOVA test: EXAMPLE cont.]{.r-fit-text}\n\n::: {style=\"font-size: 75%;\"}\nAssuming we verified the required assumptions, let's run the ANOVA test to confirm the visual intuition \n\n+ With the `stats::aov` followed by the command `summary`  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Summary of test outputs: \nsummary_ANOVA <- summary(stats::aov(length ~ location,\n                   data = mussels_data))\n\n# From which we extract all the output elements \n# F value \nsummary_ANOVA[[1]][[\"F value\"]] # 7.121019\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.121019       NA\n```\n\n\n:::\n\n```{.r .cell-code}\n# p value \nsummary_ANOVA[[1]][[\"Pr(>F)\"]]  # 0.0002812242\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002812242           NA\n```\n\n\n:::\n\n```{.r .cell-code}\n# df of numerator and denominator\nsummary_ANOVA[[1]][[\"Df\"]]      # 4, 34 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  4 34\n```\n\n\n:::\n\n```{.r .cell-code}\n# Sum of Square BETWEEN groups\nSSB <- summary_ANOVA[[1]][[\"Sum Sq\"]][1]  # 0.004519674\n# Sum of Square WITHIN groups\nSSW <- summary_ANOVA[[1]][[\"Sum Sq\"]][2]  # 0.005394906\n```\n:::\n\n\n+ A one-way ANOVA test confirms that **the mean lengths of muscle scar differed significantly between locations** ( F = 7.121, with df = [4, 34], and p = 0.000281).\n\n:::\n\n\n## [One-way ANOVA test: POWER ANALYSIS (`effect`)]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\n\nIn ANOVA it may be tricky to decide what kind of effect size we are looking for: \n  \n  + if we care about an overall significance test, the sample size needed is a function of the standard deviation of the group means\n  + if we're interested in the comparisons of means, there are other ways of expressing the effect size (e.g. a difference between the smallest and largest means)\n\nHere let's consider an overall test in which we could reasonably collect the same n. of observations in each group \n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_loc <- nrow(stats)\n\nmeans_by_loc <- c(0.0780, 0.0748, 0.103, 0.0802, 0.0957)\noverall_mean <-  mean(means_by_loc)\nsd_by_loc <- c(0.0129, 0.00860, 0.0162, 0.0120, 0.0130)\noverall_sd <-  mean(sd_by_loc)\n```\n:::\n\n:::\n\n## [One-way ANOVA test: POWER ANALYSIS (`effect`)]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\n\n::: {.cell}\n\n```{.r .cell-code}\n# Effect Size f formula\nCohen_f = sqrt( sum( (1/n_loc) * (means_by_loc - overall_mean)^2) ) /overall_sd\nCohen_f # EXTREMELY BIG \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.877622\n```\n\n\n:::\n\n```{.r .cell-code}\n# Power analysis with given f \npwr::pwr.anova.test(k = n_loc,\n                    n = NULL,\n                    f = Cohen_f,\n                    sig.level = 0.05,\n                    power = 0.80)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 5\n              n = 4.166759\n              f = 0.877622\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n```\n\n\n:::\n:::\n\n\n> The `n` output ( = **5 observations per group**) -as opposed to >6 per group- would be sufficient if we wanted to confidently detect the difference observed in the previous study  \n\n:::\n\n<!-- ## [One-way ANOVA test: POWER ANALYSIS]{.r-fit-text} -->\n<!-- ::: {style=\"font-size: 95%;\"} -->\n<!-- The effect size for power analysis in ANOVA is the $f$ metric, indicating respectively a small = 0.10, medium = 0.25 and large = 0.40 effect.  -->\n\n<!-- + Let's check out what would be the needed `n` to find a \"medium\" effect in an experiment like the one at hand:   -->\n\n<!-- ```{r} -->\n<!-- pwr::pwr.anova.test(k = 5 , f = 0.25, sig.level = 0.05, power = 0.80) -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- ## Assume we have prior knowledge of the group means: -->\n<!-- groupmeans <- c(0.0780, 0.0748, 0.103, 0.0802, 0.0957) -->\n<!-- power.anova.test(groups = length(groupmeans), -->\n<!--                  between.var = var(groupmeans), -->\n<!--                  within.var = 500, power = .90) # n = 15.18834 -->\n<!-- ``` -->\n\n<!-- ::: -->\n\n<!-- ## [Balanced one way ANOVA (`pwr.anova.test`)]{.r-fit-text} -->\n\n<!-- ## [Two samples with unequal n t-test (`pwr.t2n.test`)]{.r-fit-text} -->\n\n## [Linear Regression with grouped data: EXE data]{.r-fit-text}\n\n::: {style=\"font-size: 75%;\"}\n\nThe ideas covered before apply also to **linear models**, although here:\n\n+ we use `pwr.f2.test()` to do the power calculation\n+ the `effect sizes` ($f^2$) is based on $R^2$\n\n$$ f^2=\\ \\frac{R^2}{1-\\ R^2}$$ \n\n::: {.cell}\n\n```{.r .cell-code}\n# define the linear model\nlm_mussels <- lm(length ~ location, \n                 data = mussels_data)\n```\n:::\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code}\n# summarise the model\nsummary(lm_mussels)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = length ~ location, data = mussels_data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.025400 -0.007956  0.000100  0.007000  0.031757 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.078012   0.004454  17.517  < 2e-16 ***\nlocationNewport    -0.003213   0.006298  -0.510  0.61331    \nlocationPetersburg  0.025430   0.006519   3.901  0.00043 ***\nlocationTillamook   0.002187   0.005975   0.366  0.71656    \nlocationTvarminne   0.017687   0.006803   2.600  0.01370 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0126 on 34 degrees of freedom\nMultiple R-squared:  0.4559,\tAdjusted R-squared:  0.3918 \nF-statistic: 7.121 on 4 and 34 DF,  p-value: 0.0002812\n```\n\n\n:::\n:::\n\n:::\n\n\n## [Linear Regression with grouped data: POWER ANALYSIS]{.r-fit-text}\n::: {style=\"font-size: 95%;\"}\nFrom the linear model we get that the $R^2$ value is 0.4559 and we can use this to calculate Cohen‚Äôs $f^2$ value using the formula \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract R squared\nR_2 <- summary(lm_mussels)$r.squared\n# compute f squared\nf_2 <- R_2 / (1 - R_2)\nf_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.837767\n```\n\n\n:::\n:::\n\n\nOur model has 5 parameters (because we have 5 groups) and so the numerator degrees of freedom $u$ will be 4 (5‚àí1=4). \n\nHence, we carry out the power analysis with the function `pwr.f2.test`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# power analysis for overall linear model \npwr::pwr.f2.test(u = 4, v = NULL, \n                 f2 = 0.8378974,\n                 sig.level = 0.05 , power = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Multiple regression power calculation \n\n              u = 4\n              v = 14.62182\n             f2 = 0.8378974\n      sig.level = 0.05\n          power = 0.8\n```\n\n\n:::\n:::\n\n\n:::\n\n## [Linear Regression with grouped data: POWER ANALYSIS interpret.]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\n\nRecall that, in the F statistic evaluating the model, \n\n+ **u** the df for the numerator: $df_{between} =k‚àí1 = 5-1 = 4$ \n+ **v** the df for the denominator: $df_{within} = n-k = ?$ \n  + so $n = v+5$ \n\n\n::: {.cell}\n\n```{.r .cell-code}\npwr::pwr.f2.test(u = 4, f2 = 0.8378974,\n            sig.level = 0.05 , power = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Multiple regression power calculation \n\n              u = 4\n              v = 14.62182\n             f2 = 0.8378974\n      sig.level = 0.05\n          power = 0.8\n```\n\n\n:::\n:::\n\n\n> This tells us that the denominator degrees of freedom **v** should be 15 (14.62 rounded up), and this means that we would only need 20 observations **n = v+5** in total across all 5 groups to detect this effect size \n\n<!-- (Remember: number of observations = numerator d.f. + denominator d.f. + 1) -->\n:::\n\n\n# SAMPLE SPLITTING IN MACHINE LEARNING\n\n::: {.r-fit-text}\n> Embracing a different philosophical approach... \n\n:::\n\n \n## [2 different approaches with different takes on empirical data]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\n[*(Simplifying a little)*]{style=\"color:#77501a\"}\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n#### Inferential statistics\n+ `GOAL`: Convincingly explain\n\n+ `APPROACH`: Strong emphasis on defining assumptions (about variables distributions) and/or hypotheses on the relationship between them \n\n+ `DATA`: \n  + The **collection strategy is designed *ex-ante* **, according to the experiment goal\n  + Usually, ALL AVAILABLE DATA are used to estimate effect of interest (as `sampling` was designed to be representative of a population).\n  \n:::\n  \n::: {.column width=\"50%\"}\n#### Machine Learning\n+ `GOAL`: Accurately predict\n\n+ `APPROACH`: Focus on labeling observations or uncovering (\"learn\") a pattern, without worrying about explaining them\n\n+ `DATA`: \n  + Data drives the search for patterns, but there is a huge risk of *\"overfitting\"* models (too specific to initial data!)\n  + It is critical to SPLIT THE DATA (usually 75% for `training` and 25% for `testing` the algorithms) **leaving aside a sub-sample to test the model** with unseen new data\n\n:::\n  \n::::\n:::\n\n\n## [Data Splitting in ML approaches]{.r-fit-text}\n\n::: {style=\"font-size: 65%;\"}\n\nConsistent with the ML approach (**learning from (data) examples**), it is critical to split the available data to obtain: \n\n1. 60-80% ‚ûú `training sample` for *fitting a model* and making prediction on the training data itself \n2. 20-40% ‚ûú `testing sample` for *evaluating the performance* of the selected model(s) and test it works on new data too\n  + [*Since in ML we don't claim to know _what works_ in advance, it is essential to \"test\" a candidate predictive model on fresh new data and see if it holds*]{style=\"color:#77501a\"}\n\n![](../../images/split_data.png)\n\n:::\n\n\n\n\n<!-- ## [Correlation: EXE data]{.r-fit-text} -->\n<!-- ::: {style=\"font-size: 95%;\"} -->\n<!-- ::: -->\n<!-- ## [Correlation: EXAMPLE cont.]{.r-fit-text} -->\n<!-- ::: {style=\"font-size: 95%;\"} -->\n<!-- ::: -->\n<!-- ## [Correlation: POWER ANALYSIS]{.r-fit-text} -->\n<!-- ::: {style=\"font-size: 95%;\"} -->\n<!-- ::: -->\n<!-- ## [Chi-Squared Test: EXE data]{.r-fit-text} -->\n<!-- ::: {style=\"font-size: 95%;\"} -->\n<!-- ::: -->\n<!-- ## [Chi-Squared Test: EXAMPLE cont.]{.r-fit-text} -->\n<!-- ::: {style=\"font-size: 95%;\"} -->\n<!-- ::: -->\n<!-- ## [Chi-Squared Test: POWER ANALYSIS]{.r-fit-text} -->\n<!-- ::: {style=\"font-size: 95%;\"} -->\n<!-- ::: -->\n\n## [Introducing R (metapackage) `tidymodels` for modeling and ML]{.r-fit-text}\n\n:::: {.columns}\n\n::: {.column width=\"35%\"}\nThe package `tidymodels` (much like the `tidyverse`) is an ecosystem of packages meant to enable a wide variety of approaches for modeling and statistical analysis.\n\n+ One package in this system is `rsample` is one of its building blocks for resampling data \n<!-- (@fig-tidymodels) -->\n:::\n  \n::: {.column width=\"65%\"}\n![Tidymodels ecosystem](../../images/tidymodels.png)\n<!-- ![Tidymodels ecosystem](../../images/tidymodels.png){#fig-tidymodels} -->\n:::\n::::\n\n\n## [Revisiting NHANES for a quick demonstration of predictive modeling]{.r-fit-text}\n\n::: {style=\"font-size: 95%;\"}\nLet's re-load a dataset from Lab # 3 (the NHANES dataset) for a quick demonstration of data splitting in an ML predictive modeling scenario \n\n+ We can try predicting `BMI` from `age` (in years), `PhysActive`, and `gender`, using linear regression model (which is a `Supervised ML algorithm`)\n\n+ (we already saved this dataset)\n\n::: {.cell}\n\n```{.r .cell-code}\n# (we already saved this dataset in our project folders)\n\n# Use `here` in specifying all the subfolders AFTER the working directory \nnhanes <- read.csv(file = here::here(\"practice\", \"data_input\", \"03_datasets\",\n                                      \"nhanes.samp.csv\"), \n                          header = TRUE, # 1st line is the name of the variables\n                          sep = \",\", # which is the field separator character.\n                          na.strings = c(\"?\",\"NA\" ), # specific MISSING values  \n                          row.names = NULL) \n```\n:::\n\n:::\n\n## Splitting the dataset into training and testing samples\n<!-- Julia Silge https://supervised-ml-course.netlify.app/ -->\n\n+ With this approach, it is best practice to **\"hold back\" some data for testing** to get a better estimate of how models will perform on new data\n \n+ We can easily specify training and testing sets using `rsample`'s function `initial_split`\n\n  <!-- So, when you evaluate your model on data that *it was not trained on*, you get a better estimate of how it will perform on new data. -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ensure we always get the same result when sampling (for convenience )\nset.seed(12345)\n\nnhanes_split <- nhanes %>%\n  # define the training proportion as 75%\n  rsample::initial_split(prop = 0.75,\n  # ensuring both sets are balanced in gender\n                         strata = Gender)\n\n# resulting datasets\nnhanes_train <- rsample::training(nhanes_split)\ndim(nhanes_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 374  77\n```\n\n\n:::\n\n```{.r .cell-code}\nnhanes_test <- rsample::testing(nhanes_split)\ndim(nhanes_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 126  77\n```\n\n\n:::\n:::\n\n\n<!-- https://supervised-ml-course.netlify.app/chapter1 -->\n\n\n## [Fitting a linear model on the training data]{.r-fit-text}\n::: {style=\"font-size: 90%;\"}\n\nIn this case the **regression models** serves for predicting numeric, continuous quantities \n\n::: {.cell}\n\n```{.r .cell-code}\n# fitting  linear regression model specification\nlin_mod <- lm(BMI ~ Age + Gender + PhysActive, data = nhanes_train)\n\nsummary(lin_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = BMI ~ Age + Gender + PhysActive, data = nhanes_train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.685  -4.674  -1.419   4.257  38.016 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   30.14217    1.30426  23.110  < 2e-16 ***\nAge            0.01429    0.02198   0.650  0.51596    \nGendermale    -0.72960    0.72176  -1.011  0.31275    \nPhysActiveYes -2.26539    0.73620  -3.077  0.00225 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.903 on 367 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.03416,\tAdjusted R-squared:  0.02626 \nF-statistic: 4.327 on 3 and 367 DF,  p-value: 0.005155\n```\n\n\n:::\n:::\n\n\n:::\n\n## [Predicting BMI estimates for new data set]{.r-fit-text}\n::: {style=\"font-size: 95%;\"}\nUsing the above model, we can predict the BMI for different individuals (those left in the testing data)\n\n  + with the function `predict`, where we specify the argument `newdata = nhanes_test`)\n  + adding the prediction `interval` (the 95% CI), which gives uncertainty around a single value of the prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtain predictions from the results of a model fitting function\npred_bmi <- stats::predict(lin_mod, \n               newdata = nhanes_test,\n               interval = \"confidence\" )\nhead(pred_bmi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       fit      lwr      upr\n1 28.59148 27.33499 29.84797\n2 27.70464 26.45051 28.95878\n3 30.88546 29.72888 32.04203\n4 28.01911 26.64955 29.38867\n5 29.78421 28.04027 31.52815\n6 27.60459 26.24230 28.96688\n```\n\n\n:::\n:::\n\n\n:::\n\n\n<!-- ## [TIdy]{.r-fit-text} -->\n<!-- ::: {style=\"font-size: 95%;\"} -->\n\n\n<!-- ```{r} -->\n<!-- broom::tidy(lin_mod) -->\n<!-- broom::glance(lin_mod) -->\n<!-- broom::augment(lin_mod) -->\n<!-- ``` -->\n\n<!-- ::: -->\n\n\n## [Evaluating the predictive performance in testing data]{.r-fit-text}\n::: {style=\"font-size: 95%;\"}\n\nThe ultimate goal of holding data back from the model training process was to **evaluate its predictive performance on new data**. \n<!-- We want to be able to estimate how well our model will perform on new data, and the best way to do that is to use data that was not an input to training the model at all.  -->\n\n<!-- + *Specifically, Holding out testing data allows you to assess if your model is overfitting.*  -->\n\nA common measure used is the `RMSE (Root Mean Square Error)` = a measure of the distance between observed values and predicted values **in the testing dataset**  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Computing the Root Mean Square Error\nRMSE_test <- sqrt(mean((nhanes_test$BMI - predict(lin_mod, nhanes_test))^2, na.rm = T))\nRMSE_test # 6.170518\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.170518\n```\n\n\n:::\n:::\n\n\n> The RMSE (= 6.170518) tells us, (roughly speaking) by how much, on average, the new observed BMI values differ from those predicted by our model\n:::\n\n\n## [... and what about RMSE in training data?]{.r-fit-text}\nLet's see the RMSE in the training dataset (for comparison)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRMSE_train <- sqrt(mean((nhanes_train$BMI - predict(lin_mod, nhanes_train))^2, na.rm = T))\nRMSE_train # 6.866044\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.866044\n```\n\n\n:::\n\n```{.r .cell-code}\n# R squared is also quite low \nsummary(lin_mod)$r.squared     # R^2 0.0341589\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0341589\n```\n\n\n:::\n:::\n\n\n> This is not what expected ü§î, since RMSE on the training data is sliglthly bigger that in the testing data! \n\nA possible explanation is that out model is `underfitting` in the first place (model's ${R}^2$ was quite low too), so we should definitely try different models...  \n\n\n# WRAPPING UP TODAY'S KEY MESSAGE \n\n<!-- ## [A conceptual map to understand this sampling size/sample splitting endeavour]{.r-fit-text}  -->\n<!-- ::: {style=\"font-size: 90%;\"} -->\n\n<!-- Without going too in depth, these days have introduced 2 fundamentally different approaches, and (hopefully) given you a general conceptual frame:  -->\n\n<!-- 1. **[Inferential Statistics & classical Hypothesis testing]{style=\"color:#1c7c6b\"}** -->\n<!-- 2. **[Machine Learning for prediction]{style=\"color:#9b4923\"}**   -->\n\n<!-- The 2 approaches: -->\n\n<!-- + operate under diverse `assumptions` (*[1. strict and explicit]{style=\"color:#1c7c6b\"} v. [2. relaxed if at all present]{style=\"color:#9b4923\"}*),  -->\n<!-- + have different `goals` (*[1. explaining with statistical certainty]{style=\"color:#1c7c6b\"} v. [2. uncovering a pattern/classification from data]{style=\"color:#9b4923\"}*)  -->\n<!-- + (therefore) treat `available data` differently (*[1. curate a data generation process that matches hypothesis]{style=\"color:#1c7c6b\"} v. [2. saves a portion of untouched data to test plausible model]{style=\"color:#9b4923\"}*) -->\n\n<!-- ::: -->\n\n\n\n\n## [Recap of the workshop's content]{.r-fit-text}\n\n::: {style=\"font-size: 95%;\"}\n\n**TOPICS WE COVERED**\n\n1. Motivated the choice of learning/using **R for scientific quantitative analysis**, and lay out some fundamental concepts in biostatistics with concrete R coding examples.\n\n2. Consolidated understanding of **inferential statistic**, through R coding examples conducted on real biostatistics research data.\n\n3. Discussed the **relationship between any two variables**, and introduce a widely used analytical tool: **regression**.\n \n4. Presented a popular ML technique for dimensionality reduction (**PCA**), performed both with `MetaboAnalyst` and `R`. \n\n5. Introduction to **power analysis** to define the correct sample size for hypotheses testing and discussion of how ML approaches deal with available data.\n\n\n:::\n\n\n## Final thoughts\n\n::: {style=\"font-size: 95%;\"}\n::: {style=\"color:#77501a\"}\n\n\n+ While the workshop only allowed for a synthetic overview of fundamental ideas, it hopefully provided a solid foundation on the most common statistical analysis you will likely run in your daily work: \n  + Thorough **understanding of the input data** and the data collection process \n  + Univariate and bivariate **exploratory analysis** (accompanied by visual intuition) to form hypothesis \n  + Upon verifying the assumptions, we **fit data** to hypothesized model(s)\n  + **Assessment of the model performance** ($R^2$, $Adj. R^2$, $F-Statistic$, etc.)\n\n\n+ You should now have a solid grasp on the R language to keep using and exploring the huge potential of this programming ecosystem\n\n+ We only scratched the surface in terms of ML classification and prediction models, but we got a hang of the **fundamental steps** and some **useful tools** that might serve us also in more advanced analysis \n\n:::\n\n:::\n\n",
    "supporting": [
      "slides_lab04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}