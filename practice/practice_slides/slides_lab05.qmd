---
title: "Lab 5: Intro to Machine Learning"
subtitle: "<span style='font-size:2em;'> Practice session</span>"
#author: "Luisa M. Mimmi | [https://luisamimmi.org/](https://luisamimmi.org/)"
author: "<span style='font-size:1.3em; font-weight: bold; color:#77501a'> Luisa M. Mimmi — &ensp;</span>  <a href='https://luisamimmi.org/' style='color:#72aed8; font-weight:600; font-size:1.3em;'>https://luisamimmi.org/</a>"
date: today
date-format: long
code-link: true
format:
  revealjs:
    math: mathjax
    smaller: true
    scrollable: true
    theme: ../../theme/slidesMine.scss # QUARTO LOOKS IN SAME FOLDER 
    css: ../../theme/styles.css
#    logo: imgs_slides/mitgest_logo.png
    footer: '[R 4 Statistics]({{< var websites.live_site >}}) | 2025'
#    footer: <https://lulliter.github.io/R4biostats/lectures.html>
## ------------- x salvare come PDF 
    standalone: false
    ## -------Produce a standalone HTML file with no external dependencies,
    embed-resources: true
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    slide-number: true
    fig-cap-location: top
    # fig-format: svg
    pdf-separate-fragments: false
    # fig-align: center
execute:
  # Quarto pre code blocks do not echo their source code by default
  echo: true
  include: true
  freeze: auto
bibliography: ../../bib/R4biostats.bib
csl: ../../bib/apa-6th-edition.csl 
suppress-bibliography: true
---

# GOAL OF TODAY'S PRACTICE SESSION

## Lab # 5
::: {style="font-size: 100%;"}
::: {style="color:#77501a"}

+ In this Lab session, we will focus on **Machine Learning** (ML), as introduced in Lecture 5
+ We will review examples of both **supervised** and **unsupervised** ML algorithms
  +  **Supervised ML algorithms** examples:
      + Logistic regression
      + Classification and regression trees (CART)
      + 🌳 Random Forest classifier
  + **Unsupervised ML algorithms** examples:
      + K-means Clustering
      + PCA for dimension reduction
  + (optional) PLS-DA for classification, a supervised ML alternative to PCA
    
:::
:::

 



## 🟠 ACKNOWLEDGEMENTS

 
The examples and datasets in this Lab session follow very closely two sources:

1. The tutorial on ***"Data Analytics with R"*** by: [Brian Machut, Nathan Cornwell](https://bookdown.org/brianmachut/uofm_analytics_r_hw_sol_2/#acknowledgements)  
 
2. The tutorial on ***"Principal Component Analysis (PCA) in R"*** by: [Statistics Globe](https://statisticsglobe.com/principal-component-analysis-r)

<!-- The materials in support of the "Core Statistics using R" course by: [Martin van Rongen](https://github.com/mvanrongen/corestats-in-r_tidyverse) -->
<!-- POWER ANALYSIS https://mvanrongen.github.io/corestats-in-r_tidyverse/power-analysis.html -->
 

# R ENVIRONMENT SET UP & DATA

## Needed R Packages
::: {style="font-size: 85%;"}

+ We will use functions from packages `base`, `utils`, and `stats` (pre-installed and pre-loaded) 
+ We may also use the packages below (specifying `package::function` for clarity).


```{r}
# Load pckgs for this R session
options(scipen = 999)
# --- General 
library(here)     # tools find your project's files, based on working directory
library(dplyr)    # A Grammar of Data Manipulation
library(skimr)    # Compact and Flexible Summaries of Data
library(magrittr) # A Forward-Pipe Operator for R 
library(readr)    # A Forward-Pipe Operator for R 
library(tidyr)    # Tidy Messy Data
library(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax

# ---Plotting & data visualization
library(ggplot2)      # Create Elegant Data Visualisations Using the Grammar of Graphics
library(ggfortify)     # Data Visualization Tools for Statistical Analysis Results
library(scatterplot3d) # 3D Scatter Plot

# --- Statistics
library(MASS)       # Support Functions and Datasets for Venables and Ripley's MASS
library(factoextra) # Extract and Visualize the Results of Multivariate Data Analyses
library(FactoMineR) # Multivariate Exploratory Data Analysis and Data Mining
library(rstatix)    # Pipe-Friendly Framework for Basic Statistical Tests
library(car)        # Companion to Applied Regression
library(ROCR)       # Visualizing the Performance of Scoring Classifiers

# --- Tidymodels (meta package)
library(rsample)    # General Resampling Infrastructure  
library(broom)      # Convert Statistical Objects into Tidy Tibbles
```

<!-- # Data  -->
<!-- # devtools::install_github("OI-Biostat/oi_biostat_data") -->
<!-- #library(oibiostat) # Data Package for OpenIntro Biostatistics  -->
:::

# LOGISTIC REGRESSION 

(EXAMPLE of SUPERVISED ML ALGORITHM)
<!-- https://openintro-ims.netlify.app/regression-modeling -->
<!-- STATQUEST YT https://www.youtube.com/watch?v=C4N3_XJJ-jU -->
<!-- STATQUEST GH  https://github.com/StatQuest/logistic_regression_demo/blob/master/logistic_regression_demo.R -->
<!-- EQUITABLE EQUATIONS = https://youtu.be/E7J3M1oYVlc?si=zmBIfoqvErliFqU3 -->

<!-- (biopsy) ROTTO!!! https://github.com/sws8/biopsy-analysis/blob/main/biopsy_analysis.pdf -->
<!-- (biopsy) OK!!! https://www.linkedin.com/pulse/logistic-regression-dataset-biopsy-giancarlo-ronci-twpke/ -->


## Logistic regression: review
<!-- GLM intro https://medium.com/@sahin.samia/a-comprehensive-introduction-to-generalized-linear-models-fd773d460c1d -->

::: {style="font-size: 90%;"}
+ Logistic regression is a classification model used with a **binary response variable**, e.g.: 
  + `yes|no`, or `0|1`, or `True|False` in a survey question;
  + `success|failure` in a clinical trial experiment;
  + `benign|malignant` in a biopsy experiment.
+ Logistic regression is a type of **`Generalized Linear Model (GLM)`**: a more _flexible_ version of linear regression that can work also for **categorical response** variables or **count data** (e.g. poisson regression).
+ When logistic regression fits the coefficients $\beta_0$, $\beta_1$, ..., $\beta_k$ to the data, it minimizes errors using the **`Maximum Likelihood Estimation`** method (as opposed to linear regression's `Least Squares Error Estimation` method).


<!-- + `GLMs` (introduced in 1972) provide a unified framework to accommodate response variables that come from a wide range of distributions: -->

<!--   + the `error distribution` (or `family`) of the response variable can be a `normal distribution` (continuous response), but also a `binomial distribution` (binary response), a `Poisson distribution` (count data), etc.  -->
<!--   + the `link function` is used to model the relationship between the predictors and the response variable. For example, in logistic regression, the link function is the `logit` function. -->

  <!-- + GLM can be thought of as a two-stage modeling approach. We first model the response variable using a probability distribution. Second, we model the parameter of the distribution using a collection of predictors and a special form of multiple regression.  -->
  
  <!-- Ultimately, the application of a GLM will feel very similar to multiple regression, even if some of the details are different. -->
:::

## Logistic regression: `logit` function
::: {style="font-size: 85%;"}
If we have predictor variables like $X_{1}$, $X_{2}$, ..., $X_{k}$ and a binary response variable $Y$ (where $y_i = 0$ or $y_i = 1$), we need a **"special" function** to transform the expected value of the response variable into the $[0,1]$ outcome we’re trying to predict. <br><br>

The **`logit` function** (of the GLM family) helps us determine the coefficients $\beta_0$, $\beta_1$, ..., $\beta_k$ that best fit this sort of data and it is defined as:
$$
logit (p_i) = \ln\left( \frac{p_i}{1-p_i} \right)= \beta_0 + \beta_1 x_{1,i} + \cdots + \beta_k x_{k,i}
$$
    where $p_i$ is the probability that $y_i = 1$
<br><br>

Via the **`inverse-logit` function** (`logistic`), we solve for the probability $p_i$, given values of the predictor variables, like so: 

$$
P(y_i = 1 | x_{1,i}, \ldots, x_{k,i} ) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_{1,i} + \cdots + \beta_k x_{k,i})}}
$$

::: aside
*[Predicting probabilities without a `logit` function could give values over 1, which doesn’t make sense.]*
<!-- + Predicting counts without a `log link` could result in negative numbers, which are impossible. -->

:::

:::

# 🟠 DATASETS for today

<br>

::: {style="font-size: 80%;"}
In this tutorial, we will use: 

+ a (toy) `heart_data` dataset from the book by: Brian Machut, Nathan Cornwell [Data Analytics with R](https://bookdown.org/brianmachut/uofm_analytics_r_hw_sol_2/#acknowledgements)  

+ the `biopsy` dataset attached to the [`MASS` package](https://cran.r-project.org/web/packages/MASS/MASS.pdf). 

+ a few clean datasets used in the "Core Statistics using R" course by: [Martin van Rongen](https://github.com/mvanrongen/corestats-in-r_tidyverse)

:::

## [Dataset on Heart Disease (`heart_data`)]{.r-fit-text}

**Name**: `heart_data.csv`  
**Documentation**: Toy dataset prepared for teaching purposes. See reference on the data here [Data Analytics with R](https://bookdown.org/brianmachut/uofm_analytics_r_hw_sol_2/#acknowledgements)  
**Sampling details**: This dataset contains 10,000 observations on 4 variables.  

<br><br>

```{r}
# Use `here` in specifying all the subfolders AFTER the working directory 
heart_data <- read.csv(file = here::here("practice", "data_input", "05_datasets",
                                      "heart_data.csv"), 
                          header = TRUE, # 1st line is the name of the variables
                          sep = ",", # which is the field separator character.
                          na.strings = c("?","NA" ), # specific MISSING values  
                          row.names = NULL) 
```

## `heart_data` variables with description

::: {style="font-size: 80%;"}
<!-- [[EXCERPT: see complete file in Input Data Folder]]{style="color:#77501a"} -->

```{r}
#| output: true
#| echo: false

heart_data_desc <- tibble::tribble(
  ~Variable, ~ Type, ~Description,
#"X" ,  "integer" ,         "row counter ", 
"heart_disease",    "int",  "whether an individual has heart disease (1 = yes; 0 = no)", 
"coffee_drinker",   "int",  "whether an individual drinks coffee regularly (1 = yes; 0 = no)", 
"fast_food_spend",  "dbl",  "a numerical field corresponding to the annual spend of each individual on fast food", 
"income",           "dbl",   "a numerical field corresponding to the individual’s annual income"

)                 

kableExtra::kable(heart_data_desc)
```
:::

## `heart_data` dataset splitting

In Machine Learning, it is good practice to split the data into `training` and `testing` sets.

+ We will use the **training set** (70%) to fit the model and then the **testing set** (30%) to evaluate the model's performance.
```{r}
set.seed(123)

# Obtain 2 sub-samples from the dataset: training and testing
sample  <-  sample(c(TRUE, FALSE), nrow(heart_data), replace = TRUE , prob = c(0.7, 0.3) )
heart_train  <-  heart_data[sample,]
heart_test  <-  heart_data[!sample,]
```

+ Which results in:
```{r}
# check the structure of the resulting datasets
dim(heart_train)
dim(heart_test)
```

## Convert binary variables to factors

Before examining the training dataset `heart_train`, we converting the binary variables `heart_disease` and `coffee_drinker` to factors (for better readability).

```{r}
heart_train <- heart_train %>% 
  # convert to factor with levels "Yes" and "No"
  mutate(heart_disease = factor(heart_disease, levels = c(0, 1),
                                labels = c("No_HD", "Yes_HD")),
         coffee_drinker = factor(coffee_drinker, levels = c(0, 1),
                                 labels = c("No_Coffee", "Yes_Coffee")) 
  )

# show the first 5 rows of the dataset
heart_train[1:5,]
```

## Plotting `Y` by `X1` (continuous variable)
Let's visualize the relationship between the binary outcome variable `heart_disease` and the continuous predictor variable `fast_food_spend`.

+ `geom_jitter` adds a bit of randomness to the points to avoid overplotting.
+ `geom_boxplot` shows the distribution of the continuous variable by the binary outcome variable.

<!-- https://bookdown.org/brianmachut/uofm_analytics_r_hw_sol_2/logreg.html -->

```{r}
#| output: true
#| output-location: slide
#| fig.cap: |
#|   + The boxplots indicate that subjects with heart disease (HD =1) seem to spend higher amounts on fast food <br>
#|   + Also, this sample has many more subjects without heart disease (HD = 0) than with heart disease (HD = 1)

# plot the distribution of heart disease status by fast food spend
heart_train %>% 
  ggplot(aes(x = heart_disease, y = fast_food_spend, fill = heart_disease)) + 
  geom_jitter(aes(fill = heart_disease), alpha = 0.3, shape = 21, width = 0.25) +  
  scale_color_manual(values = c("#005ca1", "#9b2339")) + 
  scale_fill_manual(values = c("#57b7ff", "#e07689")) + 
  geom_boxplot(fill = NA, color = "black", linewidth = .7) + 
  coord_flip() +
    theme(plot.title = element_text(size = 13,face="bold", color = "#873c4a"),
        axis.text.x = element_text(size=12,face="italic"), 
        axis.text.y = element_text(size=12,face="italic"),
        legend.position = "none") + 
  labs(title = "Fast food expenditure by heart disease status") + 
  xlab("Heart Disease (Y/N)") + 
  ylab("Annual Fast Food Spend") 
```


## Plotting `Y` by `X2` (discrete variable)

::: {style="font-size: 90%;"}
Let's see also the relationship between the binary outcome variable `heart_disease` and the binary predictor variable `coffee_drinker`.

  + we use the handy `count` function from `dplyr` to count occurrences in categorical variable(s) combinations.
```{r}
#| output: true
#| output-location: slide
#| fig.cap: 'Also drinking coffee seems associated to a higher likelihood of heart disease (HD =1)'

# Dataset manipulation
heart_train %>% 
  # count the unique values per each group from 2 categorical variables' combinations
  dplyr::count(heart_disease, coffee_drinker, name = "count_by_group") %>% 
  dplyr::group_by(coffee_drinker) %>% 
  dplyr::mutate(
    total_coffee_class = sum(count_by_group),
    proportion = count_by_group / total_coffee_class) %>% 
  dplyr::ungroup() %>% 
  # filter only those with heart disease
  dplyr::filter(heart_disease == "Yes_HD") %>% 
# Plot  
  ggplot(aes(x = coffee_drinker, y = proportion, fill = coffee_drinker)) + 
  geom_bar(stat = "identity") + 
  scale_fill_manual(values = c("#57b7ff", "#e07689")) + 
  theme_minimal() +
  ylab("Percent with Heart Disease") +
  xlab("Coffee Drinker (Y/N)") +
  ggtitle("Figure 3: Percent of Coffee Drinkers with Heart Disease") +
  labs(fill = "Coffee Drinker") + 
  scale_y_continuous(labels = scales::percent)
```

:::

## `Linear regression` wouldn't work!
::: {style="font-size: 90%"}

+ In principle, we could use a **linear regression** model to study likelihood of having `Heart Disease` in relation to risk factors:
$$ Y = \beta_0 + \beta_1\text{(US\$ Spent Fast Food)} + \beta_2\text{(Coffee Drinker = YES)} $$

+ But we'll see why the **logistic regression** model is a better option.

$$ \log \left( \frac{p}{1-p} \right) = \beta_0 + \beta_1\text{(US\$ Spent Fast Food)} + \beta_2\text{(Coffee Drinker = YES)} $$

+ Let's compare the 2 models (for simplicity, we ignore the `coffee_drinker` variable for now.):
```{r}
# --- 1) Linear regression model
linear_mod <- lm(heart_disease ~ fast_food_spend# + coffee_drinker
                 , data = heart_data)
# --- 2) Logistic regression model
logit_mod <- glm(heart_disease ~ fast_food_spend# + coffee_drinker
                 , data = heart_data, family = "binomial")
```
:::

## [Compute alternative models' predictions]{.r-fit-text} 
::: {style="font-size: 85%"}

We can now extract the **coefficients** from the linear regression and logistic models, then estimate the **predicted outcomes** from these models: `lin_pred`, `logit_pred`, and `logistic_pred` (i.e. the conversion form log(odds) to probability).

```{r}
# --- 1) Extract coefficients from linear regression model
intercept_lin <- coef(linear_mod)[1]
fast_food_spend_lin <- coef(linear_mod)[2]
coffee_drinker_lin <- coef(linear_mod)[3]

# --- 2) Extract coefficients from logit regression model
intercept_logit <- coef(logit_mod)[1]
fast_food_spend_logit <- coef(logit_mod)[2]
coffee_drinker_logit <- coef(logit_mod)[3]

# --- Estimate predicted data from different models   
heart_data <- heart_data %>%
  mutate(
    # Convert outcome variable to factor
    heart_disease_factor = factor(heart_disease, 
                                  labels = c("No Disease (Y=0)", "Disease (Y=1)")),
    # 1) Linear model prediction
    lin_pred = intercept_lin + fast_food_spend_lin * fast_food_spend, 
    # 2) Logit model prediction
    logit_pred = intercept_logit + fast_food_spend_logit * fast_food_spend,  coffee_drinker,
    # 3) Convert logit to probability (logistic model prediction)
    logistic_pred = 1 / (1 + exp(-logit_pred)) 
    ) %>%
  arrange(fast_food_spend)   
```
:::


## Plot alternative models' outcomes
We can also plot the **predicted outcomes** from the 3 models to see how they differ.  

::: {style="font-size: 85%"}
```{r}
#| output: true
#| output-location: slide
#| fig.width: 8
#| fig.height: 4.5
#| fig.cap: |
#|      + (The **actual** data points are shown as the grey dots)<br>
#|      + The **linear** model predicts `values` that are ≠ 0 and 1, which poorly fit the actual data <br>
#|      + The **logit** model predicts `log(odds)` ranging from -Inf to +Inf, which is not interpretable <br>
#|      + The **logistic** model _squeezes_ `probabilities` between 0 and 1, which fits the data better  

# --- Plot  
ggplot(heart_data, aes(x = fast_food_spend)) +
  # Actual dataset observations (Y=0, Y=1 ) using `color =`
  geom_jitter(aes(y = heart_disease, color = heart_disease_factor), 
              width = 200, height = 0.03, alpha = 0.75, size = 2, shape = 16) + 
  # Models' predictions (smooth trends)
  geom_smooth(aes(y = lin_pred, color = "Linear Regression"), method = "lm", se = FALSE, 
              linewidth = 1.25, linetype = "dashed") +
  geom_smooth(aes(y = logit_pred, color = "Logit (Log-Odds)"), method = "lm", se = FALSE, 
              linewidth = 1.25, linetype = "dotdash") +
  geom_smooth(aes(y = logistic_pred, color = "Logistic Regression"), method = "glm", 
              method.args = list(family = "binomial"), se = FALSE, 
              linewidth = 1.25, linetype = "solid") +
  # Separate legends: color for dots, color for lines
  scale_color_manual(name = "Actual Y values & Prediction Models", 
                     values = c("No Disease (Y=0)" = "#A6A6A6", "Disease (Y=1)" = "#4c4c4c",
                                "Linear Regression" = "#d02e4c","Logit (Log-Odds)" = "#239b85", 
                                "Logistic Regression" = "#BD8723")) +
  # Define scales for the axes
  scale_x_continuous(breaks = seq(0, 6500, by = 500), limits = c(0, 6500), expand = c(0, 0))+
  scale_y_continuous(breaks = seq(-3, 3, by = .25)) +
  coord_cartesian(ylim = c(-1.25,1.25), xlim = c(0, 6500))  + theme_minimal() +
  labs(title = "Comparing Linear and Logistic Regression Predictions v. actual Y values",
       #subtitle = "(For simplicity, only fast food spend is considered)",
       y = "Y = Heart disease [0,1]", x = "Fast food spend [US$/yr]", color = "Actual Y values and Predictions")
```
:::

## `Linear regression` didn't work!

::: {style="font-size: 90%"}
+ Besides the poor fit, recall linear regression models implies certain **assumptions**: 
    + ***Linear relationship*** between `Y` and the predictors `X1` and `X2`  
<!-- (plot residuals v fitted values with no discernible pattern)-->
    + ***Residuals*** must be 1) approximately normally distributed and 2) uncorrelated
<!-- (QQ plot close to diagonal line for normality ) (e.g. like in time series if adjacent values have similar errors)-->
    + ***Homoscedasticity***: residuals should have constant variance
    + ***Non collinearity***: predictor variables should not be highly correlated with each other

<br><br>

+ Assumptions that are **not** met in this case
```{r}
#| label: fig-lin-reg_diag
#| output: true
#| output-location: slide
#| fig.cap: 'Diagnostic plots for a hypothetical linear regression model 👎🏻'

# diagnostic plots
par(mfrow=c(2,2))
plot(linear_mod)
```
:::

```{r}
#| eval: false # need at least 2 predictors
#| echo: false
#| output: false

# multicollinearity
vif(linear_mod)
# ideally < 5
```

## [Fitting a `logistic regression` model]{.r-fit-text}

+ [Now that we are convinced...]
+ ...let's fit instead a **logistic regression model** to the `heart_train` data using:
  + the `glm` function for Generalized Linear Models, 
    + with argument `family = binomial` to specify logistic regression which will use `logit` as the `link` function,
    + (after `~`) we include the 3 predictor variables in the model. 

```{r}
#| output: true

# Fit a logistic regression model
heart_model <- glm(heart_disease ~ coffee_drinker + fast_food_spend + income,
                   data = heart_train, 
                   family = binomial(link = "logit"))
```


## Model output

::: {style="font-size: 85%"}
+ @tbl-logit_heart (next) shows the model output, with the coefficient estimate for each predictor.
  + The `broom::tidy` function converts the model summary into a more readable data frame.
  + The `odds ratio` (= exponentiated coefficient estimate) is more interpretable than the coefficient itself.

```{r}
#| echo: true
#| output: true
#| output-location: slide
#| label: tbl-logit_heart
#| tbl-cap: |
#|  Logistic regression model output <br><br>
#|    + **estimates** of coefficients are in the form of `natural logarithm of the odds` (log (odds)) of the event happening (Heart Disease) <br>
#|        + a `positive estimate` indicates an increase in the odds of having Heart Desease  <br>
#|        + a `negative estimate` indicates a decrease in the odds of having Heart Desease <br>
#|    + **odds ratio** = the exponentiated coefficient estimate <br><br>

# Convert model's output summary into data frame
heart_model_coef <- broom::tidy(heart_model) %>% 
  # improve readability of significance levels
  dplyr::mutate('signif. lev.' = case_when(
    `p.value` < 0.001 ~ "***",
    `p.value` < 0.01 ~ "**",
    `p.value` < 0.05 ~ "*",
    TRUE ~ ""))%>%
  # add odds ratio column
  dplyr::mutate(odds_ratio = exp(estimate)) %>%
  dplyr::relocate(odds_ratio, .after = estimate) %>%
  dplyr::mutate(across(where(is.numeric), ~ round(.x, 4))) %>%
  # format as table
  knitr::kable() %>% 
  # reduce font size
  kable_styling(font_size = 20) %>% 
  # add table title
  kableExtra::add_header_above(c("Logistic Regression Analysis of Heart Disease Risk Factors"= 7))

heart_model_coef
```
::: 


## [Interpreting the logistic `coefficients`]{.r-fit-text}

:::{style="font-size: 80%"}

```{r}
#| echo: false
#| output: true

broom::tidy(heart_model) %>% 
  # improve readability of significance levels
  dplyr::mutate('signif. lev.' = case_when(
    `p.value` < 0.001 ~ "***",
    `p.value` < 0.01 ~ "**",
    `p.value` < 0.05 ~ "*",
    TRUE ~ ""))%>%
  # add odds ratio column
  dplyr::mutate(odds_ratio = exp(estimate)) %>%
  dplyr::relocate(odds_ratio, .after = estimate) %>%
  dplyr::mutate(across(where(is.numeric), ~ round(.x, 4))) %>%
  knitr::kable() %>% 
  # reduce font size
  kable_styling(font_size = 20) %>% 
  # highlight rows of terms (intercept) and income
  kableExtra::row_spec(1,  background = "#f1e7d3") %>%
  kableExtra::row_spec(4,  background = "#f1e7d3")

```
+ **Intercept**: gives the log-odds of heart disease when all predictor variables are zero. This is not generally interpreted, but the highly negative value suggests very low probability of heart disease <u>*in the sample of reference*</u>.
  + (If interpreted) the intercept term `-11.0554` would translate as **probability** $P =  e^{-11.0554} / (1 + e^{-11.0554}) = 0.00002$ or $0.002\%$.
  + ...which means: when $\text{coffee_drinker} = NO$, and $\text{fast_food_spend} = 0$, and $\text{income} = 0$, the probability of heart disease is as low as $0.002\%$.
  
<!-- WHICH MAKES TOTAL SENSE BASED ON OUR EXPLORATORY PLOTS -->

+ **Income**: Based on a `p-value = 0.8182`, we conclude that income is not significantly associated with heart disease. (Anyhow, the odds ratio of `≈1` would suggest no change in odds based on income.)

:::
 

## [The `coefficient` of fast food $$ 🍔🍟]{.r-fit-text}

:::{style="font-size: 75%"}

```{r}
#| echo: false
#| output: true

broom::tidy(heart_model) %>% 
  # improve readability of significance levels
  dplyr::mutate('signif. lev.' = case_when(
    `p.value` < 0.001 ~ "***",
    `p.value` < 0.01 ~ "**",
    `p.value` < 0.05 ~ "*",
    TRUE ~ ""))%>%
  # add odds ratio column
  dplyr::mutate(odds_ratio = exp(estimate)) %>%
  dplyr::relocate(odds_ratio, .after = estimate) %>%
  dplyr::mutate(across(where(is.numeric), ~ round(.x, 4))) %>%
  # filter for intercept and fast food spend
  dplyr::filter(term %in% c("(Intercept)", "fast_food_spend")) %>% 
  knitr::kable() %>% 
  # # reduce font size
  # kable_styling(font_size = 20) %>% 
  # highlight rows of terms (intercept) and income
  kableExtra::row_spec(2,  background = "#f1e7d3")  
```

<!-- + **Coefficients** - A positive coefficient implies an increased odds of the binary outcome (e.g., presence of a disease) for each unit increase in the predictor variable. Conversely, a negative coefficient suggests decreased odds. -->
<!-- + **Odds ratios** CONTINUOUS X VAR - The odds ratio represents the `factor by which the odds of the event increase or decrease for a one-unit change` in the predictor variable. An odds ratio > 1 indicates higher odds of the outcome, while an odds ratio < 1 indicates lower odds of the outcome. -->
<!-- + **Odds ratios** DUMMY X VAR - The odds ratio represents how the odds of the outcome occurring change for that category relative to the reference category. -->

+ The positive coefficient of **Fast Food Annual Spending (US$)**, `0.0024` (highly statistically significant as `p-value = 0.0000`), suggests a **positive association with heart disease**.

+ This means that **for each additional** $\Delta X_1 = +1 \; US\$$ spent on fast food annually:  
  + the **log(odds)** of heart disease increases by: $\Delta \log(\text{odds}) = \beta_{1} \times \Delta X_1 = 0.0024 \times 1 = 0.0024$  

  + the **odds** of heart disease (with spending) increase by a factor of: $OR = e^{0.0024} \approx 1.0024$ (compared to without spending).
  
<!-- or equivalently: the odds go up by $0.24\%$ for each additional dollar spent on fast food annually. -->

+ The **probability** of heart disease can be computed using the logistic function:  $P_{HD=1} = \frac{e^{\beta_0 + (\beta_1 \times X_1)}}{1 + e^{\beta_0 + (\beta_1 \times X_1)}}$  
  + Solving for $\beta_0 = -11.0554$, $\beta_1 = 0.0024$ and $X_1 = 1$ gives: 
  
  $$P_{HD=1} = \frac{e^{-11.0554 + (0.0024 \times 1)}}{1 + e^{-11.0554 + (0.0024 \times 1)}} = 0.00001583981$$ 
  
which indicates a probability of heart disease is **as low as $0.00159\%$** when `fast_food_spend` $= 1\; US\$$.
:::

## [The `coefficient` of fast food $$ 🍔🍟]{.r-fit-text}

:::{style="font-size: 75%"}
```{r}
#| echo: false
#| output: true

broom::tidy(heart_model) %>% 
  # improve readability of significance levels
  dplyr::mutate('signif. lev.' = case_when(
    `p.value` < 0.001 ~ "***",
    `p.value` < 0.01 ~ "**",
    `p.value` < 0.05 ~ "*",
    TRUE ~ ""))%>%
  # add odds ratio column
  dplyr::mutate(odds_ratio = exp(estimate)) %>%
  dplyr::relocate(odds_ratio, .after = estimate) %>%
  dplyr::mutate(across(where(is.numeric), ~ round(.x, 4))) %>%
  # filter for intercept and fast food spend
  dplyr::filter(term %in% c("(Intercept)", "fast_food_spend")) %>% 
  knitr::kable() %>% 
  # # reduce font size
  # kable_styling(font_size = 20) %>% 
  # highlight rows of terms (intercept) and income
  kableExtra::row_spec(2,  background = "#f1e7d3")  
```

+ However, considering that this is **annual spending**, we should use a more **adequate scale**!

+ For example, **for an additional** $\Delta X_1 = +100 \; US\$$ spent on fast food annually:  
  + the **log(odds)** of heart disease increases by: $\Delta \log(\text{odds}) = \beta_{1} \times \Delta X_1 = 0.0024 \times 100 = 0.24$  

  + the **odds** of heart disease  (with spending) increase by a factor of: $OR = e^{0.24} \approx 1.271$  (compared to without spending).

<!-- or equivalently: the odds go up by $27.1\%$ for each additional hundred dollars spent on fast food annually. --> 

+ The **probability** of heart disease can be computed using the logistic function: $P_{HD=1} = \frac{e^{\beta_0 + (\beta_1 \times X_1)}}{1 + e^{\beta_0 + (\beta_1 \times X_1)}}$  

+ Solving for $\beta_0 = -11.0554$, $\beta_1 = 0.0024$ and and $X_1 = 100$ gives:  
  $$P_{HD=1} = \frac{e^{-11.0554 + (0.0024 \times 100)}}{1 + e^{-11.0554 + (0.0024 \times 100)}} \approx 0.00002008$$    

  which (this time) indicates a (**still very low probability**) of heart disease at **$0.002008\%$** when `fast_food_spend` $= 100\; US\$$.  
:::
<!-- + WHY SUCH A LOW PROBABILITY? -->
<!--   + The probability at such a low baseline level (starting from an extremely low probability) does not increase linearly. -->
<!--   + The change in absolute probability is tiny because the function is still in the flat lower part of the logistic curve. -->
<!--   + The mean expenditure on fast food in the dataset is $2090.478, so the probability would be even lower. -->

## [The `coefficient` of coffee drinking ☕️]{.r-fit-text}

:::{style="font-size: 75%"}

```{r}
#| echo: false
#| output: true

broom::tidy(heart_model) %>% 
  # improve readability of significance levels
  dplyr::mutate('signif. lev.' = case_when(
    `p.value` < 0.001 ~ "***",
    `p.value` < 0.01 ~ "**",
    `p.value` < 0.05 ~ "*",
    TRUE ~ ""))%>%
  # add odds ratio column
  dplyr::mutate(odds_ratio = exp(estimate)) %>%
  dplyr::relocate(odds_ratio, .after = estimate) %>%
  dplyr::mutate(across(where(is.numeric), ~ round(.x, 4))) %>%
  # filter for intercept and fast food spend
  dplyr::filter(term %in% c("(Intercept)", "coffee_drinkerYes_Coffee")) %>% 
  knitr::kable() %>% 
  # reduce font size
  # kable_styling(font_size = 20) %>% 
  # highlight rows of terms (intercept) and income
  kableExtra::row_spec(2,  background = "#f1e7d3")  
```

+ The negative coefficient of **Coffee Drinker (=YES)**, `-0.7296` expressed in **log-odds**, means that coffee drinkers have _<u>lower</u> odds of having heart disease_ (relative to non-coffee drinkers). 
+ This effect is statistically significant as `p-value = 0.0122`.
+ Transforming the estimate into **odds ratio**, we obtain $\text{O.R.} = e^{-0.7296} = 0.48$ , meaning **coffee-drinkers** have 0.48 (or 48%) the odds that non-coffee drinkers have to experience heart disease (holding all other predictors fixed). 
$$ \text{Odds Ratio} = e^{-0.7296} = 0.48 $$
  + How much is the drop of the odds? Since O.R. is < 1, I have a percentage decrease:  
  $(1 - 0.48) = 0.52$ or $52\%$ **lower odds of having heart disease compared to non-coffee drinkers**.

<!-- 1 is no effect, so "how much did the odds drop relative to 1 (no effect, i.e. there is no difference between groups.)?" -->

:::{.aside}
Odds are typically expressed as a ratio, but they can be converted into a percentage increase relative to a baseline. However, the percentage **is not a probability**. 
:::
:::

<!-- + For non coffee drinkers, the **odds ratio** of heart disease is -->
<!-- $$ \text{Odds Ratio} = \frac{1}{0.48} = 2.08 $$    -->
<!--   + How much higher the odds? Since O.R. is > 1, I have a percentage increase (compared to the reference group) of:   -->
<!--   $(2.08 - 1) = 1.08$ or $108\%$ **higher odds of having heart disease compared to coffee drinkers**.  -->

## Understanding Odds
:::{style="font-size: 90%"}
+ **Odds** represent the ratio of the probability of an event occurring to the probability of it not occurring:
  - **Example:** If the probability of heart disease is **0.25 (25%)**, then the odds are:

$$ \text{Odds} = \frac{P(\text{event})}{1 - P(\text{event})} $$
  
  $$ \frac{0.25}{1 - 0.25} = \frac{0.25}{0.75} = 0.33 $$

  This means that for **every 1 person without heart disease, there are 0.33 people with it.**  
  (Or equivalently: for every **3 people without heart disease, 1 person has it.**)
:::

## Understanding Odds Ratio
:::{style="font-size: 90%"}
+ **Odds Ratio (O.R.)** compares the odds of an event occurring in one group (exposed) relative to another group (reference).

  + **Example:** If the odds of heart disease for coffee drinkers are **0.48**, and for non-coffee drinkers are **1**, then the odds ratio is:

$$\text{Odds Ratio} = \frac{\text{Odds}_{\text{exposed}}}{\text{Odds}_{\text{reference}}} = 
\frac{\text{Odds}_{\text{coffee drinkers}}}{\text{Odds}_{\text{non-coffee drinkers}}} = \frac{0.48}{1} = 0.48$$

+ **Interpretation:**
  - If **O.R. > 1**, the event is **more likely** in the exposed group.
  - If **O.R. < 1**, the event is **less likely** in the exposed group.
  - If **O.R. = 1**, there is **no difference** between groups.

::: 

## [Example: Coffee Drinkers vs. Non-Coffee Drinkers]{.r-fit-text}
:::{style="font-size: 75%"}
If the **odds of heart disease** are:

- Coffee drinkers (exposed group): $0.48$
- Non-coffee drinkers (reference group): $1$

Then the **Odds Ratio** is:

$$\text{Odds Ratio} = \frac{\text{Odds}_{\text{coffee drinkers}}}{\text{Odds}_{\text{non-coffee drinkers}}} =
\frac{0.48}{1} = 0.48$$

This means **coffee drinkers have 52% lower odds** of heart disease than non-coffee drinkers.

```{r}
#| output: true
#| echo: false

library(knitr)
library(kableExtra)

# Create the table with properly formatted headers
odds_table <- data.frame(
  "Odds Ratio (O.R.)" = c("O.R. < 1", "O.R. > 1"),
  "Example O.R." = c("0.48", "2.08"),
  "Formula for % Change in Odds" = c("(1 - O.R.) × 100", "(O.R. - 1) × 100"), 
  "Calculation" = c("(1 - 0.48) × 100 = 52%", "(2.08 - 1) × 100 = 108%"),
  "Interpretation" =  c("52% lower odds of having heart disease compared to non-coffee drinkers", 
                         "108% higher odds of having heart disease compared to coffee drinkers"),
  check.names = FALSE # Prevents R from replacing spaces with dots
)

# Print as a kable table with better formatting
kable(odds_table, align = "c") %>% 
  #caption = "Odds Ratio Interpretation: Coffee Drinkers vs. Non-Coffee Drinkers",  %>%
  # kableExtra::kable_styling(full_width = FALSE, font_size = 18) %>%  # Reduced font size slightly for better fit
  # kableExtra::row_spec(0, bold = TRUE, font_size = 22) %>%  # Bolder and slightly larger header
  kableExtra::column_spec(3:5, background = "#f1e7d3")  
```
:::

## [Wait, is drinking coffee good or bad? 🤔]{.r-fit-text}

Our **plot** above showed that there was a higher proportion of coffee drinkers with heart disease as compared to non coffee drinkers. 
However, our **model** just told us that coffee drinking is associated with a decrease in the likelihood of having heart disease.
<br><br>  
 How can that be❓
<br><br>
It’s because `coffee_drinking` and `fast__food_spend` are <u>**correlated**</u> so, on it’s own, it would _appear_ as if coffee drinking were associated with heart disease, but this is only because coffee drinking is also associated with fast food spend, which our model tells us is the real contributor to heart disease.

# 🖍️🖍️ qui 
<!-- SEGUI  -->

<!-- https://bookdown.org/brianmachut/uofm_analytics_r_hw_sol_2/logreg.html -->
<!-- e  -->
<!-- https://www.geeksforgeeks.org/how-to-interpret-odds-ratios-in-logistic-regression/?ref=next_article -->

## Making predictions from `logistic regression` model

```{r}

# Make predictions
heart_train$heart_disease_pred <- predict(heart_model, type = "response")



```

## Converting predictions into classifications

```{r}
#| output: true

# Convert predictions to classifications
heart_train$heart_disease_pred_class <- ifelse(heart_train$heart_disease_pred > 0.5, 1, 0)

```


## Evaluating the model]


## ROC curve]

```{r}

# Load the pROC package
library(pROC)

# Create a ROC curve
roc_curve <- roc(heart_train$heart_disease, heart_train$heart_disease_pred)

# Plot the ROC curve
plot(roc_curve, col = "blue", main = "ROC Curve", legacy.axes = TRUE)
```

## Confusion matrix]
```{r}
# Confusion matrix
confusion_matrix <- table(heart_train$heart_disease, heart_train$heart_disease_pred_class)

confusion_matrix
```

## Sensitivity and Specificity]

```{r}
# Sensitivity
sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])

# Specificity
specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])

```

# 🔴🟠🟡🟢🔵🟣

## Dataset on Breast Cancer Biopsy]

::: {style="font-size: 95%;"}
**Name**: Biopsy Data on Breast Cancer Patients  
**Documentation**: See reference on the data downloaded and conditioned for R here [https://cran.r-project.org/web/packages/MASS/MASS.pdf](https://cran.r-project.org/web/packages/MASS/MASS.pdf)  
**Sampling details**: This breast cancer database was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. He assessed biopsies of breast tumours for 699 patients up to 15 July
1992; each of nine attributes has been scored on a scale of 1 to 10, and the outcome is also known. The dataset contains the original Wisconsin breast cancer data with 699 observations on 11 variables.  
::: 

## Importing Dataset `biopsy`]

<!-- + **[Option 1]** the data can be directly obtained form the `MASS` R package -->
<!--   + Adapting the function `here` to match your own folder structure -->

<!-- <!-- FATTO IO MA LORO NON VEDONO -->  
<!-- ```{r} -->
<!-- #| eval: false -->
<!-- #| output: false -->
<!-- #| echo: false -->

<!-- # (after loading pckg) -->
<!-- library(MASS) # Support Functions and Datasets for Venables and Ripley's MASS -->
<!-- # I can call  -->
<!-- utils::data(biopsy) -->

<!-- # li salvo nel mio folder per poi darglieli  -->
<!-- readr::write_csv(biopsy, file = here::here("practice", "data_input", "04_datasets", -->
<!--                                       "biopsy.csv")) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| eval: false -->
<!-- #| output: false -->
<!-- #| echo: true -->

<!-- # (Check my working directory location) -->

<!-- # Use `here` in specifying all the subfolders AFTER the working directory  -->
<!-- biopsy2 <- readr::read_csv(file = here::here("practice", "data_input", "04_datasets", -->
<!--                                       "biopsy.csv") ,  -->
<!--                           show_col_types = FALSE, -->
<!--                           col_types = c("c", # id,  -->
<!--                                          "i", "i", "i", "i", "i", "i", "i", "i", "i",   #V1:V9  -->
<!--                                          "f"  #class  -->
<!-- )) -->
<!-- ``` -->

+ The data can be interactively obtained form the `MASS` R package

```{r}
# (after loading pckg)
# library(MASS)  

# I can call 
utils::data(biopsy)
```

 
## `biopsy` variables with description]

::: {style="font-size: 80%;"}
<!-- [[EXCERPT: see complete file in Input Data Folder]]{style="color:#77501a"} -->

```{r}
#| output: true
#| echo: false

biopsy_desc <- tibble::tribble(
  ~Variable, ~ Type, ~Description,
#"X" ,  "integer" ,         "row counter ", 
"id" ,   "character",               "Sample id",
"V1",    "integer 1 - 10",        "clump thickness",       
"V2",    "integer 1 - 10",        "uniformity of cell size",   
"V3",    "integer 1 - 10",        "uniformity of cell shape",  
"V4",    "integer 1 - 10",        "marginal adhesion",              
"V5",    "integer 1 - 10",        "single epithelial cell size",   
"V6",    "integer 1 - 10",        "bare nuclei (16 values are missing)",
"V7",    "integer 1 - 10",        "bland chromatin",             
"V8",    "integer 1 - 10",        "normal nucleoli",       
"V9",    "integer 1 - 10",        "mitoses",               
"class", "factor"     ,        "benign or malignant" )                 

kableExtra::kable(biopsy_desc)
```
:::

## `biopsy` variables exploration]
::: {style="font-size: 90%;"}
The `biopsy` data contains **699 observations of 9 continuous variables**, `V1`, `V2`, ..., `V9`.  

The dataset also contains a character variable: `id`, and a factor variable: `class`, with two levels ("benign" and "malignant").

```{r}
# check variable types
str(biopsy)
```
:::

## `biopsy` missing data]
::: {style="font-size: 90%;"}
There is one incomplete variable `V6` = "bare nuclei" with 16 missing values. 

  + remember the package `skimr` for exploring a dataframe?
```{r}
# check if vars have missing values
biopsy %>% 
  # select only variables starting with "V"
  skimr::skim(starts_with("V")) %>%
  dplyr::select(skim_variable, 
                n_missing)
```


```{r}
#| eval: false
#| echo: false
#| output: false

# [or] check vars with missing values
purrr::map (biopsy, skimr::n_missing  )
```

:::


<!-- ```{r} -->
<!-- # check for missing data -->
<!-- (missing_data  <-  biopsy %>%  -->
<!--   purrr::map_df(~sum(is.na(.))) %>% -->
<!--   tidyr::gather(., key = "Variable", value = "Missing") -->

<!-- ) -->
<!-- ``` -->

## `biopsy` missing data options]

::: {style="font-size: 90%;"}
We can decide what to do in these cases (*informed by our knowledge of the dataset*): 

+ **Option 1)** We drop the observation with incomplete data (i.e. with missing values for `V6` = **"bare nuclei"**)  with 16 missing values. 

```{r}
# remove rows with missing values
biopsy_drop <- biopsy %>% 
  dplyr::filter(!is.na(V6))

mean(biopsy_drop$V6)
```

+ **Option 2)** We impute the missing values with the mean of the variable `V6` = **"bare nuclei"**.

```{r}
# impute missing values with the median of the variable
biopsy_impute <- biopsy %>% 
  dplyr::mutate(V6 = ifelse(is.na(V6), median(V6, na.rm = TRUE), V6))

mean(biopsy_impute$V6)
```
:::

## `biopsy` dataset exploration]

::: {style="font-size: 75%;"}
+ Biopsied cells of 700 breast cancer tumors, used to determine if the tumors were benign or malignant. 
+ This determination was based on 9 characteristics of the cells, ranked from 1(benign) to 10(malignant):
  + **1) Clump Thickness** – How the cells aggregate. If monolayered they are benign and if clumped on top of each other they are malignant
  + **2) Uniform Size** – All cells of the same type should be the same size.
  + **3) Uniform Shape** If cells vary in cell shape they could be malignant
  + **4) Marginal Adhesion** – Healthy cells have a strong ability to stick together whereas cancerous cells do not
  + **5) Single Epithelial Size** – If epithelial cells are not equal in size, it could be a sign of cancer
  + **6) Bare nuclei** – If the nucleus of the cell is not surrounded by cytoplasm, the cell could be malignant
  + **7) Bland Chromatin** – If the chromatin's texture is coarse the cell could be malignant
  + **8) Normal Nucleoli** – In a healthy cell the nucleoli is small and hard detect via imagery. Enlarged nucleoli could be a sign of cancer
  + **9) Mitosis** – cells that multiply at an uncontrollable rate could be malignant

:::


## `biopsy` dataset preparation]

::: {style="font-size: 85%;"}
+ The explanatory variable(s) (`clump_thickness`, ..., `mitosis`) can be renamed for better readability
+ The observations with missing values (`bare_nuclei`) are removed for simplicity  
+ Patient ID (`id`) can be dropped as it is not used in this analysis

```{r}
#| output-location: column

# Create a clean version of the dataset
biopsy_clean <- biopsy %>%
  # rename the columns (new = old)
  rename(
    id                = ID,
    clump_thickness   =  V1,
    uniform_size      =  V2,
    uniform_shape     =  V3,
    marginal_adhesion =  V4,
    single_epith_size =  V5,
    bare_nuclei       =  V6,
    bland_chromatin   =  V7,
    normal_nuclei     =  V8,
    mitosis           =  V9,
    class             =  class) %>% 
  # remove rows with missing values
  na.omit(bare_nuclei) %>% 
  # remove the id column
  select(-id)

# check the structure of the dataset
paint::paint(biopsy_clean)
```
::: 

## `biopsy` sample splitting]

In Machine Learning, it is good practice to split the data into `training` and `testing` sets.

+ We will use the **training set** (80%) to fit the model and then the **testing set** (20%) to evaluate the model's performance.
```{r}
set.seed(123)

# Obtain 2 sub-samples from the dataset: training and testing
sample  <-  sample(c(TRUE, FALSE), nrow(biopsy_clean), replace = TRUE , prob = c(0.8, 0.2) )
biopsy_train  <-  biopsy_clean[sample,]
biopsy_test  <-  biopsy_clean[!sample,]
```

+ Which results in:
```{r}
# check the structure of the resulting datasets
dim(biopsy_train)
dim(biopsy_test)
```


## Indipendent variables' visualization]

::: {style="font-size: 80%;"}
1. We create a new df `biopsy_train2` (with only 3 columns) 
2. Then, in @fig-boxplot, we visualize the distribution of the explanatory variables, where each is plotted between the two classes of the tumor. 

```{r}
#| label: fig-boxplot
#| output-location: slide
#| fig-cap: |
#|  Boxplot of the independent variables <br> 
#|   - values are consitently higher & more dispersed for malignant tumors <br>
#|   - values between 1 and 2 are classified as benign and values greater than 2 are classified as malignant 

# New df for plotting
biopsy_train2 <- data.frame(
  "level" = c(biopsy_train$clump_thickness, biopsy_train$uniform_size,
              biopsy_train$uniform_shape, biopsy_train$marginal_adhesion,
              biopsy_train$single_epith_size, biopsy_train$bare_nuclei,
              biopsy_train$bland_chromatin, biopsy_train$normal_nuclei,
              biopsy_train$mitosis),
  "type" = c("Clump Thickness", "Uniform Size", "Unifrom Shape",
             "Marginal Adhesion", "Single Epithilial Size", "Bare Nuclei",
             "Bland Chromatin", "Normal Nuclei", "Mitosis"), 
  "class" = c(biopsy_train$class))

# Plot
ggplot(biopsy_train2, aes(x = level, y = class , colour = class)) + 
  geom_boxplot(fill = NA) +
  scale_color_manual(values = c("#005ca1", "#9b2339")) + 
  geom_jitter(aes(fill = class), alpha = 0.25, shape = 21, width = 0.2) +  
  scale_fill_manual(values = c("#57b7ff", "#e07689")) +  
  facet_wrap(~type, scales = "free") +  
  theme(plot.title = element_text(size = 13,face="bold", color = "#873c4a"),
        axis.text.x = element_text(size=12,face="italic"), 
        axis.text.y = element_text(size=12,face="italic"),
        legend.position = "none") + 
  labs(title = "Distribution of each explanatory variable by tumor class (benign/malignant) in samples") + 
  ylab(label = "") + xlab(label = "")
```
:::


## Logistic regr.: model fitting]

::: {style="font-size: 90%"}
+ We fit a **logistic regression model** to the `biopsy_train` data using:
  + the `glm` function with argument `family = binomial` to specify the logistic regression model;
  + and with `Class ~ .` to specify an initial model that uses all the variables as predictors (**backward elimination** approach).

```{r}
# Building initial model 
model = stats::glm(class ~ . , family = binomial, data=biopsy_train)
```

+ @tbl-logit_null shows the model `summary`, with the `coefficient estimate` for each  predictor.
  + the `broom::tidy` function converts the model summary into a data frame.

```{r}
#| echo: true
#| output: false

broom::tidy(model) %>% 
  mutate('Sign.lev' = case_when(
    `p.value` < 0.001 ~ "***",
    `p.value` < 0.01 ~ "**",
    `p.value` < 0.05 ~ "*",
    TRUE ~ ""))%>%
  mutate(across(where(is.numeric), ~ round(.x, 4))) %>%
  knitr::kable() 
```

:::


## Logistic regr.: model coefficients]

```{r}
#| echo: false
#| output: true
#| label: tbl-logit_null
#| tbl-cap: |
#|  Complete logistic regression model <br>
#|    + coefficients are in the form of `natural logarithm of the odds` of the event happening  <br>
#|    + `positive estimate` indicates an increase in the odds of finding a malignant tumor

broom::tidy(model) %>% 
  mutate('signif. lev.' = case_when(
    `p.value` < 0.001 ~ "***",
    `p.value` < 0.01 ~ "**",
    `p.value` < 0.05 ~ "*",
    TRUE ~ ""))%>%
  mutate(across(where(is.numeric), ~ round(.x, 4))) %>%
  knitr::kable() 
```





## Logistic regr.: coefficients' interpretation

::: {style="font-size: 80%"}
In logistic regression, the coefficients are in the form of the `natural logarithm of the odds` of the response event happening (i.e. $Y_i = 1$):

$$logit(p_i) = \ln\left( \frac{p_i}{1-p_i} \right) = -9.5063 + 0.3935 \times Clump\_Thickness + ... + 0.5065 \times Mitosis$$

However, with some algebraic transformation, the `logit` function can be inverted to obtain the `probability of the response event happening` as a function of the predictors:

$$p_i = \frac{1}{1 + e^{-(-9.5063 + 0.3935 \times Clump\_Thickness + ... + 0.5065 \times Mitosis)}}$$

This equation represents the **logistic regression model's best-fit line**.
:::

<!-- ## 🟡 Logistic Regr.: coefficients' interpretation -->
<!-- <!-- STATS QUEST https://github.com/StatQuest/logistic_regression_demo/blob/master/logistic_regression_demo.R -->  

<!-- ::: {style="font-size: 75%;"} -->

<!-- @tbl-logistic shows the estimated coefficients of the logistic regression model in the form of `natural logarithm of the odds` of an event happening:   -->

<!-- $odds = \frac{p}{1-p}$ ,  $Estimate = \log-odds = \log\left(\frac{p}{1-p}\right)$  -->

<!-- + `Intercept's` (*negative* and *significant*) indicates that when all other variables are 0, the log-odds of a malignant tumor **decreases** by 9.50632.  -->

<!-- + `Bare_Nuclei`, `clump_thickness` and `Marginal_Adhesion` have *positive* and *significant*) coefficients.  -->
<!--   + They represent the change in the log-odds of the outcome (a malignant tumor) for a one-unit increase in the predictor, holding other predictors constant.   -->

<!-- ```{r} -->
<!-- #| label: tbl-logistic -->
<!-- #| tbl-cap: Estimated coefficients of the logistic regression -->
<!-- #| output-location: slide -->

<!-- summary(model)$coefficients %>% -->
<!--   as.data.frame() %>% -->
<!--   mutate(Signif. = case_when( -->
<!--     `Pr(>|z|)` < 0.001 ~ "***", -->
<!--     `Pr(>|z|)` < 0.01 ~ "**", -->
<!--     `Pr(>|z|)` < 0.05 ~ "*", -->
<!--     TRUE ~ "")) %>% -->
<!--   mutate(Estimate = round(Estimate, 4)) %>%  -->
<!--   select(1,5) %>%  -->
<!--   kable() -->
<!-- ``` -->
<!-- ::: -->


<!-- ## 🚫 Logistic Regr.: model evaluation -->
<!-- <!-- EQUITABLE EQUATIONS = https://youtu.be/E7J3M1oYVlc?si=zmBIfoqvErliFqU3 -->

<!-- Here I am verifying the logit shape . . .  -->

<!-- ```{r} -->
<!-- df_sum <- biopsy_train %>%  -->
<!--   # just one variable  -->
<!--   select(Bare_Nuclei, Class) %>%  -->
<!--   group_by(Bare_Nuclei) %>% -->
<!--   summarise( prop_malig = mean(Class == "malignant"), -->
<!--              count = n()) -->


<!-- ggplot(df_sum, aes (x = Bare_Nuclei,  -->
<!--                       y = prop_malig)) + -->
<!--   geom_point() -->
<!-- ``` -->

## Logistic regr.: multicollinearity]
Let’s check for **collinearity** using the `VIF` function from the ‘`car`’ package. 

+ A Variance Inflation Factor $VIF > 5$ indicates that there could be correlation between predictor variables.

+ The VIF values are all less than 5, which indicates that **there is no severe correlation** between predictor variables in the model. ✅
```{r}
car::vif(model)
```

## Logistic regr.: model performance]

Highlight key logistic model performance metrics using `broom::glance()` function: 

  + `AIC`: A measure of model quality; lower values indicate a better fit with fewer parameters.
  + `Null Deviance`: A measure of model error; how well the response variable can be predicted by a model with only an intercept term.
  + `Deviance`: A measure of model error; how well the response variable can be predicted by a model with predictor variables.
      + lower values mean the model fits the data better!

```{r}
#| output: true
#| label: tbl-logit_perf
#| tbl-cap: |
#|  Key logistic model performance metrics

broom::glance(model)[, c("AIC", "null.deviance","deviance")] %>% 
  # show only performance  metrics
  knitr::kable() 
```

## Logistic regr.: improving the model]
<!-- https://openintro-ims.netlify.app/model-logistic#logistic-model-with-many-variables -->

::: {style="font-size: 85%;"}
+ The model includes all variables, but we could make it more *parsimonious* by removing variables that are not significant!
+ We use a statistic called the **Akaike Information Criterion (AIC)** to compare models.
  + AIC's calculation gives a penalty for including additional variables.
  + The model with **the lowest AIC is considered the best**.

```{r}
# For example let's fit a model without the variable `uniform_size`
model2 = glm(class~ .-uniform_size, family = binomial, data=biopsy_train)
```

According to the AIC values, the `model2` seems better (AIC is lower).
```{r}
# Compare the AIC values of the 2 models
tibble(Model = c("model", "model2"), 
       AIC = c(AIC(model), AIC(model2) )) %>% 
  kable()
```


:::

## Logistic regr.: systematic model selection]

::: {style="font-size: 75%;"}
+ The `MASS` package's function `stepAIC` enables to perform a systematic model selection (by **AIC**):
  + The `direction` argument specifies the direction of the *stepwise* search.
  + The `trace` argument (if set to `TRUE`) prints out all the steps.

+ The `best_model` has removed these variables:
  + **uniform_shape**
  + **single_epith_size**
  + **normal_nuclei**
  
+ The `best_model` has the **lowest AIC value** (from 100 to 98.5), despite a **higher Residual Deviance** than the full model (from 80 to 80.6), albeit by a very slight amount.

```{r}
#| output-location: slide 
# Select the best model based on AIC
best_model <- MASS::stepAIC(model, direction = "both", trace = FALSE)

# Compare the AIC values of full and best model
tibble(Model = c("model", "best_model"), 
       AIC = c(AIC(model), AIC(model2)),
       Deviance = c(deviance(model), deviance(best_model))) %>% kable()
```
:::

```{r}
#| echo: false
#| eval: false

# OR separate extraction
formula(best_model)
coef(best_model)
AIC(best_model)
deviance(best_model)

stepwise_model <- MASS::stepAIC(model, direction = "both", trace = TRUE)
AIC(stepwise_model)
# compare manual AIC and stepAIC
aic_manual <- sapply(list(model, stepwise_model), AIC)
aic_manual
```

## Logistic regr.: predicting on `test data`]
 
+ We can use the `predict` function to predict the class of the `biopsy_test` using the `best_model`.
  + `biopsy_test_pred` contains the probability that each of observation (in test data) is malignant.
+ The classification of the `PredictedValue...` can be done using different probability thresholds (0.5, 0.4, 0.3, etc.). which will aﬀect the true and false positivity rates.: 
  + `PredictedValue_05` is the standard threshold of 0.5.
  + `PredictedValue_04` is a more conservative threshold of 0.4.
  + `PredictedValue_07` is a more aggressive threshold of 0.7.
  
```{r}
# Fitted value for the test data 205 samples based on model
biopsy_test_pred <- predict(best_model, newdata = biopsy_test, type = "response")

# Convert the predicted probabilities into 2 predicted classes
ActualValue <- biopsy_test$class

# Different possible thresholds for the predicted probabilities
PredictedValue_05 <- if_else(biopsy_test_pred > 0.5, "pred_malignant", "pred_benign")
PredictedValue_04 <- if_else(biopsy_test_pred > 0.4, "pred_malignant", "pred_benign")
PredictedValue_07 <- if_else(biopsy_test_pred > 0.7, "pred_malignant", "pred_benign")
```

## Logistic regr. predictions: `confusion matrix`]
> In diagnosing malignant tumors, it is important to keep the **false negative rate low** as this would be telling someone who has a malignant tumor that it is benign. 

+ At $p = 0.7$, the model will predict more false positives than at $p = 0.4$ (6, v. 4 FN) -- which we DON'T WANT
  + in this situation $p = 0.4$ is preferable.

Then, we can evaluate the model's performance on the `test data` by building a **confusion matrix**
```{r}
# Build the confusion matrix with p = 0.4
table(ActualValue=biopsy_test$class, PredictedValue_04) %>% 
  knitr::kable()
```

```{r}
# Build the confusion matrix with p = 0.7
table(ActualValue=biopsy_test$class, PredictedValue_07) %>% 
  knitr::kable()
```

## Logistic regr. predictions: `accuracy`]

+ We found that **a cutoﬀ of 0.4** gives a good balance of low false negatives while still maintaining a high true positive rate.

+ With the chosen threshold of $p = 0.4$, we can calculate the model's `accuracy` on the test data.

```{r}
# Build the confusion matrix with p = 0.4
conf_matr_04 <- table(ActualValue=biopsy_test$class, PredictedValue_04)  

# Calculate the accuracy
accuracy <- sum(diag(conf_matr_04)) / sum(conf_matr_04)
accuracy
```

## Logistic regr.: ROC curve]
::: {style="font-size: 90%;"}
```{r}
#| output-location: slide
#| fig-subcap: "This shows why lowering the cutoﬀ improves the accuracy of the model as some malignant tumors are being underestimated which would cause false negatives."

biopsy_test_pred

predicted.data <- data.frame(prob.of.malig=biopsy_test_pred, malig = biopsy_test$class)

predicted.data <- predicted.data[order(predicted.data$prob.of.malig, decreasing = F),]

predicted.data$rank <- 1:nrow(predicted.data)

plot_ROC <- ggplot(data=predicted.data, aes(x=rank, y=prob.of.malig)) +
  geom_point(aes(color=malig), alpha=1, shape=4, stroke=2) +
  xlab("Index") + 
  ylab("Predicted Probability of Tumor Being Malignant")  

plot_ROC
```
:::

## Logistic regr.: conclusions]

+ `Uniform Size` and `Single Epithithial Size` were not significant in predicting the malignancy of tumor cells so our model does not include these variables. 

+ Our fitted model reduces the null deviance and AIC without impacting the residual deviance by a significant amount and is able to predict the testing dataset with >90% accuracy.

+ For further analysis, we could run the model multiple times because our original and revised model are similar. 

+ New training and testing data would help confirm our results and help identify possible overfitting.


<!-- ## ...-->
<!-- # ⛔️⛔️⛔️ qui 🟠🟠 -->
<!-- ## Logistic regr.: model evaluation on `test data`-->
<!-- ::: {style="font-size: 80%;"} -->
<!-- + We can use the `predict` function to predict the class of the `test data` using the best model. -->
<!-- + Then we use the `ROCR` package evaluate and visualize the classification  -->
<!--   + the `performance` function calculates the **True Positive Rate (TPR)** and **False Positive Rate (FPR)** for various thresholds. From it we can get: -->
<!--     + `tpr` = **True Positive Rate (TPR)** is the proportion of actual positive cases that are correctly predicted as positive. -->
<!--     + `fpr` = **False Positive Rate (FPR)** is the proportion of actual negative cases that are incorrectly predicted as positive. -->
<!--     + `thresholds` = the list of thresholds used to calculate the TPR and FPR. -->

<!-- ```{r} -->
<!-- # Fitted value for the test data 205 samples based on model -->
<!-- pred = stats::predict(best_model, biopsy_test, type = "response") -->
<!-- # Create a prediction object for the ROCR package -->
<!-- ROCRPRed <- ROCR::prediction(predictions = pred, labels = biopsy_test$class) -->
<!-- # Create a performance object (True Positive Rate and False Positive Rate) for various thresholds. -->
<!-- ROCRPerf <- ROCR::performance(ROCRPRed, "tpr", "fpr") -->
<!-- # Extract TPR and FPR data -->
<!-- fpr <- ROCRPerf@x.values[[1]] # x -->
<!-- tpr <- ROCRPerf@y.values[[1]] # y -->
<!-- # List of thresholds used to calculate the TPR and FPR in the performance object -->
<!-- thresholds <- ROCRPerf@alpha.values[[1]] -->
<!-- ``` -->
<!-- ::: -->


<!-- ## Logistic regr.: visualization of model performance at varying thresholds] -->

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| output: false -->

<!-- # plot the ROC curve -->
<!-- ROCR::plot( ROCRPerf ,   -->
<!--            colorize=TRUE,  -->
<!--            print.cutoffs.at=seq(0.1, by=0.2) ,  -->
<!--            xlim=c(0,0.1),  -->
<!--            ylim=c(0.8,1) -->
<!--            ) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| output-location: slide -->
<!-- #| fig-subcap: "Threshold color gradient: YELLOW = the model predicts positive only for very confident cases, PURPLE: the model predicts positive for many cases, even with lower confidence." -->

<!-- # Create a data frame for ggplot2 -->
<!-- perf_data <- data.frame(FPR = fpr, TPR = tpr, Threshold = thresholds) -->

<!-- # Plot using ggplot2 -->
<!-- library(ggplot2) -->
<!-- ggplot(perf_data, aes(x = FPR, y = TPR, color = Threshold)) + -->
<!--   geom_line(linewidth = 1) + -->
<!--   scale_color_viridis_c(option = "C") +  # Optional: better color scale -->
<!--   labs(title = "TPR vs FPR", -->
<!--        x = "False Positive Rate [0, 0.1]", -->
<!--        y = "True Positive Rate [0.8, 1]", -->
<!--        color = "Threshold ") + -->
<!--   coord_cartesian(xlim = c(0, 0.1), ylim = c(0.8, 1)) + -->
<!--   theme_minimal() -->
<!-- ``` -->

<!-- The practical use of this is to identify a threshold where the **True Positive Rate (TPR)** is acceptably high, without a significant increase in **False Positive Rate (FPR)**.  -->



# 🟠 K-MEANS CLUSTERING: EXAMPLE of UNSUPERVISED ML ALGORITHM

## ...]

# PCA: EXAMPLE of UNSUPERVISED ML ALGORITHM

Reducing high-dimensional data to a lower number of variables
<!-- 1) PCA fatta a mano. -->
<!-- PCA step by step come in Statology ma con il data set della Lecture nmr_bins…csv  -->

<!-- https://www.statology.org/principal-components-analysis-in-r/ -->

<!-- Probabilmente non viene proprio uguale perchè in MA fa normalizzazione e scaling mentre Statology fa solo scaling, ma fa niente, diciamo che ci serve per vedere la differenza -->

## `biopsy` dataset manipulation]

We will: 

+ exclude the non-numerical variables (`id` and `class`) before conducting the PCA.   
+ exclude the individuals with missing values using the `na.omit()` or `filter(complete.cases()` functions.

+ We can do both in 2 equivalent ways:

<br> 

:::: {.columns}

::: {.column width="50%"}
#### with `base` R (more compact)
```{r}
#| eval: false

# new (manipulated) dataset 
data_biopsy <- na.omit(biopsy[,-c(1,11)])
```
:::
  
::: {.column width="50%"}
#### with `dplyr` (more explicit)
```{r}
# new (manipulated) dataset 
data_biopsy <- biopsy %>% 
  # drop incomplete & non-integer columns
  dplyr::select(-ID, -class) %>% 
  # drop incomplete observations (rows)
  dplyr::filter(complete.cases(.))
```
:::
  
::::


## `biopsy` dataset manipulation]

We obtained a new dataset with 9 variables and 683 observations (instead of the original 699).  
```{r}
# check reduced dataset 
str(data_biopsy)
```


## Calculate Principal Components

The first step of PCA is to calculate the principal components. To accomplish this, we use the `prcomp()` function from the `stats` package.  

+ With argument `“scale = TRUE”` each variable in the biopsy data is scaled to have a mean of `0` and a standard deviation of `1` before calculating the principal components (just like option `Autoscaling` in MetaboAnalyst)


```{r}
# calculate principal component
biopsy_pca <- prcomp(data_biopsy, 
                     # standardize variables
                     scale = TRUE)
```


## Analyze Principal Components

Let’s check out the elements of our obtained `biopsy_pca` object 

  + (All accessible via the  `$` operator)


```{r}
names(biopsy_pca)
```

**"sdev"** = the standard deviation of the principal components

**"sdev"\^2** = the variance of the principal components (**eigenvalues** of the covariance/correlation matrix)

**"rotation"** = the matrix of variable **loadings** (i.e., a matrix whose columns contain the **eigenvectors**).

**"center"** and **"scale"** = the means and standard deviations of the original variables before the transformation;

**"x"** = the principal component scores (after PCA the observations are expressed in principal component scores)

## Analyze Principal Components (cont.)

::: {style="font-size: 90%;"}
We can see the summary of the analysis using the `summary()` function

1. The first row gives the **Standard deviation** of each component, which can also be retrieved via `biopsy_pca$sdev`. 
2. The second row shows the **Proportion of Variance**, i.e. the percentage of explained variance.

```{r}
summary(biopsy_pca)
```
:::


## Proportion of Variance for components]

2. The row with **Proportion of Variance** can be either accessed from summary or calculated as follows:

```{r}
# a) Extracting Proportion of Variance from summary
summary(biopsy_pca)$importance[2,]

# b) (same thing)
round(biopsy_pca$sdev^2 / sum(biopsy_pca$sdev^2), digits = 5)
```

<br>

> The output suggests the **1st principal component** explains around 65% of the total variance, the **2nd principal component** explains about 9% of the variance, and this goes on with diminishing proportion for each component. 


## Cumulative Proportion of variance for components]

3. The last row from the `summary(biopsy_pca)`, shows the **Cumulative Proportion** of variance, which calculates the cumulative sum of the Proportion of Variance. 

```{r}
# Extracting Cumulative Proportion from summary
summary(biopsy_pca)$importance[3,]
```

<br>

> Once you computed the PCA in R you must decide the number of components to retain based on the obtained results.


# VISUALIZING PCA OUTPUTS

## Scree plot

There are several ways to decide on the number of components to retain. 

+ One helpful option is visualizing the percentage of explained variance per principal component via a **scree plot**. 
  + Plotting with the `fviz_eig()` function from the `factoextra` package

```{r}
#| output-location: slide
#| fig-cap: "The obtained **scree plot** simply visualizes the output of `summary(biopsy_pca)`."

# Scree plot shows the variance of each principal component 
factoextra::fviz_eig(biopsy_pca, 
                     addlabels = TRUE, 
                     ylim = c(0, 70))
```

<br> 

> Visualization is essential in the interpretation of PCA results. Based on the number of retained principal components, which is usually the first few, the observations expressed in component scores can be plotted in several ways.

## Principal Component `Scores`]

After a PCA, the observations are expressed as **principal component scores**.   

1. We can retrieve the principal component scores for each Variable by calling `biopsy_pca$x`, and  store them in a new dataframe `PC_scores`.
2. Next we draw a `scatterplot` of the observations -- expressed in terms of principal components 

```{r}
#| output-location: slide

# Create new object with PC_scores
PC_scores <- as.data.frame(biopsy_pca$x)
head(PC_scores)
```

It is also important to visualize the observations along the new axes (principal components) to interpret the relations in the dataset:

## Principal Component `Scores` plot (adding label variable)]

3. When data includes a factor variable, like in our case, it may be interesting to show the grouping on the plot as well.

  + In such cases, the label variable `class` can be added to the PC set as follows.

```{r}
# retrieve class variable
biopsy_no_na <- na.omit(biopsy)
# adding class grouping variable to PC_scores
PC_scores$Label <- biopsy_no_na$class
```

<br>
The visualization of the observation points (point cloud) could be in 2D or 3D.

## Principal Component `Scores` plot (2D)]

The Scores Plot can be visualized via the `ggplot2` package. 

+ grouping is indicated by argument the `color = Label`; 
+ `geom_point()` is used for the point cloud.


```{r}
#| output-location: slide
#| fig-cap: "Figure 1 shows the observations projected into the new data space made up of principal components"

ggplot(PC_scores, 
       aes(x = PC1, 
           y = PC2, 
           color = Label)) +
  geom_point() +
  scale_color_manual(values=c("#245048", "#CC0066")) +
  ggtitle("Figure 1: Scores Plot") +
  theme_bw()
```



## Principal Component `Scores` (2D Ellipse Plot)]

Confidence ellipses can also be added to a grouped scatter plot visualized after a PCA. We use the `ggplot2` package. 

+ grouping is indicated by argument the `color = Label`; 
+ `geom_point()` is used for the point cloud; 
+ the `stat_ellipse()` function is called to add the ellipses per biopsy group.

```{r}
#| output-location: slide
#| fig-cap: "Figure 2 shows the observations projected into the new data space made up of principal components, with 95% confidence regions displayed." 

ggplot(PC_scores, 
       aes(x = PC1, 
           y = PC2, 
           color = Label)) +
  geom_point() +
  scale_color_manual(values=c("#245048", "#CC0066")) +
  stat_ellipse() + 
  ggtitle("Figure 2: Ellipse Plot") +
  theme_bw()
```



## Principal Component `Scores` plot (3D)]

::: {style="font-size: 80%;"}
A 3D scatterplot of observations shows the first **3 principal components’ scores**. 

+ For this one, we need the `scatterplot3d()` function of the `scatterplot3d` package;
+ The color argument assigned to the Label variable;
+ To add a legend, we use the `legend()` function and specify its coordinates via the `xyz.convert()` function.

```{r}
#| output-location: slide
#| fig-cap: "Figure 3 shows the observations projected into the new 3D data space made up of principal components." 

# 3D scatterplot ...
plot_3d <- with(PC_scores, 
                scatterplot3d::scatterplot3d(PC_scores$PC1, 
                                             PC_scores$PC2, 
                                             PC_scores$PC3, 
                                             color = as.numeric(Label), 
                                             pch = 19, 
                                             main ="Figure 3: 3D Scatter Plot", 
                                             xlab="PC1",
                                             ylab="PC2",
                                             zlab="PC3"))

# ... + legend
legend(plot_3d$xyz.convert(0.5, 0.7, 0.5), 
       pch = 19, 
       yjust=-0.6,
       xjust=-0.9,
       legend = levels(PC_scores$Label), 
       col = seq_along(levels(PC_scores$Label)))
```


:::

## Biplot: principal components v. original variables]

Next, we create another special type of scatterplot (a **biplot**) to understand the relationship between the principal components and the original variables.  
In the `biplot` each of the observations is projected onto a scatterplot that uses the ***first and second principal components as the axes***.

+ For this plot, we use the `fviz_pca_biplot()` function from the `factoextra` package 
  + We will specify the color for the variables, or rather, for the "loading vectors"
  + The `habillage` argument allows to highlight with color the grouping by `class`

```{r}
#| output-location: slide
#| fig-cap: "The axes show the principal component scores, and the vectors are the loading vectors"

factoextra::fviz_pca_biplot(biopsy_pca, 
                repel = TRUE,
                col.var = "black",
                habillage = biopsy_no_na$class,
                title = "Figure 4: Biplot", geom="point")
```

## Interpreting biplot output
::: {style="font-size: 95%;"}
Biplots have two key elements: **scores** (the 2 axes) and **loadings** (the vectors). 
As in the scores plot, each point represents an observation projected in the space of principal components where:

+ Biopsies of the same class are located closer to each other, which indicates that they have similar **scores**  referred to the 2 main principal components; 
+ The **loading vectors** show strength and direction of association of original variables with new PC variables.

> As expected from PCA, the single `PC1` accounts for variance in almost all original variables, while `V9` has the major projection along `PC2`.

:::

## Interpreting biplot output (cont.)
 
```{r}
scores <- biopsy_pca$x

loadings <- biopsy_pca$rotation
# excerpt of first 2 components
loadings[ ,1:2] 
```

<!-- # PLS-DA: step by step (example) -->
<!-- 1) PCA + PLS_DA + CLuster  -->
<!-- https://rpubs.com/Anita_0736/PD_ANALYSIS  -->

<!-- 2) PLS fatta a mano -->
<!-- PLS step by step come in Statology ma con il data set della Lecture nmr_bins…csv  -->

<!-- https://www.statology.org/partial-least-squares-in-r/ -->

<!-- In MetaboAnalyst usano la PLS-DA che non so cosa ha di diverso ma può essere anche carino vedere la differenza -->



<!-- # ML WITH UNSUPERVISED ALGORITHMS -->

<!-- # Hierarchical Clustering (example) -->
<!-- 3) Hierarchical Clustering fatto a mano come in Statology ma con il data set della Lecture nmr_bins…csv  -->

<!-- https://www.statology.org/hierarchical-clustering-in-r/ -->

<!-- Se non hai tempo o non si riesce l’alternativa è che li faccio giocare anche loro con il MetaboAnalyst anche nelle esercitazioni, sperando che la rete regga e la piattaforma pure.. -->



<!-- # _______   -->

<!-- ## Fonti ...    -->

<!--  + **Cocca**  https://www.statmethods.net/stats/power.html -->

<!--  + **CORESTATS 6 !!!** https://mvanrongen.github.io/corestats-in-r_tidyverse/power-analysis.html  -->

<!--  + **G*Power (free application)** https://www.linkedin.com/learning/the-data-science-of-experimental-design/installing-g-power?resume=false -->
<!--  + **library(pwrss)** https://cran.r-project.org/web/packages/pwrss/vignettes/examples.html#7_Analysis_of_(Co)Variance_(F_Test) -->

<!--  + **dakota** https://med.und.edu/research/daccota/_files/pdfs/berdc_resource_pdfs/sample_size_r_module.pdf -->
<!--  + **!!! Salvatore Mangiafico**  https://rcompanion.org/rcompanion/d_02.html  -->
<!--   + each test ends with power analsyis  -->




## Recap of the workshop's content]

::: {style="font-size: 95%;"}

**TOPICS WE COVERED**

1. Motivated the choice of learning/using **R for scientific quantitative analysis**, and lay out some fundamental concepts in biostatistics with concrete R coding examples.

2. Consolidated understanding of **inferential statistic**, through R coding examples conducted on real biostatistics research data.

3. Discussed the **relationship between any two variables**, and introduce a widely used analytical tool: **regression**.
 
4. Presented a popular ML technique for dimensionality reduction (**PCA**), performed both with `MetaboAnalyst` and `R`. 

5. Introduction to **power analysis** to define the correct sample size for hypotheses testing and discussion of how ML approaches deal with available data.


:::


## Final thoughts

::: {style="font-size: 95%;"}
::: {style="color:#77501a"}


+ While the workshop only allowed for a synthetic overview of fundamental ideas, it hopefully provided a solid foundation on the most common statistical analysis you will likely run in your daily work: 
  + Thorough **understanding of the input data** and the data collection process 
  + Univariate and bivariate **exploratory analysis** (accompanied by visual intuition) to form hypothesis 
  + Upon verifying the assumptions, we **fit data** to hypothesized model(s)
  + **Assessment of the model performance** ($R^2$, $Adj. R^2$, $F-Statistic$, etc.)


+ You should now have a solid grasp on the R language to keep using and exploring the huge potential of this programming ecosystem

+ We only scratched the surface in terms of ML classification and prediction models, but we got a hang of the **fundamental steps** and some **useful tools** that might serve us also in more advanced analysis 

:::

:::

